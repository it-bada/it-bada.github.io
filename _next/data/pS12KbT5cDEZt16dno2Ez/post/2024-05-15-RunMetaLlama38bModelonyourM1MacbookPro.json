{"pageProps":{"post":{"title":"M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요","description":"","date":"2024-05-15 23:37","slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro","content":"\n\nLet's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\n\n![Click here to see the image.](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png)\n\nOllama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\n\nIt usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\n\n\n\n대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\n\n## 1. 'https://ollama.com/download/mac'으로 이동해주세요.\n\n![이미지](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png)\n\nZip 파일을 다운로드하고 압축을 풉니다.\n\n\n\n## 2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png)\n\n## 3. 터미널을 열고 다음과 같이 입력해주세요.\n\n```js\nollama run llama3\n```\n\n\n\n그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png)\n\n브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.\n\n## 4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\n\n\n\n마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\n\n```js\nvim ~/.zshrc\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop='osascript -e \"tell application \\\"Ollama\\\" to quit\"'\nalias ollama_start='ollama run llama3'\n\n#새 세션을 열고 아래 명령어를 사용하여 Ollama 시작 및 중지\n\nollama_start\nollama_stop\n```\n\n## 5. Llama3 성능 평가\n\n```js\ngit clone https://github.com/shadabshaukat/llm-benchmark.git\n\ncd llm-benchmark\n\npython3.11 -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n# Ollama가 실행 중인지 확인하세요 'ollama serve'\n\npython benchmark.py - verbose - prompts \"하늘이 파란 이유는 무엇인가요?\" \"Nvidia의 재무에 관한 보고서 작성\"\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-llama3:latest\n         프롬프트 평가: 40.44 t/s\n         응답: 30.13 t/s\n         총합: 30.45 t/s\n\n        통계:\n         프롬프트 토큰: 25\n         응답 토큰: 576\n         모델 로드 시간: 0.00초\n         프롬프트 평가 시간: 0.62초\n         응답 시간: 19.12초\n         총 시간: 19.75초\n---------------------------------------------------- \n```\n\n\n\n행복한 AI체험 되세요 :)","ogImage":{"url":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png"},"coverImage":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png","tag":["Tech"],"readingTime":3},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>Let's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.</p>\n<p><img src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png\" alt=\"Click here to see the image.\"></p>\n<p>Ollama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.</p>\n<p>It usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.</p>\n<p>대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.</p>\n<h2>1. '<a href=\"https://ollama.com/download/mac&#x27;%EC%9C%BC%EB%A1%9C\" rel=\"nofollow\" target=\"_blank\">https://ollama.com/download/mac'으로</a> 이동해주세요.</h2>\n<p><img src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png\" alt=\"이미지\"></p>\n<p>Zip 파일을 다운로드하고 압축을 풉니다.</p>\n<h2>2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.</h2>\n<p><img src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png\" alt=\"image\"></p>\n<h2>3. 터미널을 열고 다음과 같이 입력해주세요.</h2>\n<pre><code class=\"hljs language-js\">ollama run llama3\n</code></pre>\n<p>그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.</p>\n<p><img src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png\" alt=\"image\"></p>\n<p>브라우저에서 <a href=\"http://localhost:11434/%EC%9D%84\" rel=\"nofollow\" target=\"_blank\">http://localhost:11434/을</a> 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.</p>\n<h2>4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.</h2>\n<p>마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.</p>\n<pre><code class=\"hljs language-js\">vim ~/.<span class=\"hljs-property\">zshrc</span>\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop=<span class=\"hljs-string\">'osascript -e \"tell application \\\"Ollama\\\" to quit\"'</span>\nalias ollama_start=<span class=\"hljs-string\">'ollama run llama3'</span>\n\n#새 세션을 열고 아래 명령어를 사용하여 <span class=\"hljs-title class_\">Ollama</span> 시작 및 중지\n\nollama_start\nollama_stop\n</code></pre>\n<h2>5. Llama3 성능 평가</h2>\n<pre><code class=\"hljs language-js\">git clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/shadabshaukat/llm-benchmark.git</span>\n\ncd llm-benchmark\n\npython3<span class=\"hljs-number\">.11</span> -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.<span class=\"hljs-property\">txt</span>\n\n# <span class=\"hljs-title class_\">Ollama</span>가 실행 중인지 확인하세요 <span class=\"hljs-string\">'ollama serve'</span>\n\npython benchmark.<span class=\"hljs-property\">py</span> - verbose - prompts <span class=\"hljs-string\">\"하늘이 파란 이유는 무엇인가요?\"</span> <span class=\"hljs-string\">\"Nvidia의 재무에 관한 보고서 작성\"</span>\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-<span class=\"hljs-attr\">llama3</span>:latest\n         프롬프트 평가: <span class=\"hljs-number\">40.44</span> t/s\n         응답: <span class=\"hljs-number\">30.13</span> t/s\n         총합: <span class=\"hljs-number\">30.45</span> t/s\n\n        통계:\n         프롬프트 토큰: <span class=\"hljs-number\">25</span>\n         응답 토큰: <span class=\"hljs-number\">576</span>\n         모델 로드 시간: <span class=\"hljs-number\">0.00</span>초\n         프롬프트 평가 시간: <span class=\"hljs-number\">0.62</span>초\n         응답 시간: <span class=\"hljs-number\">19.12</span>초\n         총 시간: <span class=\"hljs-number\">19.75</span>초\n---------------------------------------------------- \n</code></pre>\n<p>행복한 AI체험 되세요 :)</p>\n</body>\n</html>\n"},"__N_SSG":true}