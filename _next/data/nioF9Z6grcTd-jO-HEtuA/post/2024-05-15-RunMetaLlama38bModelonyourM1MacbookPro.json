{"pageProps":{"post":{"title":"M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요","description":"","date":"2024-05-15 23:37","slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro","content":"\n\nLet's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\n\n![Click here to see the image.](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png)\n\nOllama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\n\nIt usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\n\n\n\n대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\n\n## 1. 'https://ollama.com/download/mac'으로 이동해주세요.\n\n![이미지](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png)\n\nZip 파일을 다운로드하고 압축을 풉니다.\n\n\n\n## 2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png)\n\n## 3. 터미널을 열고 다음과 같이 입력해주세요.\n\n```js\nollama run llama3\n```\n\n\n\n그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png)\n\n브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.\n\n## 4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\n\n\n\n마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\n\n```js\nvim ~/.zshrc\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop='osascript -e \"tell application \\\"Ollama\\\" to quit\"'\nalias ollama_start='ollama run llama3'\n\n#새 세션을 열고 아래 명령어를 사용하여 Ollama 시작 및 중지\n\nollama_start\nollama_stop\n```\n\n## 5. Llama3 성능 평가\n\n```js\ngit clone https://github.com/shadabshaukat/llm-benchmark.git\n\ncd llm-benchmark\n\npython3.11 -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n# Ollama가 실행 중인지 확인하세요 'ollama serve'\n\npython benchmark.py - verbose - prompts \"하늘이 파란 이유는 무엇인가요?\" \"Nvidia의 재무에 관한 보고서 작성\"\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-llama3:latest\n         프롬프트 평가: 40.44 t/s\n         응답: 30.13 t/s\n         총합: 30.45 t/s\n\n        통계:\n         프롬프트 토큰: 25\n         응답 토큰: 576\n         모델 로드 시간: 0.00초\n         프롬프트 평가 시간: 0.62초\n         응답 시간: 19.12초\n         총 시간: 19.75초\n---------------------------------------------------- \n```\n\n\n\n행복한 AI체험 되세요 :)","ogImage":{"url":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png"},"coverImage":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png","tag":["Tech"],"readingTime":3},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h2: \"h2\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Let's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png\",\n        alt: \"Click here to see the image.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"It usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"1. 'https://ollama.com/download/mac'으로 이동해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Zip 파일을 다운로드하고 압축을 풉니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3. 터미널을 열고 다음과 같이 입력해주세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"ollama run llama3\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \\\"Ollama is running\\\"이 표시된다면 정상적으로 작동 중입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"vim ~/.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"zshrc\"\n        }), \"\\n\\n#파일에 아래 두 줄 추가\\n\\nalias ollama_stop=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'osascript -e \\\"tell application \\\\\\\"Ollama\\\\\\\" to quit\\\"'\"\n        }), \"\\nalias ollama_start=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ollama run llama3'\"\n        }), \"\\n\\n#새 세션을 열고 아래 명령어를 사용하여 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \" 시작 및 중지\\n\\nollama_start\\nollama_stop\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"5. Llama3 성능 평가\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"git clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/shadabshaukat/llm-benchmark.git\"\n        }), \"\\n\\ncd llm-benchmark\\n\\npython3\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".11\"\n        }), \" -m venv venv\\n\\nsource venv/bin/activate\\n\\npip install -r requirements.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"txt\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \"가 실행 중인지 확인하세요 \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ollama serve'\"\n        }), \"\\n\\npython benchmark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"py\"\n        }), \" - verbose - prompts \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"하늘이 파란 이유는 무엇인가요?\\\"\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Nvidia의 재무에 관한 보고서 작성\\\"\"\n        }), \"\\n\\n----------------------------------------------------\\n\\n평균 통계:\\n\\n----------------------------------------------------\\n        dolphin-\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama3\"\n        }), \":latest\\n         프롬프트 평가: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.44\"\n        }), \" t/s\\n         응답: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"30.13\"\n        }), \" t/s\\n         총합: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"30.45\"\n        }), \" t/s\\n\\n        통계:\\n         프롬프트 토큰: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"25\"\n        }), \"\\n         응답 토큰: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"576\"\n        }), \"\\n         모델 로드 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.00\"\n        }), \"초\\n         프롬프트 평가 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.62\"\n        }), \"초\\n         응답 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"19.12\"\n        }), \"초\\n         총 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"19.75\"\n        }), \"초\\n---------------------------------------------------- \\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"행복한 AI체험 되세요 :)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}