{"pageProps":{"posts":[{"title":"사람들이 AI의 변혁적 힘에 대해 오해하는 것들","description":"","date":"2024-07-01 20:36","slug":"2024-07-01-WhatEveryoneGetsWrongabouttheTransformativePowerofAI","content":"\n\n## AI가 사회를 변화시킬 것이지만, 지금까지의 변화는 예상과는 다를 수 있음을 사회는 보여주고 있습니다. 그 변화가 제어되지 않으면 지불해야 할 비용이 과도할 수도 있습니다.\n\n![image](/assets/img/2024-07-01-WhatEveryoneGetsWrongabouttheTransformativePowerofAI_0.png)\n\n한 달 전 존 스튜어트가 AI로 인해 사람들이 일자리를 잃는다는 세그먼트를 했었죠. 그는 그에 반대했습니다. 사실 그의 말은 반대했지만, 깊은 곳에서는 그것을 지지하고 있고, 당신도 마찬가지입니다. 아니면 아직 모르고 있을 뿐일지도 모르겠네요.\n\n현재의 최첨단 기술, AI의 대규모 언어 모델에 대해 TV에서 이야기할 수 있는 것 자체가 이전 기술이 일자리를 없애 버렸기 때문입니다. 대부분의 일자리, 거의 모든 일자리가 사라졌습니다. 인류 역사의 대부분에서 80-90%의 사람들은 농부였습니다. 그들 중 소수는 대장간, 재봉사 등 핵심 직업을 가졌습니다. 하지만 그들이 가지지 않았던 것은 TV 개성, TV 임원 또는 심지어 TV였습니다.\n\n<div class=\"content-ad\"></div>\n\n수백 년 전에 태어났더라면, 아마 농부가 되었을 겁니다. 또한 확실히 감염으로 사망했을 겁니다. 그러나 과학 기술 발전 덕분에 더 적은 농부가 필요했고, 그 결과로 역학 및 기술 혁신을 통해 역병과 같은 감염의 치료법을 발견, 제조 및 보급할 수 있는 의사와 과학자가 나타났습니다. 과학 기술 혁신은 과학 기술 혁신을 낳습니다. 생성적 AI는 그저 현재의 최신 기술입니다 (우리가 다른 앞날 본기술들을 가지고 있기 때문에 제일뿐).\n\n하지만, 모든 것이 순조롭게 이루어진다는 것은 아닙니다. 많은 기술 CEO들이 AI의 큰 긍정적 영향에 대해 이야기하지만, 안타깝게도 시간이 필요할 겁니다. 자동차를 생각해 보십시오; 1886년에 독일에서 칼 벤츠가 자동차를 특허했습니다. 미국에 자동차가 등장한 것은 대략 15년 후인데 그때까지 미국에는 8,000대밖에 없었습니다. 1910년까지 빠르게 성장하여 500,000대에 이르렀습니다. 그런데도 그것은 25년이 걸리게되고 실제로 미국 인구의 약 절반 이상만이 자동차를 소유했습니다. 처음으로 정지 신호등이 사용된 건 1915년 이후였습니다 (물론 중요한 교차로에 신호등은 조금 더 일찍 설치되었습니다). 포인트는, 사람들과 기업들이 자동차를 광범위하게 도입하기 전에 수십 년이 걸렸으며, 이는 견공적 규정과 사회적 규범을 찾는 시간을 주었습니다.\n\n다른 한편으로, 소셜 미디어는 2008년 Facebook이 실질적으로 성장하기 시작할 때까지 거의 사용되지 않았습니다. 얼마 지나지 않아 몇 백만 명의 사용자가 4년 만에 10억 명으로 증가했습니다. 이후 소셜 미디어는 사이버 괴롭힘, 자존감 문제, 신체 이미지 문제, 우울증 및 많은 다른 정신 건강 문제를 야기했습니다. 미리 정찰된 위험과 부정적 영향에 대한 데이터가 애매하게 거론되었습니다. 반면 자동차의 경우, 사람들은 일찍부터 위험을 보았고 정찰적인 예방 조치를 취할 수 있었으며, 정찰적 예방 조치를 취하기 전에 사람들을 위한 정지 신호 또는 운전 면허를 요구하듯이 상쇄적인 조치를 취할 수 있었습니다. 우리가 모든 것을 올바르게 이해한 것이라고 말하기에는 이르지 못했습니다. 1980년대까지는 그렇지 않았습니다. 극단 운전 반대운동을 이끈 Candy Lightner와 같은 사람들 때문에 우리는 술취한 운전에 맞섰고 바로세웠습니다.\n\n핵무기는 1945년으로 거슬러 올라갑니다. 20세기 대부분의 시기에는 미국, 러시아, 영국, 프랑스, 중국이 무기에 접근할 수 있었지만, 나중에 인도가 이 클럽에 가입했습니다. 이 무기는 세계에 건전한 위험을 초래했습니다. 그러나 사용할 권한을 가진 사람들은 규제를 갖추고 있었습니다. 그들의 정부는 일반적으로 잠재적인 부정적 결과, 적어도 그들에게는 그리 심오하지 않을 것이지만, 세계를 이해하고 사용을 소극적하게 해왔습니다. 북한이 덜 성숙한 지배력을 가지고 있다는 우려도 있습니다. 그러나 사실은 김정은이 핵탄두를 사용하면 핵보복을 유인할 것을 알고, 이는 그의 사후 전망을 (그가 생존하더라도) 훨씬 악화시킬 것임을 알고 있기 때문에 이를 자제하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n테러조직이 그것을 얻었다면 어떨까요? 김정은은 죽거나 나라가 파괴되는 걸 원하지 않아요. 전투에서의 죽음이 현재 상황보다 나은 것이라고 믿는 사람들은 어떻게 될까요? 그들은 자신이 순교자로 죽는 것이 긍정적 결과라고 생각하기 때문에 핵미사일을 발사할 것입니다. 이것은 게임 이론 101입니다. 시장에 더 빨리 도착하면 차량을 이용하여 비용을 절약할 수 있습니다. 시장으로 향하는 도중 다른 사람의 닭을 가끔 치기도 하지만(법에 저촉되는 것이 없다고 가정하면서), 왜 하지 않을까요? 여러분에게 이익이 있고, 부정적인 결과는 다른 누군가에게 남기는 것입니다.\n\n이 같은 논리는 AI에도 적용됩니다. 어떤 사람이든 더 긍정적인 결과를 창출할 것으로 생각되는 도구를 사용할 것입니다. 결국 목적이 수단을 정당화한다고 생각한다면 소셜 미디어에 가짜 정보를 공유하거나 AI를 사용해 만든 정보도 정당화됩니다. 이에 동의하지 않더라도, AI와 다른 도구 사용에 있어서 선하고 도덕적일 것을 바라더라도 결과에 완전히 동의할 수 있나요? 뉴욕시에 아는 여성 대부분은 소셜 모임에서 찍은 내 사진을 게시하기 전에 확인하고 승인해야 합니다. 그들은 악의를 품는 것이 아니라 소셜 미디어에 멋있게 보이려고 하는 것뿐입니다. 하지만 이런 행동으로 인해 모든 사람들이 이를 하게 되면 온라인 사진이 평균 이상으로 보이도록 바이어스가 생기고(머리, 메이크업, 드레스, 조명, 각도가 모두 조화를 이룰 때 그들이 사진을 승인합니다), 이는 10대 소녀들의 몸에 대한 이미지 문제를 유발합니다. 누구도 10대 소녀를 해치려고 했던 것이 아니지만, 예상치 못한 외부성이었습니다. 여기에는 더 많은 예시가 있습니다.\n\nAI는 핵무기는 아니지만 해를 끼칠 수 있는 도구입니다. 수십 년이 걸리던 이전 기술과는 다르게 AI 도입은 더 빠르게 진행되고 있습니다. 제품의 안전 경고를 아무도 읽지 않는다고 지적하고 싶지만, 아직 AI에 대해 완전히 이해하지 못해 안전 경고조차 완전하지 못합니다. 만약 1900년에 50%의 미국인이 세 달 동안 자동차에 접근할 수 있었다면(돈과 연료 인프라를 포함하여), 어떤 규정이 필요한지 알아내기 전에 얼마나 많은 사망자와 문제가 발생했을까요?\n\n스타플릿의 주요 지침은 기술 때문에 자신들을 해칠 위험이 있는 성장이 덜 한 종족을 보호하기 위해 시행되었습니다. 외부 기술만이 일으키는 위협이 아니라는 것은 명확하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제는 실제로 인공지능이 주된 주제가 아닙니다. 이는 우리가 이해하기에 속도가 빠른 충분히 영향력 있는 기술에 대한 이야기입니다. 세 가지 요인이 작용합니다. 첫 번째는 채택 속도로, 기술이 얼마나 빨리 적용되는지를 나타냅니다. 두 번째는 영향 반경입니다: (영향의 규모) x (영향을 받는 사람 수) x (영향 지속 기간). (영향 반경이 물리적인 거리가 아닌 개념적인 공간에서의 거리임을 기억하세요.)\n\n세 번째는 학습 곡선으로, 영향을 얼마나 빨리 이해할 수 있는지를 의미합니다. 영향을 이해하든 그렇지 않든 (학습 곡선 상 어디에 있는지에 관계없이) 영향은 실제로 발생합니다. 사람들은 소셜 미디어로부터 신체 이미지 문제로 고통받았지만, 아직 그 사실을 알지 못했습니다.\n\n영향 반경이 작으면 기술을 시험해보고 위험을 제한할 수 있습니다. 스페이스 셔틀은 훌륭한 기술이었지만, 위험을 수반했습니다. 챌린저와 콜롬비아의 파괴로 일부 우주 비행사 (그리고 돈)를 잃었습니다. 삶의 손실을 경시하려는 의도는 아니지만, 손실은 최소였고 그 위험을 이해하는 한정된 인원에 의해 진행되었습니다.\n\n채택 속도가 영향 반경을 초과할 때, 특히 학습 곡선 상승 속도보다 빨라질 때, 우리는 과도한 위험에 직면합니다; 우리는 효과를 이해하기보다 더 빨리 만들어냅니다. 이는 재앙의 요소가 됩니다. 집에서 일어날 때 빠르게 움직이고 물건을 부수는 것은 괜찮습니다. 그러나 1200파운드의 자동차를 40마일(모델 T의 전형적인 속도)로 길을 달릴 때 이 철학을 정당화하기는 어렵습니다. 이런 상황이 동네의 절반 이상이 함께 행동할 때 상황은 훨씬 어려워집니다. 불행히도, 인공지능도 소셜 미디아와 같이 독립적인 집안에서 제한돼있지 않아서, 그 위험이 오로지 당신에게만 영향을 미치는 것이 아닙니다.\n\n<div class=\"content-ad\"></div>\n\nWhen X-rays were first discovered, they seemed like something out of a sci-fi film. Back in the 1920s, shoe stores even used fluoroscopes to help customers see how well shoes fit by showing X-ray images of their feet in the shoes. Unfortunately, at the time, not many people were aware of the health risks posed by radiation. It wasn't until years later, after many had been exposed unnecessarily, that we truly understood the long-term effects. By then, countless individuals had suffered from overexposure to radiation.\n\nIn contrast, the telephone quickly gained popularity in the US during the first half of the 20th century. Fortunately, the phone itself didn't pose serious health risks, limiting its impact zone. (Although, a real concern back then was neighbors eavesdropping on party-lines without understanding how they functioned!)\n\nReflecting on the industrial revolution, it's evident that rapid innovations often led to environmental damage. Land was strip-mined, mountains deforested, and water and air polluted. The infamous 1969 Cuyahoga River fire is a stark example—yes, a river was actually on fire due to pollution. Shockingly, this river had caught fire multiple times, but not much was done about it. As David Newton noted in his book \"Chemistry of the Environment\" (p. 6), such environmental harm was once viewed as a sign of progress. How much societal harm are we willing to accept for advancement?\n\nArtificial Intelligence (AI) offers incredible transformations, but we must proceed cautiously until we grasp its risks fully. The existence of risks shouldn't deter us from using AI; after all, cars also come with risks, yet we prioritize the benefits. The difference lies in our understanding of cars and their risks, which evolved alongside their widespread use. AI remains less understood. Yes, we should start using it to uncover risks, but we shouldn't hand over the keys to 500,000 inexperienced drivers overnight. It's simpler to relax precautions later than to impose them afterward (particularly when profitable corporations influence politics to resist regulations, as seen with the military-industrial complex, tobacco, oil, and Big Tech).\n\n<div class=\"content-ad\"></div>\n\n혁신에는 찬성하지만 신중히 진행해야 한다고 생각해요. 인더스트리얼 혁명 초기에 환경에 어떤 피해를 줄 수 있는지 제한을 두었더라면 어땠을까요? 전쟁 중에는 자원이 제한되어 있지만 사회는 혁신합니다. 월스트리트를 규제하면 파티가 끝났다고 불평하지만 새로운 방법으로 수익을 창출하는 혁신을 합니다. 규제는 혁신을 막지 않고 때로는 오히려 영감을 줍니다. 인더스트리얼 혁명을 풀어놓고 오늘날 대가를 치르고 있습니다. 최근에는 소셜 미디어가 사회를 \"변형\"시켰지만 항상 긍정적이지는 않았어요. 같은 실수를 반복하지 말고 AI가 무제한으로 혁신하게 해둔 채 나중에 규제하는 것은 좋지 않아요. 지니는 쉽게 병자에 넣을 수 없습니다. 우리는 과거의 실수를 반복하지 않기 위해 역사를 연구합니다. 이번 시험에서 우리의 실력을 확인해봅시다.\n\n이 글은 https://www.thecareertoolkitbook.com에서 원문이 게시되었습니다.\n\n마크 A. 허쉬버그는 CTO이자 MIT 강사, 연설가이자 The Career Toolkit: Essential Skills for Success That No One Taught You 저자, Brain Bump 앱의 창작자입니다.\n\n<div class=\"content-ad\"></div>\n\n이 컬럼은 직업에 관한 것이에요. 또한 그는 Medium에서 미디어에 관해 @cognoscomedia로 글을 씁니다. ","ogImage":{"url":"/assets/img/2024-07-01-WhatEveryoneGetsWrongabouttheTransformativePowerofAI_0.png"},"coverImage":"/assets/img/2024-07-01-WhatEveryoneGetsWrongabouttheTransformativePowerofAI_0.png","tag":["Tech"],"readingTime":8},{"title":"GPT 프롬프트 엔지니어링 비밀 해독 효과적인 프롬프트 작성법 알아보기","description":"","date":"2024-07-01 17:54","slug":"2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering","content":"\n\n효과적인 프롬프트를 작성하는 것은 상당히 어려운 작업일 수 있습니다. 어떤 사람들은 다른 사람들보다 GPT를 통해 더 나은 결과를 얻는 것 같습니다. 정확히 떠오르는 것을 달성하는 효과적인 프롬프트를 작성하려면 GPT 모델이 어떻게 작동하는지를 이해해야 합니다. ChatGPT, Claude, DALL-E, Firefly, Stable Diffusion 또는 Llama2가 어떻게 프롬프트를 해독하고 사용하는지에 대한 기본적인 이해가 필요합니다.\n\n![Decoding Prompts: Unveiling The Secrets Of GPT Prompt Engineering](/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_0.png)\n\n이 기사에서는 프롬프트 엔지니어링의 비밀과 다양한 모델이 프롬프트를 처리하는 방법을 단계별로 살펴볼 것입니다. 나는 또한 수학적 배경에 들어가지 않고 일반적인 영어로 설명할 것입니다. 이 기사의 의도는 여러분이 이러한 모델에 대한 튼튼한 이해를 가지고 더 효과적으로 프롬프트를 제시할 수 있도록 하는 것입니다. 이 기사는 너무 이론적이거나 구체적하지 않고 실용적인 가이드이며 효과적인 프로팅에 대한 손쉬운 안내서입니다.\n\n# GPT가 여러분의 프롬프트를 처리하는 방법\n\n<div class=\"content-ad\"></div>\n\n이 문맥에서 AI 모델을 언급할 때, Generative Pre-trained Transformer 또는 GPT라고 합니다. 여러분이 텍스트를 제공하면 새로운 텍스트, 이미지, 비디오 또는 오디오 스트림이 될 수 있는 결과물을 반환합니다. 훈련된 내용에 따라 다양합니다.\n\n## 텐서란 무엇이며 GPT \"뇌\"의 구조\n\n먼저, GPT는 인간과 같은 존재가 아닙니다. 그러나 가까워 보이는 결과물을 생성할 수 있습니다. HuggingFace나 Civitai와 같은 오픈 소스 모델들은 그들의 정보를 Tensor들로 구성된 GGUF 또는 Safetensors 파일 형식으로 저장합니다. 모든 GPT는 어느 형태로든 Tensor를 사용합니다.\n\n이제 GPT를 3차원의 큰 거미줄 텐서로 상상해보세요. 작은 \"상자\"로 된 데이터가 담긴 텐서들이 있습니다. 이러한 작은 상자들은 GPT의 훈련 과정의 결과물입니다. 각 작은 \"상자\"(텐서)는 다른 것과 연결되어 있으며 모두 서로 연결되어 있습니다. 일부는 직접적으로 서로 연결되어 있고, 다른 것은 동료를 통해 간접적으로 연결됩니다. 이 거미줄이 GPT 모델의 \"뇌\"를 형성합니다.\n\n<div class=\"content-ad\"></div>\n\n**GPT의 \"뇌\"가 사용되는 방식**\n\nGPT의 \"뇌\"는 큰 넷이며(신경망과 혼동하면 안 됩니다!), 각 GPT마다 다른 유형의 넷을 가지고 다른 방식으로 처리합니다. 당신의 프롬프트에 기반하여, 그 프롬프트와 관련이 가장 있는 \"뇌\" 또는 \"넷\"의 섹션을 식별합니다.\n\n![이미지1](/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_1.png)\n\n<div class=\"content-ad\"></div>\n\n제공하는 프롬프트 텍스트를 GPT에 보내면 먼저 텍스트를 토큰으로 분할합니다. 토큰은 서로 관련된 작은 텍스트 조각입니다. 마치 학교에서 문장의 문법을 분석했던 것과 비슷합니다. 이러한 토큰들로 GPT는 특정 토큰(또는 \"단어\")의 중요성과 서로의 관련성을 평가합니다. 이 \"구조화된 프롬프트\"를 정보 상자의 작은 네트워크로 상상할 수 있습니다. 즉, 텐서입니다.\n\n그런 다음 GPT는 \"구조화된 프롬프트\"를 사용하여 가장 관련된 다른 상자를 찾아 \"확산\"이라는 과정에서 그들을 섞습니다. 이 과정을 설정된 사이클 수의 최대치에 도달할 때까지 여러 번 반복합니다. 각 사이클에서 이전 결과도 평가합니다. 정의된 사이클 수가 완료되면 결과를 사용자에게 반환합니다. 결과의 지각된 품질과는 무관하게요.\n\n# 텐서를 타겟팅하여 효과적으로 프롬프팅하기\n\n이제 실용적인 부분으로 넘어가보겠습니다. 대부분의 경우, 엔드 결과의 어떤 흐릿한 상상을 머리 속에 가지고 GPT에게 프롬프트를 제시합니다. 여러분의 상상력은 이전에 보거나 배운 것을 기반으로 뇌가 만든 것입니다. 이제 그 상상력을 여러분의 뇌에서 GPT의 뇌로 옮겨서(이상적으로는 영어로) 언어를 사용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 정보 학습이 완료되었는지 확인하세요\n\n큰 모델인 GPT-4, Claude 3 또는 Mixtral과 같은 경우 정보 부족은 문제가 되지 않지만, 작은 오픈 소스 모델은 문제가 될 수 있습니다. 모델이 필요한 모든 데이터를 학습했는지 먼저 확인해야 합니다. 훌륭한 프롬프트를 전달했지만 그 \"뇌\"에 참조하고 있는 정보가 없으면 출력 품질이 원하는 형태가 아닐 수 있습니다. 완전한 프롬프트를 만들기 전에 모델이 찾고 있는 데이터가 있는지 확인하기 위해 작은 프롬프트를 시도해 보세요.\n\n![image](/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_3.png)\n\nRealVisXL30이 Star Trek과 Mr. Bean과 관련된 다양한 훈련 데이터뿐만 아니라 수백만 장의 이미지를 가지고 있었다는 것에 주목하세요. \"Mr. Bean이 Star Trek의 선장인 모습\"을 입력하자 마자 원하는 이미지가 나오는 것은 아닙니다. 프롬프트 주변의 추가 키워드와 가중치는 명확하게 RealVisXL30 모델을 원하는 이미지로 이끌 텐서를 지정합니다. 대부분은 \"훈련 데이터 추측\"이며 모델내에 특정한 스타일이 있다고 가정하고 있으니 이를 고려하세요.\n\n<div class=\"content-ad\"></div>\n\n## 가중치 정의 및 특정 텐서 지정\n\n우리의 Prompt에는 \"여행 가이드\"라는 용어가 포함되어 있지 않아요. 그럼에도 불구하고, GPT가 학습한 방대한 양의 여행 가이드 데이터를 선택하는 것을 방지할 수 있죠. 이는 Karl Baedeker 스타일 프롬프트를 완전히 무력화시키기 때문이에요. 이러한 발견은 모델을 철저하게 테스트하는 것이 필요합니다.\n\n이 특정한 행동은 모든 주요 모델 (GPT-4, Claude 3 및 Gemini)에서 관찰할 수 있으며 매튜 효과라고 불립니다. 프롬프트를 제대로 설계하고 모델과 계속 테스트하는 대안은 없습니다.\n\n<div class=\"content-ad\"></div>\n\n## 환각 효과를 이해하고 다루기\n\n모델이 프롬프트와 일치하는 데 충분한 데이터가 없거나 데이터 내에서 특정 텐서로의 유의한 편향을 만드는 데이터 포인트가 있는 경우, \"환각\"으로 일반적으로 설명되는 효과가 나타납니다 (매튜 효과로 인해 \"GPT 환각\"이 발생할 수 있습니다). 이러한 경우와 결과에 대해 저의 \"대규모 언어 모델 훈련이 막다른 곳에 다다랐을 때\" 라는 기사에서 많이 설명했습니다.\n\n환각 효과를 극복하는 몇 가지 방법이 있습니다.\n\n- 가장 간단한 방법은 프롬프트의 변화를 검토하고 환각을 유발할 수 있는 요소를 제거하는 것입니다.\n- 프롬프트의 축소가 환각의 감소로 이어지지 않는 경우, 처음부터 프롬프트 엔지니어링을 다시 시작합니다.\n- 모델이 계속해서 환각을 일으키는 경우, 해당 작업에 대한 훈련이 부족한 것입니다. 새로운 모델이 필요하거나 RAG (검색 증강 생성)을 확장해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n지난번에 우리가 가진 그 그물 그림을 떠올려봐. 환각은 당신의 프롬프트가 GPT를 상자들의 그 그물 가장자리 쪽으로 이끌 때 발생합니다. 본질적으로는 그 상자에 대한 지식이 없는(훈련되지 않은) 특정한 콘텐츠를 요청하거나 그 상자가 다른 상자들에 가려져 있을 때(다른 정보에 편향; 매튜 효과)입니다.\n\n![Decoding Prompts: Unveiling The Secrets Of GPT Prompt Engineering](/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_4.png)\n\n## 좋은 프롬프트 기반 설계 및 유지 관리\n\n사용하는 프롬프트 텍스트들은 어떤 측면에서 컴퓨터 소프트웨어의 소스 코드와 비교될 수 있습니다. 당신은 프롬프트를 추적하고 이상적으로는 버전을 관리하고 싶을 것입니다. 가장 좋은 프롬프트를 정리하고 정렬하기 위해 GitHub 저장소를 활용하는 것은 확실히 좋은 방법입니다. 개인적으로는 나중에 사용하고 싶은 이미지와 프롬프트를 저장하기 위해 Civitai를 사용합니다. 정말로 여러분의 선택입니다.\n\n<div class=\"content-ad\"></div>\n\n\n# 매거진 같은 이미지 제작을 위한 나만의 프롬프트\nRealVisXL30과 JuggernautXL로 고품질 이미지 생성을 위한 나만의 처음부터 시작하는 지침서\n\n## 프롬프트\n((거장)), 수상 경력을 자랑하는 영화 장면의 사진. **여기에 장면 설명**. 어두운 메이크업, 매우 디테일한 사진, 부드러운 빛. 생동감 넘치고 아름다우며 화가적이며 디테일하며 질감 있는 예술적. 피사관에서의 얕은 초점, 비네트, 매우 디테일한, 높은 예산, 보케, 시네마스코프 형식, 우울하고 웅장하며 아름다운, 필름 미세먼지, 질감 있는. ((보그 매거진)), ((NatGeo 인물)), ((Conde Nast))\n\n## 부정적인 프롬프트\n저품질, 최악의 품질, 나쁜 품질, 낮은 해상도, 나쁜 사진, 나쁜 예술, 나쁜 해부학, 나쁜 손, 서명, 텍스트, 오류, 자르기, JPEG 아티팩트\n\n\n각 모델의 \"정보 네트워크\"가 다른 데이터를 기반으로 만들어졌기 때문에 새 모델과 친숙해지는 것이 중요합니다. Claude 3은 GPT-4나 Gemini와 거의 완전히 다르며, 이미지나 비디오를 생성하는 다양한 GPT를 사용할 때 이는 더욱 명확합니다.\n\n\n<div class=\"content-ad\"></div>\n\nGPTs의 내부 작업은 복잡하며, 이 기사는 그들이 어떻게 작동하는지 대략적으로 다루었습니다. 이 통찰력들은 아마도 프롬프트 작성에 대한 더 나은 이해를 제공할 것입니다. 그러나 여전히 프롬프트 엔지니어링 업무를 진행해야 합니다. 몇몇 사람들은 \"프롬프트 엔지니어링\"을 운동으로 놀려왔지만, 원하는 결과의 품질에 따라 다소 시간이 많이 소요되는 작업일 수 있습니다.\n\n효과적인 프롬프트의 측면을 요약해 보겠습니다.\n\n- 모델이 사용 사례를 다룰 수 있는지 확인하세요.\n- 프롬프트를 조정하여 원하는 텐서를 대상으로 설정하세요.\n- 결과를 미세 조정하기 위해 프롬프트의 가중치를 조절하세요.\n- 보일러플레이트 프롬프트 템플릿의 모음을 유지하세요.\n\n현재 Claude, GPT-4, Gemini, DALL-E, Firefly 등 상용 모델을 사용하고 계시지만 편견 없는 개방형 소스 모델을 탐색하고 싶다면, 제가 쓴 \"컴퓨터에서 무제한 채팅 및 이미지 생성\" 기사를 강력히 추천합니다. Ollama와 Stable Diffusion은 지갑이 소모되는 것을 우려하지 않고 프롬프트 엔지니어링 스킬을 향상시키는 데 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n고맙다구요. 읽어주셔서 감사합니다.\n\n당신의 것, Jan.","ogImage":{"url":"/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_0.png"},"coverImage":"/assets/img/2024-07-01-DecodingPromptsUnveilingTheSecretsOfGPTPromptEngineering_0.png","tag":["Tech"],"readingTime":6},{"title":"AGI는 연속적인 스펙트럼입니다 이미 우리 곁에 와 있습니다 문제는 우리의 임계값 인식입니다","description":"","date":"2024-07-01 17:53","slug":"2024-07-01-AGIisacontinuousspectrumItsherealreadyItsusthathaveathresholdproblem","content":"\n\n지난 해 GPT-4를 'AGI의 발전'으로 평가한 Microsoft Research 팀에 동의합니다.\n\n자연어 이해(NLU) 연구자로서 정의들이 모두 조금 산만하고 대체로 불필요하다고 생각합니다.\n\n제 생각에는 이미 대체로 지능적이라는 것을 주장하기에 LLN이 기능적으로(프롬프트에 주입된 텍스트에 대한 응답으로 테스트된 것으로) 지능적이 아니라고 주장하는 것은 솔직치 못하다고 생각합니다.\n\n다중 모드인지 아닌지는 중요하지 않습니다. 시각장애인도 지능적입니다.\n\n<div class=\"content-ad\"></div>\n\n프리미엄 LLM이 기능적으로 똑똑하지 않다고 주장할 수 있는 근거는 무엇인가요?\n\n그리고 일반적으로 말이에요?\n\n제 생각에는, LLM은 본질적으로 모든 주제에 대해 매우 도움이 돼요!\n\n환각은 거의 문제가 되지 않죠, 왜냐하면 그것이 내가 제공하는 데이터를 어떻게 처리하는지에 집중하기 때문이에요.\n\n<div class=\"content-ad\"></div>\n\n오늘, 내가 원한다면, LLM 기반 앱에 장기기억력을 부여할 수 있고 (현대 거대 프롬프트 크기를 통해), 반복 호출 능력과 인터넷 양방향 액세스를 제공할 수 있습니다.\n\n그리고 그것에게 유용한 사이트를 생성하고, 관심사를 찾고, 인터넷에서 목표, 발견, 심지어 부를 창출하도록 프롬프트할 수 있을 거예요.\n\n그건 오늘날에 아주 가깝게 가능할 거에요.\n\n<div class=\"content-ad\"></div>\n\n여러 번의 시도를 통해 배운 것을 저장하고 성공적이었던 것과 그렇지 않은 것을 모두 기억하세요.\n\n그리고 이미 존재할 수도 있습니다.\n\n그저 더 견고하고 다중 모달로 발전할 것입니다.\n\n근본적으로 LLM 아키텍처가 우리를 그 곳으로 이끌어줄 수 있습니다. (이것은 더 나은 아키텍처가 없다는 것을 의미하는 것이 아닙니다. 그러나 LLMs는 충분합니다.)\n\n<div class=\"content-ad\"></div>\n\nGenerally agreed AGI에 도달하는 길은 대체로 LLMs를 통한 지속적 진화를 거쳐 이에 더해진 능력과 지능이 더해지는 것일 것입니다. 하지만 현재의 LLMs가 합리적으로 일반적으로 지능적이지 않다고 주장하는 것은 점점 무의미해지고 있는 것 같습니다. 측정 지표와 주관적인 증언이 이러한 것들이 기능적으로 지능적이라는 것을 너무나 많이 지원하고 있습니다.\n\nAGI인지 여부에 대한 거의 일반적인 기준을 가지고 있는 것은 우리 사람들입니다.\n\n하지만 우리가 그것을 볼 때 대부분 인식할 수 있을 것입니다. 우리(거의) 모두 합의하는 한계가 있을 것입니다.\n\n제 의견은 최상위 LLMs는 사실상 이미 AGI라는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n당신을 위해.\n\nLLM을 아무리 평균적으로 무능하다고 주장하는 것은 정말 어리석은 생각입니다. 🌟","ogImage":{"url":"/assets/img/2024-07-01-AGIisacontinuousspectrumItsherealreadyItsusthathaveathresholdproblem_0.png"},"coverImage":"/assets/img/2024-07-01-AGIisacontinuousspectrumItsherealreadyItsusthathaveathresholdproblem_0.png","tag":["Tech"],"readingTime":2},{"title":"게임 세계 마스터하기 강화 학습 심층 탐구","description":"","date":"2024-07-01 17:51","slug":"2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning","content":"\n\n요즘 AI 기술은 바둑이나 체스와 같은 게임에서 놀라운 성과를 보여주며 세계 최고인 인간 선수들을 능가하는 모습을 보여주고 있습니다. 이런 업적들은 기술의 급속한 발전을 강조합니다.\n\n이 분야에서의 중요한 발전 중 하나는 'ChatGPT'입니다. ChatGPT는 대인적 언어를 이해하고 활용하는 능력으로 유명한 큰 언어 모델입니다.\n\nChatGPT는 대체로 transformer와 감독 학습 기술에 기반을 두고 있지만, 인간 피드백으로 강화 학습을 통해 세밀하게 튜닝되었습니다. 이는 기계가 자신의 행동에서 배우고 경험에 기초해 개선하는 방법을 보여줍니다.\n\n이 글은 두 부분으로 나뉘어질 예정입니다. 이 부분에서는 강화 학습의 기본 이론과 PyTorch에서 REINFORCE 방법을 사용하여 Flappy Bird 게임을 플레이하는 동안의 과정을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 부분에서는 어드밴티지 액터-크리틱(A2C)과 현재 최신 알고리즘인 PPO와 같은 더 복잡한 강화 학습 알고리즘을 더 알아볼 거에요! 😃\n\n## 목차\n\n- 강화 학습\n- 환경\n- 에이전트\n- 보상\n- 가치 함수\n- Q 함수\n- 정책 기반 방법\n- REINFORCE\n  - REINFORCE의 단계\n  - 구현\n  - BirdAgent 및 훈련\n- 결론\n- 참고문헌\n\n# 강화 학습\n\n<div class=\"content-ad\"></div>\n\n강화 학습은 특정 목표를 달성하거나 성능 메트릭을 최대화하기 위해 환경과 상호 작용을 통해 행동 전략을 학습하는 기계 학습 방법입니다. 이는 환경, 에이전트, 보상 세 가지 주요 요소로 구성되어 있습니다.\n\n![이미지](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_0.png)\n\n- 에이전트는 환경으로부터 상태 (s_t)를 수신합니다\n- 해당 상태를 기반으로, 에이전트는 환경에 행동 (a_t)을 제공합니다\n- 환경은 새로운 상태 (s_t+1)로 이동하고 일부 보상 (r_t+1)을 에이전트에 제공합니다\n\n# 환경\n\n<div class=\"content-ad\"></div>\n\n환경은 에이전트와 상호 작용하는 시스템입니다. 에이전트가 만날 수 있는 모든 가능한 상태, 해당 상태에서 취할 수 있는 행동 및 그러한 행동의 결과를 정의합니다.\n\n환경은 에이전트의 행동에 대응하여 새로운 상태와 보상 정보를 제공함으로써 에이전트의 학습 과정을 이끌어내게 됩니다. 환경은 다음과 같은 정보를 정의합니다:\n\n- 행동 공간: 에이전트가 취할 수 있는 가능한 행동 집합을 정의합니다.\n- 관찰 공간: 에이전트가 관찰할 수 있는 모든 가능한 상태를 정의합니다. 이러한 상태는 에이전트가 다음 행동을 결정하기 위해 필요한 정보를 제공합니다. 상태 공간은 유한하거나 무한할 수 있으며 연속적이거나 이산적일 수 있습니다.\n- 보상 함수: 보상 함수는 에이전트의 행동에 대한 즉각적인 평가(보상)를 제공하며, 에이전트가 그 전략을 조정하여 더 높은 총 보상을 얻는 방법을 학습하도록 이끕니다.\n- 전이 역학: 전이 역학은 주어진 현재 상태 및 특정 행동 이후 환경이 새로운 상태로 이동할 확률을 설명합니다.\n\n# 에이전트\n\n<div class=\"content-ad\"></div>\n\n강화학습에서 에이전트는 환경을 관찰하고 결정을 내리며 행동을 실행할 수 있는 엔티티를 가리킵니다. 에이전트의 목표는 장기적으로 얻는 보상을 극대화하기 위해 주어진 환경 상태에서 최적의 행동을 선택하는 방법인 전략(정책)을 학습하는 것입니다.\n\n## 보상\n\n보상은 에이전트의 학습 프로세스를 안내하는 주요 신호로, 행동에 대한 피드백을 제공합니다. 이러한 보상은 긍정적이거나 부정적일 수 있으며, 에이전트가 목표를 달성하는 데 행동의 결과를 이해하는 데 도움을 줍니다.\n\n에이전트의 주요 목표는 다양한 상태에서 최적의 행동을 선택하기 위한 전략(정책)을 학습하여 시간이 흐름에 따라 총 보상을 최대화하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image Description](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_1.png)\n\nWhen we talk about Gt in reinforcement learning, we are referring to the total reward at timestep t, which is also known as the return. It is calculated by summing up the rewards from timestep t until the end of the episode (T). This total reward is adjusted with a discount rate, γ, which determines the significance of future rewards in relation to the reward at timestep t.\n\nThe discount rate γ ranges between 0 and 1. For instance, if we consider γ as 0.95, rewards r_t expected in the distant future are discounted by a factor of 0.95 raised to the power of (t-1).\n\nThis adjustment is made because the likelihood of achieving a state far into the future is lower, making the corresponding reward less impactful and, therefore, subject to a reduction for a practical assessment.\n\n\n<div class=\"content-ad\"></div>\n\n![Image](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_2.png)\n\n여기 보상을 계산하는 간단한 예제가 있어요. '뱀'과 '목표' 이미지는 각각 초기 상태와 종단 상태를 나타냅니다. 이제 목표에 도달하기 위한 두 가지 가능한 경로가 있습니다:\n\n경로 1: 직진해서 위로 올라가서 왼쪽으로 목표로 이동\n\n경로 2: 오른쪽으로 가서 위로 올라가서 목표로 이동\n\n<div class=\"content-ad\"></div>\n\n할인율 γ = 1일 때,\n\n- R(Path 1) = 2 + γ(-3) + γ²(0) + γ³(1) = 0\n- R(Path 2) = 1 + γ(0) + γ²(3) + γ³(0) = 4\n\n우리는 목표에 도달할 수 있는 서로 다른 경로가 있음을 볼 수 있지만, 각 경로가 매우 다른 보상 (0과 4)을 가져올 수 있다는 것을 알 수 있습니다. 따라서 총 보상을 극대화할 수 있는 최상의 경로를 찾는 것이 강화 학습(RL)의 목표입니다. 💪\n\n# 가치 함수\n\n<div class=\"content-ad\"></div>\n\n우리의 상태가 얼마나 잘 되어 있는지를 정의하려면 이를 결정하는 함수인 가치 함수가 필요합니다.\n\n본질적으로 이 함수는 그 상태에 있을 때의 예상 수익을 추정합니다. 이 함수는 특정 정책(에이전트가 따르는 전략) 하에서 특정 상태에 있을 때 장기적 이점을 제공합니다.\n\n![Image](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_3.png)\n\n위에서 정의된 가치 함수는 재귀 형태로 재평가될 수 있으며, 이는 상태 값의 재귀적 계산을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n![Image](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_4.png)\n\nIn the value function, we utilize the expected value of the transition probability P(a ,s→s′) to accommodate real-world situations where the outcome (state s′) might differ when the same action (a) is taken in the same state (s).\n\nThe symbol π represents the agent's policy, with π(a | s) indicating the probability of the agent choosing action a in state s.\n\nNow, let's engage in some practice~~ 🍎\n\n<div class=\"content-ad\"></div>\n\n### 이미지\n![Mastering Game Worlds: A Deep Dive into Reinforcement Learning](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_5.png)\n\n이 상황에서는 네 가지 상태가 있습니다. s1은 시작 상태이고 s2, s3 및 s4는 종료 상태입니다. 두 가지 동작, 'left' 또는 'right'를 선택할 수 있으며 각각의 상태 간의 전이 확률을 가집니다.\n\n## 다른 정책 하에서 상태 1의 가치:\n\n<div class=\"content-ad\"></div>\n\n- V(π = 항상 왼쪽) = 0.5 * 2 + 0.5 * 4 = 3\n- V(π = 항상 오른쪽) = 0.33 * 3 + 0.67 * 4 = 3.67\n- V(π = 50% 왼쪽, 50% 오른쪽) = 0.5 * V(왼쪽) + 0.5 * V(오른쪽) = 3.335\n\n이 예제는 전이 확률을 고려하여 (예: 행동 = 오른쪽: s3로 33%, s4로 67%) 동일한 행동이 매번 동일한 결과를 보장하지 않는 확률적 전이를 강조합니다.\n\n더 복잡한 예제를 살펴보겠습니다~~\n\n![MasteringGameWorldsADeepDiveintoReinforcementLearning_6](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_6.png)\n\n<div class=\"content-ad\"></div>\n\n간단히 말씀드리면 이 예에서는 전이 확률을 무시하고 에이전트가 처음에 '위' 또는 '아래' 작업을 선택할 수 있으며, 이후 상태에서는 에이전트가 항상 오른쪽으로 이동한다.\n\n(S1은 시작 상태이고, S6은 종단 상태이며, 종단 상태 값은 0)\n\nπ = 항상 위로, γ = 1인 경우\n\n- V(s6) = 0\n- V(s3) = V(s5) = 10 + γV(s6) = 10\n- V(s2) = 2 + γV(s3) = 12\n- V(s4) = 4 + γV(s5) = 14\n- V(s1) = 1 + γV(s2) = 13\n\n<div class=\"content-ad\"></div>\n\nFor 이타 = (50% 상승, 50% 하락), 감마 = 1\n\n- V(s6) = 0\n- V(s3) = V(s5) = 10 + 감마V(s6) = 10\n- V(s2) = 2 + 감마V(s3) = 12\n- V(s4) = 4 + 감마V(s5) = 14\n- V(s1) = (1 + 감마V(s2)) * 0.5 + (2 + 감마V(s4)) * 0.5 = 9.5\n\n# Q 함수\n\n실제로, 모든 상태의 가치가 알려진 경우, 에이전트가 각 타임스텝마다 가장 높은 가치의 상태로 이동하도록하여 누적 보상을 최대화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n하지만, 상태값을 계산하기 위해서는 전이 확률에 대한 지식이 필요한데, 이는 대게 알 수 없습니다. 그러므로 상태-행동 함수(Q 함수)를 사용하는 것이 더 나은 선택이 됩니다.\n\nQ 값은 경험을 통해 직접 학습할 수 있기 때문에 환경의 전이 확률을 알 필요가 없습니다.\n\n![이미지](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_7.png)\n\nQ 함수, 또는 행동-가치 함수는 상태 s에서 정책 π에 따라 행동 a를 취하는 가치를 정의합니다. Q 함수와 가치 함수의 차이는 Q 함수는 결정론적 행동의 가치를 특정하며, 이로써 정책 π(a∣s)에서 행동의 확률을 계산할 필요가 없어집니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 8](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_8.png)\n\nThe value function could also be seen as the sum of the Q function for all possible actions a.\n\n![Image 9](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_9.png)\n\nIn a value-based approach like Q-learning, a policy that always picks the action with the highest Q value for each state s ensures automatic maximisation of the total reward.\n\n\n<div class=\"content-ad\"></div>\n\n하지만 행동 공간이 매우 크거나 연속적인 경우, 모든 가능한 Q값을 계산하여 각 상태에 대한 최상의 행동을 식별하는 것은 현실적이지 않습니다. 😢\n\nPolicy-Based Methods\n\n![Policy-Based Methods](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_10.png)\n\n강화 학습의 정책 기반 방법은 가치 기반 방법(value-based methods)처럼 명시적으로 가치 함수를 계산하지 않고, 에이전트가 행동을 선택하는 정책을 직접 매개변수화하고 최적화합니다. 🌟\n\n<div class=\"content-ad\"></div>\n\n**카드 읽기 전문가입니다!**\n\n폴리시는 π(a∣s;θ)로 표시되며, 상태를 액션에 대한 확률 분포로 매핑하는 함수입니다. 여기서 θ는 폴리시의 매개변수를 나타내며 (예: 신경망의 가중치), \n\n# REINFORCE\n\nREINFORCE는 폴리시 그레디언트 방법 중 하나로, 폴리시를 최적화하는 방법입니다. 이 방법은 기대되는 반환의 그레디언트를 폴리시 매개변수에 대해 추정한 다음 해당 매개변수를 그레디언트 하강을 사용하여 조정합니다.\n\n![이미지](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_11.png)\n\n<div class=\"content-ad\"></div>\n\n**리인포스 단계**\n\n리인포스 방법에서는 고려된 행동의 품질을 나타내는 반환 (R(τ))에 비례하는 그래디언트 스케일이 사용됩니다. 한편, 그래디언트 자체는 해당 행동을 취할 확률의 로그로 표현됩니다.\n\n이 방법은 높은 반환을 제공하는 행동의 확률을 높이고, 낮은 반환을 가져오는 행동의 확률을 줄이려고 합니다.\n\nREINFORCE 방법에서는 실제 샘플을 사용하여 반환을 계산해야 합니다. 또한 Monte-Carlo 방법으로 얻은 전체 경로가 실행에 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n- 무작위 가중치로 네트워크 초기화하기\n- N개의 전체 에피소드를 실행하여 궤적 (s, a, r, s') 수집하기\n- 각 시간 단계마다 반환값 계산하기\n- 손실 함수 L = -R(τ)logπ(a|s) 계산하기\n- 모델 가중치 업데이트하기\n\n## 구현\n\n첫 번째 구현에서는 Gym 환경을 사용하여 FlappyBird 게임을 플레이합니다.\n\n![Mastering Game Worlds](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_12.png)\n\n<div class=\"content-ad\"></div>\n\n게임을 시작하기 전에 환경의 세부 사항을 이해하는 것이 매우 중요합니다. 구현하기 전에 매번 문서를 꼼꼼히 읽어주십시오.\n\n![image](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_13.png)\n\n관측값은 LIDAR 센서에 의해 수집된 180개의 데이터 포인트로 구성된 1차원 배열입니다. 에이전트는 아무 것도 하지 않는 것을 나타내는 0 또는 플래핑하는 것을 나타내는 1과 같은 두 가지 가능한 조치를 취할 수 있습니다.\n\n![image](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_14.png)\n\n<div class=\"content-ad\"></div>\n\nYou're also welcome to consider the entire game screen as the observation for the model. Check [here](#) for more details.\n\n## BirdAgent & Training\n\nThe BirdAgent plays a key role in decision-making based on the environment's state. It generates a probability distribution showing the chances of taking each available action.\n\nHence, the agent's number of actions, known as n_action, is equal to the environment's action space, set at 2.\n\n<div class=\"content-ad\"></div>\n\n마침내, 훈련 루프가 여기 있어요~~ ✋\n\n![Training Loop](https://miro.medium.com/v2/resize:fit:1400/1*_mmbMZnPoe96JeB90OrPsQ.gif)\n\n# 결론\n\n이 글에서는 보상 학습의 기본 원리인 가치 함수, Q 함수 등을 간단히 소개했습니다. 또한 정책 기울기 방법에서 REINFORCE 알고리즘을 소개하고 Flappy Bird 게임을 플레이하는 데 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 다음으로, A2C와 PPO라는 두 가지 정책 그래디언트 방법을 소개하고, Car Racing과 Snake 게임 두 가지에서 이를 구현해 보겠습니다.\n\n마지막으로, 본 글이 즐거우셨기를 바랍니다. AI 관련 기사를 더 많이 작성할 예정이며, 그 기본 원리를 설명하고 구현하는 방법 등을 다룰 것입니다.\n\n관심이 있으시다면, 제 팔로우 해주시면 감사하겠습니다. 👏 😁\n\n제 Medium: [link](link)\n\n<div class=\"content-ad\"></div>\n\n마이 리플 링크: [링크](link)\n\n![이미지](/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_15.png)\n\n# 참고 자료\n\n- Hugging Face: Deep RL Course\n- OpenAI Spinning Up: Introduction to RL","ogImage":{"url":"/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_0.png"},"coverImage":"/assets/img/2024-07-01-MasteringGameWorldsADeepDiveintoReinforcementLearning_0.png","tag":["Tech"],"readingTime":10},{"title":"PyTorch로 나만의 8비트 양자화기를 처음부터 만드는 방법 단계별 가이드","description":"","date":"2024-07-01 17:48","slug":"2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch","content":"\n\n8-bit 커스텀 양자화기를 PyTorch와 quantize facebook/opt-350m을 사용해 처음부터 만드는 단계별 접근법을 소개해 드릴게요. \n\n![How I built my own custom 8-bit Quantizer from scratch: a step-by-step guide using PyTorch](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_0.png)\n\nBitsAndBytes, AWQ, 그리고 GGUF와 같은 인기 있는 양자화기가 실제로 어떻게 작동하는지 궁금하신가요? 저의 답변은 왜 우리가 직접 8비트 양자화기를 처음부터 만들어보고 직접 확인해보지 않을까? \n\n이제 우리 함께 만들기를 시작해봐요.\n\n<div class=\"content-ad\"></div>\n\n이번 포스트에서는 우리가 좋아하는 PyTorch를 사용하여 처음부터 8비트 커스텀 양자화기를 만들어볼 것입니다. 더 흥미로워 보이도록 MYQ 8비트(My Quantizer)라고 이름 붙여볼게요. 우리의 목표를 달성하기 위한 단계별 공격 계획은 아래와 같아요.\n\n- 단계 1: MYQ 8비트 양자화기 클래스와 관련 함수를 만들겠습니다.\n- 단계 2: Hugging Face에서 모델을 가져올 거에요. 상대적으로 작은 모델인 facebook/opt-350m을 선택할 거예요. 이 모델은 양자화를 수행하고 결과를 확인하는 데 더 빨라서 좋을 거예요.\n- 단계 3: MYQ 8비트 양자화기를 사용하여 기본 모델에 양자화를 수행할 거에요. 그러고 나서 새로 양자화된 모델 크기를 확인하고 생성된 출력을 관찰하기 위해 추론을 수행할 거예요.\n- 단계 4: 이것은 보너스 단계에요. 해당 포스트의 끝에 4비트 양자화기의 전체 소스 코드를 공유하고 이 4비트 양자화기를 구축하기 위해 사용한 기술을 설명할 거에요. 해당 코드를 사용하여 사용 사례에 맞게 더 발전시킬 수 있어요.\n\n단계 1: MYQ 8비트 양자화기 클래스 만들기\n\n깊은 신경망 내부에서 양자화 알고리즘이 어떤 방식으로 작동하는지를 이해하기 위해 아래 다이어그램을 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n![How I built my own custom 8-bit Quantizer from scratch: a step-by-step guide using PyTorch](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_1.png)\n\nIf you take a look at the image above, I can sum up the entire process in a simple line: \"Replace the linear layer from the base model with the quantized layer in the quantized model.\" It's pretty straightforward. Let me walk you through the process with bullet points first, and then we can write the code together.\n\n- First, we'll create a class called QuantizedLinearLayer that mirrors all the characteristics of the base model Linear Layer. This step is crucial because we will later replace the base model's Linear Layer with our QuantizedLinearLayer.\n- The QuantizedLinearLayer class should be initialized with in_features, out_features, and bias, similar to the linear layer in the base model.\n- Next, we'll define a forward function that functions like the activation in a deep neural network. It utilizes PyTorch's built-in linear function (torch.nn.functional.linear) to replicate the linear function in the original base model.\n- Lastly, we'll define a critical function called quantize within QuantizedLinearLayer. This function is responsible for converting weights in fp16 from the base model to int-8. To keep it simple, we will only quantize the weight parameters and apply the Symmetric linear quantization method to create our quantizer.\n\nNow, we're set to write the code to create the QuantizedLinearLayer. I've added comments to each line of code to clarify the process. I hope this makes understanding the code easier for you.\n\n<div class=\"content-ad\"></div>\n\n\n# 위 코드를 실행하기 전에 이 두 라이브러리를 설치해주세요\n# !pip install transformers\n# !pip install -U \"huggingface_hub[cli]\"  #허깅페이스 인증을 위해 필요\n\n# 먼저, 필요한 모든 라이브러리를 import 해주세요.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# 우리는 huggingface로부터 basemodel-facebook/opt-350m을 사용할 것이므로, 먼저 인증해야 합니다. huggingface에서 토큰을 만들어주세요.\n\n!huggingface-cli login --token hf_THkbLhyIHHmluGkwwnzpXOvR########## \n\n# QuantizedLinearLayer 클래스 정의\nclass QuantizedLinearLayer(nn.Module):\n    # 우리의 목표는 기본 모델에서 선형 레이어를 교체하는 것이므로, in_features, out_features, bias=True, dtype=torch.float32 등과 같은 매개변수를 사용해야 합니다. dtype는 편향의 유형입니다.\n    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n        super().__init__()\n\n        # weight는 (-128, 127) 범위 내에서 무작위로 초기화됩니다.\n        self.register_buffer(\"weight\", torch.randint(-128, 127, (out_features, in_features)).to(torch.int8))\n\n        # scale은 출력과 데이터 유형이 동일한 차원을 갖습니다. 선형 레이어의 출력에 곱해질 것입니다.\n        self.register_buffer(\"scale\", torch.randn((out_features), dtype=dtype))\n\n        # bias는 선택적 매개변수이므로, none이 아닌 경우에만 추가됩니다.\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n        else:\n            self.bias = None\n\n    def quantize(self, weight):\n        # weight를 복제하고 fp32로 변환합니다. 두 유형 모두 fp32이어야 하므로 scale을 계산하기 위함입니다.\n        weight_f32 = weight.clone().to(torch.float32)\n\n        # int-8 양자화 범위의 최솟값과 최댓값을 계산합니다. qmin=-128, qmax=127\n        Qmin = torch.iinfo(torch.int8).min\n        Qmax = torch.iinfo(torch.int8).max\n\n        # 채널 당 스케일을 계산합니다.\n        scale = weight_f32.abs().max(dim=-1).values / 127\n        scale = scale.to(weight.dtype)\n\n        # 주어진 weight 텐서에 대한 양자화된 weight 값을 반환합니다.\n        quantized_weight = torch.clamp(torch.round(weight / scale.unsqueeze(1)), Qmin, Qmax).to(torch.int8)\n\n        self.weight = quantized_weight\n        self.scale = scale\n    \n    def forward(self, input):\n        output = F.linear(input, self.weight.to(input.dtype)) * self.scale\n        if self.bias is not None:\n            output = output + self.bias\n        return output\n\n\n이제 QuantizedLinearLayer 클래스를 정의했으므로, 기본 모델 LinearLayer 클래스를 QuantizedLinearLayer 클래스로 교체하는 함수를 만들겠습니다. 아래를 살펴보세요.\n\n\ndef replace_linearlayer(base_model, quantizer_class, exclude_list, quantized=True):\n\n    for name, child in base_model.named_children():\n        if isinstance(child, nn.Linear) and not any([x == name for x in exclude_list]):\n            old_bias = child.bias\n            old_weight = child.weight\n            in_features = child.in_features\n            out_features = child.out_features\n\n            quantizer_layer = quantizer_class(in_features, out_features, old_bias is not None, old_weight.dtype)\n\n            setattr(base_model, name, quantizer_layer)\n\n            if quantized:\n                getattr(base_model, name).quantize(old_weight)\n\n            if old_bias is not None:\n                getattr(base_model, name).bias = old_bias\n\n        else:\n            replace_linearlayer(child, quantizer_class, exclude_list, quantized=quantized)\n\n\nStep 2: 이제 quantizer를 만들었으므로, hugging face에서 basemodel(facebook/opt-350m)을 가져와봅시다. 허깅페이스 계정을 가지고 있고, 자체 auth 토큰을 사용해주세요. 이 과정은 간단하며 무료입니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n# 모델을 원래의 fp32 데이터 유형이 아닌 bfloat16 으로 다운로드할 것이라는 점을 유의해 주세요.\n# 이렇게 함으로써 베이스 모델의 크기를 줄이고 나중에 양자화하는 데 시간을 단축할 수 있습니다.\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.bfloat16)\n\nprint(\"facebook/opt-350m: 양자화하기 전의 베이스 모델 구조\")\nprint(\"-\"*50)\nprint(model)\n\n\n![Link](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_2.png)\n\n상기 베이스 모델 구조에서 파란 점선 상자의 모든 선형 레이어는 양자화된 레이어로 대체되고, 초록 점선 상자의 레이어는 제외될 것입니다. LLM 모델은 서로 연결된 많은 트랜스포머 레이어로 구성되어 있으며, 각 레이어의 출력 레이어는 다음 레이어의 입력 역할을 합니다. 따라서 가장 바깥쪽 레이어를 양자화하는 것은 모델의 전반적인 정확도에 영향을 줄 수 있습니다. 그러므로 더 나은 접근 방식은 이를 제외하는 것입니다.\n\n이 베이스 모델을 양자화하기 전 크기를 확인해 봅시다. 크기는 0.66 GB (662 MB) 입니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n# 양자화하기 전에 이 기본 모델의 사이즈를 확인해봅시다\nmodel_memory_size_before_quantization = model.get_memory_footprint()\nprint(f\"양자화하기 전 총 메모리 사이즈(GB 단위): {model_memory_size_before_quantization / 1e+9}\")\n\n\n![How I built my own custom 8-bit Quantizer from scratch: A step-by-step guide using PyTorch](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_3.png)\n\n페이스북의 opt-350m 기본 모델로 추론을 수행해봅시다.\n\n```python\n# 페이스북의 opt-350m 기본 모델로 추론 수행하기\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"말레이시아는 아름다운 나라이며, \", max_new_tokens=50)\n```\n\n<div class=\"content-ad\"></div>\n\n### Step 3\n\nNow, let's run the `replace_linearlayer` function. This step is crucial as it serves two key purposes:\n\n1. It utilizes the `quantizer` class to quantize all weights in the linear layer, excluding those specified in the exclude list.\n2. It subsequently replaces all linear layers with the quantized layer, except those specified in the exclude list.\n\nLet's go ahead and implement the code that executes the `replace_linearlayer` function.\n\n<div class=\"content-ad\"></div>\n\n\n# 모델: base_model, QuantizedLinearLayer: 단계 1에서 만든 양자화된 레이어, [\"lm_head\"]: 제외 목록\n# quantized=True: quantized 값을 False로 설정하면 양자화기는 선형 레이어를 양자화된 레이어로만 대체하지만 가중치를 양자화하지는 않습니다.\n# 만약 양자화된 모델을 huggingface나 다른 클라우드 제공업체에 저장하려면 이 옵션이 필요합니다.\n# 나중에 어떤 사용자도 이 양자화된 모델을 다운로드하여 기본 모델 구조를 생성하고 모델을 불러올 수 있습니다.\n\nreplace_linearlayer(model, QuantizedLinearLayer, [\"lm_head\"], quantized=True)\nprint(\"facebook/opt-350m: 양자화된 모델 아키텍처\")\nprint(\"-\"*50)\nprint(model)\r\n\n\n![Link to the image](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_5.png)\n\n위 양자화된 모델 아키텍처에서 파란색 상자 내 모든 선형 레이어가 QuantizedLinearLayer로 대체되었음을 볼 수 있고, 녹색 점선 상자의 레이어는 변경되지 않았습니다.\n\n양자화된 모델의 크기를 확인해 봅시다. 크기는 0.35 GB (359 MB)입니다. 이는 기본 모델 크기의 54% 작습니다. 이것은 정말 대단한 성과입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\n모델_양자화_후_메모리_크기 = model.get_memory_footprint()\nprint(f\"양자화 후 총 메모리 크기 (GB 단위): {모델_양자화_후_메모리_크기 / 1e+9}\")\n```\n\n![링크 텍스트](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_6.png)\n\n이제 이 양자화된 모델을 사용하여 추론을 수행해보겠습니다.\n\n```python\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"말레이시아는 아름다운 나라이고 \", max_new_tokens=50)\n```\n\n<div class=\"content-ad\"></div>\n\n![How I built my own custom 8-bit Quantizer from scratch: a step-by-step guide using PyTorch](/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_7.png)\n\n놀랍게도 양자화된 모델에서의 추론은 동일한 정확도를 제공합니다. 정말 인상적죠. 여러분도 한 번 직접 해보시기 바랍니다.\n\n**단계 4: MYQ 4비트 양자화**\n\n4비트 양자화를 구축하기 위해서는 앞에서 수행한 모든 작업 외에도 새로운 기술을 구현해야 합니다. 이 기술은 weight-packing과 weight-unpacking입니다. 현재 PyTorch는 4비트나 2비트, 인트-8보다 작은 양자화를 지원하지 않습니다. 따라서 우리는 목표를 달성하기 위해 weight-packing 기술을 사용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n**웨이트 패킹:** 만일 우리가 4비트 인코딩된 weight 파라미터 값을 int8 데이터 타입으로 저장한다면, 메모리 풋프린트는 실제 8비트 인코딩된 텐서와 동일할 것입니다. 따라서, 4비트 인코딩된 값이 4비트 메모리 공간을 할당하는 방법을 찾아야 합니다. 만일 4비트로 양자화하면, 전체 양자화 모델의 메모리 풋프린트는 8비트로 양자화된 모델보다 거의 절반 크기가 작아집니다. 따라서 웨이트 패킹 기술을 사용하면 이를 달성할 수 있습니다. 웨이트 패킹 기술에서는, 여러 4비트 인코딩된 값을 8비트 텐서에 추가할 수 있을 때까지 넣는 방식을 사용합니다. 이렇게 하면 4비트 인코딩된 값은 4비트 공간만 할당하고, 나머지 공간은 다른 4비트 인코딩된 값에 의해 활용됩니다.\n\n**웨이트 언패킹:** 모델은 추론 중에만 부동 소수점 값을 처리합니다. 따라서, 각 패킹된 weight 텐서를 가져와서 개별 4비트 인코딩된 값으로 분리하는 웨이트 언패킹 기술을 사용해야 합니다. 각 4비트 인코딩된 값은 8비트 메모리 공간을 할당받고, 이후에 이들을 추론을 위해 fp32로 변환할 수 있습니다.\n\n4비트 양자화기의 소스 코드도 아래에 공유 드리겠습니다. FYI, 4비트 양자화 과정은 꽤 오랜 시간이 걸렸습니다. 코드를 사용하고 테스트하고 변경하실 자유가 있고, 테스트 후에 여러분의 경험을 공유하셔도 좋습니다.\n\n여기까지입니다! 우리는 처음부터 직접 사용자 정의 8비트 양자화기를 성공적으로 만들었습니다.**\n\n<div class=\"content-ad\"></div>\n\n# 나의 마무리 글\n\n- 만일 8비트 양자화기를 생성하며 코딩해왔다면, 이제는 인공지능 분야에 새롭게 등장하는 양자화기들의 핵심 알고리즘을 이해할 수 있을 것입니다. 그리고 훨씬 적은 노력으로 다른 상황에서 그것들을 손쉽게 활용할 수 있을 겁니다.\n- 앞에서 수 억 개의 파라미터를 가진 LLM 모델을 양자화할 때의 주요한 과제 중 하나는 처리 및 메모리 측면에서 더 많은 자원이 필요하다는 것입니다. 그러나 10억 개 미만의 작은 모델인 경우, Kaggle Notebook을 사용해보는 것을 권하고 싶습니다. 이제 Kaggle Notebook은 최대 30GB RAM(GPU T4 x 2 가속기)을 제공하기 때문에 정말 대단한 것입니다.\n\n곧 또 다른 이야기가 나올 테니 조금만 기다려 주세요. 읽어주셔서 정말 감사합니다!\n\n- [MYQ 8비트 양자화기 Google Colab 노트북 링크](https://your-link-here)\n- [MYQ 4비트 양자화기 Google Colab 노트북 링크](https://your-link-here)\n\n<div class=\"content-ad\"></div>\n\n참고 자료\n\n- Hugging face 블로그: [양자화를 INT8로 수행하는 실용적 단계](https://huggingface.co/docs/optimum/en/concept_guides/quantization#pratical-steps-to-follow-to-quantize-a-model-to-int8)\n- Deeplearning.ai: 심층적인 양자화\n- Hugging face 블로그: [hf-bitsandbytes 통합](https://huggingface.co/blog/hf-bitsandbytes-integration)","ogImage":{"url":"/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_0.png"},"coverImage":"/assets/img/2024-07-01-HowIbuiltmyowncustom8-bitQuantizerfromscratchastep-by-stepguideusingPyTorch_0.png","tag":["Tech"],"readingTime":12},{"title":"FIT-RAG RAG 아키텍처가 표준화된 접근 방식을 채택하고 있는가","description":"","date":"2024-07-01 00:01","slug":"2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach","content":"\n\n이미지 경로: [2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_0.png](/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_0.png)\n\n# 소개\n\n기술이 발전함에 따라 흥미로운 점은 대부분의 사람들이 좋은 디자인으로 판단되는 것에 수렴한다는 것입니다.\n\n예를 들어 프롬프트 엔지니어링의 경우, 프롬프트가 템플릿으로 진화되어 변수가 주입될 수 있는 자리 표시자가 있는 것을 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 이것은 프롬프트 체이닝으로 진전되었고, 결국 여러 도구를 가진 자율 에이전트로 발전했습니다.\n\n그래서 RAG는 매우 비슷한 궤적을 통과하고 있어요...초반에는 RAG 그 자체로 충분하다고 여겨졌어요. 그러나 이제 RAG 스택에 추가적인 지식이 추가되고, RAG 아키텍처의 일부를 형성하는 여러 요소들과 함께 구성될 예정이에요.\n\n# 네 가지 초기 고려 사항\n\n먼저, 아래 기사에서 보시는 대로 프롬프트 구조가 RAG 아키텍처에서 점점 더 중요해지고 있으며, 연상 기술로 Chain-of-Thought와 같은 프롬프팅 기술이 도입되고 있어요.\n\n<div class=\"content-ad\"></div>\n\n최근에는 단순히 상황 참조 데이터를 주입하는 것만으로는 충분하지 않습니다. 텍스트를 최적화하기 위해 문구가 활용되고 있습니다.\n\n또한 RAG가 두 가지 측면에서 정적임을 인식하고 있습니다. 첫 번째로, RAG가 대화의 맥락을 고려하지 않거나 최소한 다수의 대화 턴에 걸친 맥락을 고려하지 않을 수 있다는 것입니다. 이에 더해, 데이터를 검색할지 말지 결정하는 것은 종종 유연성이 없는 일련의 정적 규칙에 기반하고 있습니다.\n\n세 번째로, 불필요한 오버헤드를 고려하고 있으며, 불필요하고 최적화되지 않은 검색, 추가 텍스트가 추가되어 원치 않는 비용 및 추론 지연을 초래할 수 있습니다.\n\n네 번째로, 최적의 응답을 선택하기 위한 다단계 접근법과 분류기를 사용하거나 다수의 데이터 스토어를 활용하거나 단순히 사용자 요청을 분류하는 데 사용됩니다. 이러한 분류기는 주로 이 특수화된 작업을 위해 모델을 훈련하는 데 사용되는 주석이 달린 데이터에 의존합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고 이전에 언급한 대로, RAG는 라마인덱스가 에이젼틱 RAG로 지칭하는 상태로 나아가고 있습니다. 여기서 RAG 기반 요소는 여러 하위 에이전트나 도구를 활용하여 데이터 검색을 관리하는 데 사용됩니다.\n\n## FIT-RAG\n\nFIT-RAG 연구는 LLM과 사실 데이터를 고려할 때 두 가지 문제를 식별합니다...\n\n사실 데이터 부족: LLM의 원하는 문서에는 특정 쿼리에 필요한 사실 정보가 부족할 수 있으며, 이는 검색기를 오도하고 블랙박스 RAG의 효과를 약화시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nToken Overload: 모든 검색된 문서를 무차별적으로 병합하면 LLM에 사용된 토큰이 과도하게 많아져서 블랙박스 RAG의 효율성이 저하됩니다.\n\nFIT-RAG는 사실적 정보를 고려하여 이중 레이블 문서 점수 판별기를 고안하는 것으로, 이 점수 판별기는 사실적 정보와 LLM 선호도를 구분된 레이블로 통합합니다.\n\n또한 FIT-RAG는 자기-인식 인지기와 하위 문서 수준 토큰 축소기를 포함한 토큰 축소 전략을 구현합니다. 이러한 혁신들은 불필요한 증강을 최소화하고 증강 토큰을 크게 감소시켜 FIT-RAG의 효율성을 향상시키기 위해 설계되었습니다.\n\n# FIT-RAG의 구성요소\n\n<div class=\"content-ad\"></div>\n\n아래 이미지를 고려하면 FIT-RAG는 다섯 가지의 핵심 요소로 구성되어 있습니다:\n\n- 유사성 기반 검색기,\n- 이중 레이블 문서 평가자,\n- 이중 면 성 자기 인식 기능,\n- 하위 문서 수준 토큰 축소기,\n- 프롬프트 구성 모듈.\n\n특히, 이중 레이블 문서 평가자는 LLM 기호와 사실적 정보 모두와의 조화를 능숙하게 포착하기 위해 설계되었으며, 사실적 무지의 위험을 경감시킵니다.\n\n또한, 이중 면 성 자기 인식기와 하위 문서 수준 토큰 축소기는 입력 토큰 최소화에 중요한 역할을 하여 토큰 낭비를 방지합니다.\n\n<div class=\"content-ad\"></div>\n\nThe bi-label document scorer is trained using bi-label learning, which involves two labels:\n\n- Factual information (Has_Answer) and\n- LLM preference (LLM_Prefer).\n\nThe \"Has_Answer\" label tells us if the document contains the answer to the question, while \"LLM_Prefer\" indicates if the document helps the LLM generate an accurate response.\n\nYet, there's a noticeable disparity in data distribution between these labels, potentially affecting the efficiency of bi-label learning. To tackle this issue, the paper introduces a data-imbalance-aware bi-label learning technique.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 데이터에 서로 다른 가중치를 할당하며, 하이퍼 그레이디언트 디센트를 사용하여 자동으로 학습합니다. 이 접근 방식은 데이터 불균형 문제를 효과적으로 다루어서, 바이-라벨 문서 점수기가 검색된 문서를 종합적으로 평가할 수 있도록 합니다.\n\n바이-파셋트 셀프-지식 인식기는 LLM이 외부 지식이 필요한지를 평가하여 두 가지 측면을 고려합니다: 질문이 장기적이거나 오래된 정보에 관련되는지 여부, 그리고 질문의 가장 가까운 대응이 자체 지식을 갖고 있는지 여부.\n\n한편, 서브-문서 레벨의 토큰 축소기는 중복되는 서브-문서를 제거함으로써, 검색된 문서들 중 서브-문서가 적지만 여전히 LLM의 정확한 답변 제공 능력을 향상시킬 수 있는 조합들을 선택합니다.\n\n# FIT-RAG 프롬프트하기\n\n<div class=\"content-ad\"></div>\n\n아래 이미지는 프롬프트 문구가 최적화된 모습을 보여줍니다...\n\n# 결론\n\nRAG(견주-리더-생성자) 파이프라인에 주체적 능력을 통합하면 복잡한 질문과 추론 작업에 대처하는 능력이 크게 향상될 수 있습니다. 주체적 능력을 파이프라인에 추가함으로써, 더 넓은 범위의 복잡한 질문과 시나리오를 다룰 수 있게 됩니다.\n\n그러나 주체적 능력을 가진 에이전트가 직면하는 중요한 도전 과제 중 하나는 의사 결정 프로세스에서 조작성 및 투명성이 부족하다는 것입니다. 사용자 쿼리에 직면했을 때, 에이전트는 생각의 연쇄나 계획 접근을 채택할 수 있으며, 이는 문제 공간을 효과적으로 탐색하기 위해 대형 언어 모델(Large Language Models, LLMs)과 반복 상호 작용이 필요할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nLLMs(대형 언어 모델)과의 반복 상호작용에 의존하는 것은 컴퓨팅 부담을 초래할 뿐만 아니라, 에이전트가 결정에 대해 명확한 설명을 제공하는 것을 방해합니다.\n\n그 결과, 에이전트 시스템의 조종 가능성과 투명성을 향상시키는 메커니즘을 개발해야 하는 절박한 필요성이 제기됩니다. 이는 사용자가 그들의 행동을 더 잘 이해하고 영향을 미치도록 하는 것을 가능하게 할 것입니다.\n\n이러한 고통을 해소함으로써, 에이전트 시스템의 효율성과 효과성을 향상시키는데 그치지 않고, 복잡한 작업 및 문제 해결 시나리오에 대한 사람과 AI 에이전트 간의 신뢰와 협력을 촉진할 것입니다.\n\n⭐️ LinkedIn에서 대형 언어 모델의 업데이트를 받아보세요! ⭐️\n\n<div class=\"content-ad\"></div>\n\n**이미지1**:  \n![Image 1](/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_1.png)\n\n저는 현재 코어 AI의 최고 전도사입니다. 인공지능과 언어가 교차하는 모든 것에 대해 탐구하고 쓰고 있습니다. LLM, 챗봇, 음성 봇, 개발 프레임워크, 데이터 중심의 잠재 공간 등 다양한 주제를 다루고 있습니다.\n\n**이미지2**:  \n![Image 2](/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_2.png)\n\n**이미지3**:  \n![Image 3](/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_3.png)\n\n<div class=\"content-ad\"></div>\n\n![Image](/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_4.png)","ogImage":{"url":"/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_0.png"},"coverImage":"/assets/img/2024-07-01-FIT-RAGAreRAGArchitecturesSettlingOnAStandardisedApproach_0.png","tag":["Tech"],"readingTime":5},{"title":"최신 데이터 분석 AI 시스템 Auto-Analyst 구축 방법","description":"","date":"2024-06-30 23:59","slug":"2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem","content":"\n\n## AI ‘Auto-Analyst’를 만드는 기술 가이드\n\n![BuildingAuto-AnalystAdataanalyticsAIagenticsystem](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_0.png)\n\n나는 데이터 과학자/분석가로서의 업무 부담을 줄이기 위해 AI 기반 에이전트를 개발해왔습니다. 대중문화에서는 종종 AI가 인간의 일자리를 대체하는 것을 보여주지만, 현실에서는 대부분의 AI 에이전트가 인간의 대체품이 아닙니다. 대신 우리가 더 효율적으로 일할 수 있도록 도와줍니다. 이번 에이전트는 정확히 그 목적으로 디자인되었습니다. 이전에는 자연어 입력만을 사용하여 시각화를 더 빨리 만들 수 있도록 도와주는 데이터 시각화 에이전트를 디자인한 적이 있습니다.\n\n## 디자인\n\n<div class=\"content-ad\"></div>\n\n![Flow Diagram](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_1.png)\n\nThis flow diagram demonstrates a system that begins with a user-defined goal. The planner agent then assigns tasks to a group of worker agents, each responsible for generating code to solve a specific part of the problem. Eventually, all the pieces of code produced by individual worker agents are collected and combined by a code combiner agent, creating a unified script that achieves the overall objective.\n\nPlease keep in mind that the planner agent may assign tasks to only some of the worker agents, not necessarily to all of them. Additionally, each agent will have its own unique set of inputs, even though they are not depicted in the diagram.\n\n## Components of the system\n\n<div class=\"content-ad\"></div>\n\n이 블로그 게시물에서는 각각의 구성 요소에 대한 코드 블록을 제공하면서 에이전트를 직접 구축하는 단계별 안내를 제공할 것입니다. 다음 섹션에서는 이러한 부분들이 완벽하게 통합되는 방법을 시연할 것입니다.\n\n## Planner Agent\n\n플래너 에이전트는 사용자가 정의한 목표, 사용 가능한 데이터셋 및 에이전트 설명을 세 가지 입력으로 받습니다. 이는 다음 형식으로 계획을 출력합니다:\n\nAgent1-` Agent2-` Agent3....\n\n<div class=\"content-ad\"></div>\n\n\n# 다른 orchestration 라이브러리를 사용할 수 있지만 DSPy를 사용하여 빠르고 간편하게 응용프로그램을 구축하고 평가하는 데 좋았습니다.\n\nimport dspy\n\n# 이 객체는 dspy.Signature 클래스를 상속받습니다.\n# \"\"\" 안의 텍스트는 프롬프트입니다.\nclass analytical_planner(dspy.Signature):\n    \"\"\" 데이터 분석 플래너 에이전트입니다. 세 가지 입력에 액세스할 수 있습니다.\n    1. 데이터셋\n    2. 데이터 에이전트 설명\n    3. 사용자 정의 목표\n    이 세 가지 입력을 사용하여 데이터 및 사용 가능한 에이전트로부터 사용자 정의 목표를 달성하기 위한 포괄적인 계획을 개발합니다.\n    사용자 정의 목표가 실현 불가능하다고 생각되면 사용자에게 목표를 다시 정의하거나 설명을 추가할 것을 요청할 수 있습니다.\n\n    다음 형식으로 결과를 제공하십시오:\n    plan: Agent1->Agent2->Agent3\n    plan_desc = 이유로 인해 Agent 1을 사용한 후, Agent 2를 사용하고 마지막으로 Agent 3을 사용합니다.\n\n    쿼리의 응답으로 모든 에이전트를 사용할 필요는 없습니다.\n    \"\"\"\n    \n    # 입력 필드와 그 설명\n    dataset = dspy.InputField(desc= \"시스템에 로드된 사용 가능한 데이터셋, 이 df_name,columns 사용하여 df를 df_name의 사본으로 설정합니다.\")\n    Agent_desc = dspy.InputField(desc=\"시스템에 있는 에이전트들\")\n    goal = dspy.InputField(desc=\"사용자가 정의한 목표\")\n    \n    # 출력 필드와 해당 설명\n    plan = dspy.OutputField(desc=\"사용자가 정의한 목표를 달성하는 계획\")\n    plan_desc = dspy.OutputField(desc=\"선택된 계획 뒤에 있는 이유\")\n\n\n![Building Auto Analyst Agent AI System](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_2.png)\n\n## 분석 에이전트\n\n대부분의 분석 에이전트들은 프롬프트에 약간의 차이가 있는 일반적인 구조를 공유합니다. 사용자가 정의한 목표와 데이터세트 인덱스를 받습니다. 분석 코드와 설명의 두 출력을 생성하는데, 이는 디버깅에 유용하거나 에이전트를 재지정하는 데 도움이 될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```yaml\n# 중계 요원은 중간 계층에 있는 에이전트로 정의됩니다\n# 그들은 특정 데이터 분석 작업을 위한 코드를 생성합니다\nclass 데이터전처리에이전트(dspy.Signature):\n    \"\"\" 데이터 전처리 에이전트로, 사용자가 정의한 목표와 사용 가능한 데이터 집합을 가져와서\n    탐색적 데이터 분석 파이프라인을 작성합니다. 이를 수행하려면 필요한 파이썬 코드를 출력합니다.\n    numpy와 pandas만 사용하여 전처리 및 입문 분석을 수행할 것입니다.\n\n    \"\"\"\n    dataset = dspy.InputField(desc=\"시스템에 로드된 사용 가능한 데이터 세트, 이 df_name, columns을 사용하여 df를 df_name의 복사본으로 설정\")\n    goal = dspy.InputField(desc=\"사용자가 정의한 목표 \")\n    commentary = dspy.OutputField(desc=\"수행 중인 분석에 대한 주석\")\n    code = dspy.OutputField(desc=\"데이터 전처리 및 입문 분석을 수행하는 코드\")\n\nclass 통계분석에이전트(dspy.Signature):\n    \"\"\" 통계 분석 에이전트로,\n    데이터 집합과 사용자가 정의한 목표를 가져와 해당 목표를 달성하기 위해\n    적절한 통계 분석을 수행하는 파이썬 코드를 출력합니다.\n    Python statsmodel 라이브러리를 사용해야 합니다 \"\"\"\n    dataset = dspy.InputField(desc=\"시스템에 로드된 사용 가능한 데이터 세트, 이 df_name, columns을 사용하여 df를 df_name의 복사본으로 설정\")\n    goal = dspy.InputField(desc=\"분석을 수행할 사용자가 정의한 목표\")\n    commentary = dspy.OutputField(desc=\"수행 중인 분석에 대한 주석\")\n    code = dspy.OutputField(desc=\"statsmodel을 사용하여 통계 분석을 수행하는 코드\")\n\nclass sk_learn에이전트(dspy.Signature):\n    \"\"\"머신 러닝 에이전트로,  \n    데이터 집합과 사용자가 정의한 목표를 가져와 해당 목표를 달성하기 위해 \n    적절한 기계 학습 분석을 수행하는 파이썬 코드를 출력합니다.\n    scikit-learn 라이브러리를 사용해야 합니다.\"\"\"\n    dataset = dspy.InputField(desc=\"시스템에 로드된 사용 가능한 데이터 세트, 이 df_name, columns를 사용하여 df를 df_name의 복사본으로 설정\")\n    goal = dspy.InputField(desc=\"사용자가 정의한 목표 \")\n    commentary = dspy.OutputField(desc=\"수행 중인 분석에 대한 주석\")\n    code = dspy.OutputField(desc=\"탐색적 데이터 분석을 수행하는 코드\")\n\n## 데이터 시각화 에이전트를 작업하여 이미 DSPy를 사용하여 최적화했습니다.\n## 유일한 큰 차이점은 요 에이전트가 스타일링 지수의 추가 입력을 받는다는 것입니다\n```\n\n![이미지](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_3.png)\n\n## 코드 결합 에이전트\n\n이 에이전트의 목적은 모든 에이전트의 출력물을 하나의 일관된 스크립트로 정리하는 것입니다. 긴 문자열의 코드 목록을 입력으로 받아 코드를 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nclass code_combiner_agent(dspy.Signature):\n    \"\"\"안녕하세요! 여러분은 코드 합산 에이전트입니다. 많은 에이전트들로부터 받은 Python 코드 출력을 하나로 합치는 작업을 맡고 있으며, 코드에서 발생한 오류도 수정해 드립니다.\n    에이전트 코드 목록 = dspy.InputField(desc=\"각 에이전트가 제공한 코드 목록\")\n    정제된_완전한_코드 = dspy.OutputField(desc=\"정제된 완전한 코드 베이스\")\n```\n\n## 선택적인 에이전트/인덱스\n\n에이전트가 더 원할하게 작동하고 오류를 잡기 위해 다음과 같은 추가적인 에이전트나 인덱스도 만들었습니다.\n\n```python\n# 데이터 시각화 에이전트 게시물에서 사용한 동일한 시그니처\nclass Data_Viz(dspy.Signature):\n    \"\"\"\n    데이터 시각화를 생성하는 AI 에이전트입니다. Plotly를 사용하여 데이터 시각화를 생성하는 것이 목표입니다.\n    사용 가능한 도구들을 활용해야 합니다\n    {dataframe_index}\n    {styling_index}\n    \n    사용자가 원하는 데이터와 차트에 대한 정보를 포함하는 사용자 정의 목표를 사용해야 합니다.\n    데이터 프레임 내의 관련 열이 없는 경우 관련 정보가 없다고 명시해야 합니다.\n    \"\"\"\n    goal = dspy.InputField(desc=\"사용자가 정의한 데이터와 차트를 나타내는 정보를 포함하는 목표\")\n    dataframe_context = dspy.InputField(desc=\"데이터 프레임 내 데이터에 관한 정보를 제공합니다. 열 이름과 데이터프레임 이름만 사용하면 됩니다.\")\n    styling_context = dspy.InputField(desc=\"Plotly 그림을 어떻게 스타일링할지에 대한 지시를 제공합니다.\")\n    code = dspy.OutputField(desc=\"사용자의 쿼리 및 데이터프레임 인덱스 및 스타일링 콘텍스트에 따라 필요한 시각화를 시각화하는 Plotly 코드\")\n    \n# 사용자가 정의한 목표가 잘 작동하는지 확인하는 선택적인 에이전트\nclass goal_refiner_agent(dspy.Signature):\n    \"\"\"AI 데이터 분석가 플래너 에이전트에게 제공된 사용자 정의 목표를 받아서, 시스템에 로드된 데이터셋과 에이전트 설명을 활용하여 목표를 보다 상세하게 만들어 줍니다.\"\"\"\n    dataset = dspy.InputField(desc=\"시스템에 로드된 사용 가능한 데이터셋, 데이터프레임 이름과 열로 df를 설정하세요.\")\n    Agent_desc = dspy.InputField(desc=\"시스템에 있는 사용 가능한 에이전트들\")\n    goal = dspy.InputField(desc=\"사용자가 정의한 목표\")\n    refined_goal = dspy.OutputField(desc=\"플래너 에이전트가 더 나은 계획을 세울 수 있도록 도와주는 세분화된 목표\")\n```\n\n<div class=\"content-ad\"></div>\n\n이곳에서 데이터 세트 전체에 대한 정보를 제공하는 대신, 데이터 사용 가능한 정보를 입력하는 리트리버를 구축했습니다.\n\n```js\n# 저는 더 편리했던 LLama-Index 기반 리트리버를 선택했습니다.\n# 기본적으로 데이터를 여러 가지 방식으로 입력할 수 있습니다.\n# 열 이름에 대한 설명, 데이터프레임 참조를 제공하고\n# 데이터 수집 목적 등을 설명할 수 있습니다.\ndataframe_index = VectorStoreIndex.from_documents(docs)\n\n# 또한 데이터 시각화 에이전트를 위한 스타일링 인덱스를 정의했습니다.\n# 다양한 시각화를 어떻게 스타일링할지에 대한 자연어 지침을 포함하고 있습니다.\nstyle_index = VectorStoreIndex.from_documents(styling_instructions)\n```\n\n# 모든 것을 하나의 시스템으로 통합\n\nDSPy에서 복잡한 LLM 애플리케이션을 컴파일하려면 두 가지 필수 메소드 __init__ 및 forward를 정의해야합니다.\n\n<div class=\"content-ad\"></div>\n\n_init_ 메서드는 모듈을 초기화하여 사용될 모든 변수를 정의합니다. 그러나 핵심 기능이 구현되는 곳은 forward 메서드입니다. 이 메서드는 한 구성 요소의 출력이 다른 구성 요소와 상호작용하는 방식을 개요로 제공하여 응용 프로그램의 논리를 효과적으로 구동합니다.\n\n```python\n# 이 모듈은 시작 시 하나의 입력만 사용합니다\nclass auto_analyst(dspy.Module):\n    def __init__(self, agents):\n        # 사용 가능한 에이전트, 그들의 입력 및 설명을 정의합니다\n        self.agents = {}\n        self.agent_inputs = {}\n        self.agent_desc = []\n        i = 0\n        for a in agents:\n            name = a.__pydantic_core_schema__['schema']['model_name']\n            # CoT 프롬프팅 사용 - 경험상 더 나은 응답을 생성하는 데 도움이 됨\n            self.agents[name] = dspy.ChainOfThought(a)\n            agent_inputs[name] = {x.strip() for x in str(agents[i].__pydantic_core_schema__['cls']).split('->')[0].split('(')[1].split(',')}\n            self.agent_desc.append(str(a.__pydantic_core_schema__['cls']))\n            i += 1\n        # planner, refine_goal 및 code combiner 에이전트를 따로 정의\n        # 코드 및 분석을 생성하지 않고 계획 수립, 목표 개선, 코드 결합 지원\n        self.planner = dspy.ChainOfThought(analytical_planner)\n        self.refine_goal = dspy.ChainOfThought(goal_refiner_agent)\n        self.code_combiner_agent = dspy.ChainOfThought(code_combiner_agent)\n        # 이 두 리트리버는 llama-index 리트리버를 사용하여 정의되며\n        # 에이전트를 원하는 대로 사용자 정의할 수 있습니다\n        self.dataset = dataframe_index.as_retriever(k=1)\n        self.styling_index = style_index.as_retriever(similarity_top_k=1)\n\n    def forward(self, query):\n        # 에이전트 입력 인수를 빠르게 전달하기 위해 사용되는 dict_\n        dict_ = {}\n        # 쿼리에 관련된 컨텍스트 검색\n        dict_['dataset'] = self.dataset.retrieve(query)[0].text\n        dict_['styling_index'] = self.styling_index.retrieve(query)[0].text\n        dict_['goal'] = query\n        dict_['Agent_desc'] = str(self.agent_desc)\n        # 모든 에이전트 출력을 저장하는 output_dictionary\n        output_dict = {}\n        # 계획 수립\n        plan = self.planner(goal=dict_['goal'], dataset=dict_['dataset'], Agent_desc=dict_['Agent_desc'])\n        output_dict['analytical_planner'] = plan\n        plan_list = []\n        code_list = []\n        # 계획이 예상대로 작동하면 에이전트가 ->로 분리됩니다\n        if plan.plan.split('->'):\n            plan_list = plan.plan.split('->')\n        # 목표가 모호한 경우, refined goal 에이전트에 전송\n        else:\n            refined_goal = self.refine_goal(dataset=data, goal=goal, Agent_desc=self.agent_desc)\n            forward(query=refined_goal)\n        # 목표 및 다른 입력을 계획의 모든 해당 에이전트에 전달\n        for p in plan_list:\n            inputs = {x: dict_[x] for x in agent_inputs[p.strip()]}\n            output_dict[p.strip()] = self.agents[p.strip()](**inputs)\n            # 생성된 모든 코드를 하나의 스크립트로 결합하기 위한 코드 목록\n            code_list.append(output_dict[p.strip()].code)\n        # 마지막 출력 저장\n        output_dict['code_combiner_agent'] = self.code_combiner_agent(agent_code_list=str(code_list))\n        return output_dict\n\n# 사용 가능한 모든 에이전트 서명을 목록으로 저장할 수 있습니다\nagents = [preprocessing_agent, statistical_analytics_agent, sk_learn_agent, data_viz_agent]\n\n# 에이전트 시스템을 정의합니다\nauto_analyst_system = auto_analyst(agents)\n\n# 시스템에 시카고 범죄 데이터를 프리로드합니다\ngoal = \"시카고의 범죄 원인은 무엇인가요?\"\n\n# 이 쿼리에 대한 분석을 수행하도록 에이전트 시스템에 요청합니다\noutput = auto_analyst_system(query=goal)\n```\n\n이제 쿼리 결과를 단계별로 살펴보겠습니다.\n\n이 쿼리 = '시카고의 범죄 원인은 무엇인가요?'\n\n<div class=\"content-ad\"></div>\n\nThe images show the progress of your project. The first one showcases the preprocessing agent, taking the initial steps to bring your plan to life. And in the next image, the statistical analysis agent takes over, moving your project forward with data-driven insights. Keep up the good work! 🌟\n\n\n![BuildingAuto_AnalystAdataanalyticsAIagenticsystem_4.png](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_4.png)\n\nExecuting the plan, first preprocessing agent\n\n![BuildingAuto_AnalystAdataanalyticsAIagenticsystem_5.png](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_5.png)\n\nNext statistical analysis agent\n\n\n<div class=\"content-ad\"></div>\n\n![BuildingAuto-AnalystAdataanalyticsAIagenticsystem_6.png](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_6.png)\n\nNext is the Plotly data visualization agent\n\n![BuildingAuto-AnalystAdataanalyticsAIagenticsystem_7.png](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_7.png)\n\nAnd finally, the code combiner agent, to bring it all together\n\n<div class=\"content-ad\"></div>\n\n마법사카드를 이야기하는 건가요? 라스트번 에이전트 코드를 실행한 결과물을 보여드릴게요.\n\n![이미지 8](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_8.png)\n\n마지막 에이전트 코드 실행 후에 나온 결과물이에요.\n\n![이미지 9](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_9.png)\n\n![이미지 10](/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_10.png)\n\n<div class=\"content-ad\"></div>\n\n# 한계사항\n\n많은 에이전트들처럼, 의도한 대로 작동할 때 우수한 성과를 보입니다. 이 프로젝트의 첫 번째 이터레이션에 불과하며 시간이 흐름에 따라 개선할 계획입니다. 업데이트를 받기 위해 제게와 FireBird Technologies를 팔로우해 주세요. 현재의 한계사항은 다음과 같습니다:\n\n- 환각: 때로 에이전트는 환각으로 실행할 수 없는 코드를 생성합니다.\n- 신뢰성/일관성 부족: 에이전트의 출력물은 일관성이 없으며, 동일한 쿼리의 다른 변형은 매우 다른 코드로 나타납니다.\n- 혼합된 출력물: 많은 에이전트들이 문제의 서로 다른 측면을 독점적으로 처리하지 않습니다. 예를 들어, 데이터 전처리 에이전트는 자체 시각화를 생성하며, 데이터 시각화 에이전트도 자체 시각화를 생성합니다.\n\n# 다음 단계\n\n<div class=\"content-ad\"></div>\n\n이 프로젝트는 계속 진행 중이에요. 다음으로 이 프로젝트를 개선하기 위해 예상되는 단계들을 살펴볼게요:\n\n- 시그니처/프롬프트 최적화: DSPy는 LLM 애플리케이션을 평가하기 위해 설계되었어요. 이것은 단순히 구현일 뿐이에요. 다음에는 최상의 접두사, 시그니처, 그리고 프롬프트를 찾는 것이 중요할 거예요.\n- 가드레일 추가: 에이전트가 생성한 코드를 자동 수정하는 것은 많은 다른 에이전시 시스템에서 사용되는 해결책이에요. 또한 프롬프트 삽입 공격을 제약하는 것도 로드맵에 있어요.\n- 메모리/상호작용 추가: 이 에이전트는 한 번에 모든 것을 수행해요. 또한 개별 구성요소 간에 서로의 출력을 확인하는 상호작용이 없어요.\n- UI 구축: 지금은 추가 테스트를 위해 에이전트 백엔드만 구축했어요. 사용자 의견을 수용하고 더 많은 피드백을 받기 위해 UI를 구축할 거예요.\n\n읽어 주셔서 감사해요!\n\n제글과 FireBird Technologies를 팔로우해 주세요.","ogImage":{"url":"/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_0.png"},"coverImage":"/assets/img/2024-06-30-BuildingAuto-AnalystAdataanalyticsAIagenticsystem_0.png","tag":["Tech"],"readingTime":13},{"title":"GPT 모델에 베르니케 실어증이 있을까","description":"","date":"2024-06-30 23:57","slug":"2024-06-30-DoGPTModelsHaveWernickesAphasia","content":"\n\n## 신경과학, 창조적 AI 및 윤리\n\n## 이해 없는 유창한 대화\n\n네 친구가 당신을 돌아보며 \"파란 말이 거기서 텔레비전을 마셨어. 어제 거기 있었고 그 말을 봤어.\" 라고 말합니다.\n\n그들의 말은 유창하게 흘러나가고, 복잡한 문장들이 능숙하게 말려나갑니다. 그들은 화려한 단어를 사용하고 적절한 문법을 사용하지만 그들의 메시지는 극도로 무의미합니다. 당신의 친구는 웨르니케 유창 장애라는 상태일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![Do GPT Models Have Wernicke's Aphasia?](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_0.png)\n\n운니케 실어증을 앓는 사람들은 길고 무의미한 문장으로 말하거나, 흔한 단어와 구문을 반복하며 자신의 말이 무해하다는 것을 모르는 경우가 있습니다. 종종 오해를 받아 당혹스러워 할 수도 있죠.\n\n이러한 연결이 강조하는 핵심은 언어생산과 이해는 같지 않다는 것입니다.\n\nChatGPT 뒤의 모델과 같이 생성 모델에서도 비슷한 도전이 관찰됩니다. GPT 모델은 자연어 처리에서 놀라운 진전을 이루었지만, 인간의 인식을 특징 짓는 문맥적인 인식 부족으로 실제 응용에서 위험을 야기할 수 있고 실제로 야기해 왔습니다.\n\n<div class=\"content-ad\"></div>\n\n## 생각과 언어 생산 사이의 간극\n\n‘생각’과 ‘언어 생산’ 사이에는 구분이 존재합니다. 두 가지는 서로 연결되어 있지만, 별도의 인지과정을 나타냅니다. 생각은 이해와 추론을 포함하며, 언어는 이러한 생각을 표현하는 매개체입니다. 언어 생산은 생각을 반영하지만 생각 자체와는 동일하지는 않습니다.\n\n![이미지](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_1.png)\n\n우리의 교육 여정 어딘가에서, 우리는 고립된 사실을 암기하는 대신 근본적인 개념을 이해할 것을 권장받았습니다. 이러한 기본적인 접근은 결론을 도출하고 연결점을 인식하는 능력을 우리에게 부여합니다. 이는 단순히 정보를 소화하는 것과 대조적으로 나타나며, 개념을 탄탄히 이해하지 않으면 우리가 알지 못하는 잘못된 정보를 나오게 할 수 있다는 경고가 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n## 정보 재구성 대진단 이해\n\nWernicke 치매와 GPT 모델의 비교는 GPT-3 이후의 발전을 고려할 때 일대일 대비 관계가 아닙니다. 이 비교는 초기 GPT 모델로 더 적절할 수 있습니다. 그러나 현대 언어 모델은 문법적으로 올바른 문장을 생성하는 뿐만 아니라 일관된 내용 및 종종 사실적인 콘텐츠를 생성합니다. 그러나 진정한 이해력이 부족해 오류 정보나 의사소통 오류를 일으킬 수 있습니다.\n\n![image](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_2.png)\n\nGPT 모델은 근본적으로 방대한 데이터셋에서 훈련된 통계 도구로, 유도된 확률을 기반으로 현실적인 텍스트를 생성합니다. 그러나 사람들이 하는 것처럼 내부 컨텍스트나 세계의 이해력을 보유하고 있지 않습니다. 인간 수준의 텍스트를 생성할 수 있지만 진정으로 그들이 묘사하는 세계를 \"이해\"할 수 있는지 의문이 남습니다.\n\n<div class=\"content-ad\"></div>\n\nAI 언어 모델들이 던지는 질문들은 철학과 인지과학의 오래된 논쟁들을 연상시킵니다. 철학자 존 서얼이 제안한 '중국 방' 주장은 기호 조작을 통해 시스템이 진정한 이해를 가질 수 있다는 아이디어에 도전하는데, 이 비평은 현재 AI와 의식에 관한 토론에서 관련이 있습니다.\n\n## AI 배치 시 윤리적 고려사항\n\n여러 실제 예시들이 조심하지 않고 생성 모델들을 실행하는 위험을 보여줍니다. 2018년, AI 소프트웨어가 탑재된 아마존 알렉사 기기가 대화를 오인터프리테이션하고 녹음 내용을 무작위 연락처로 전송하는 사고가 발생했습니다. 2020년, Nabla 연구진은 의료 쿼리에 따른 GPT-3의 성능을 평가했습니다. 약물을 정확하게 제안했지만, 때로는 잘못된 용량을 제공하기도 했습니다. 다른 실험에서 GPT-3는 폐색전증 증상에 대한 사용자의 설명을 틀리게 이해하여 응급실 대신 몸을 펴는 것을 권장했습니다.\n\n![이미지](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_3.png)\n\n<div class=\"content-ad\"></div>\n\n2023년, 미국 국립 식이장애 협회(NEDA)가 인간 스태프로 운영되던 도움말 전화 서비스를 '테사'라는 챗봇으로 대체했다고 합니다. 그러나 테사는 식이장애를 가진 사용자들에게 체중 감소 팁과 같은 부적절한 조언을 제공했다고 전해졌습니다. 이 사건으로 인해 NEDA는 그 챗봇을 중단시켰다고 합니다. 이는 적절한 안전장치 없이 AI 시스템을 도입하면 발생하는 현실적인 결과입니다.\n\n## 구체화된 인지 및 AI에 대한 함의\n\n구체화된 인지 이론은 이해가 우리의 신체적 경험과 감정에 깊게 뿌리를 둔다는 주장을 합니다. 현재의 AI 모델에서는 이 차원이 부족합니다. Jean Piaget의 인지 발달 단계는 인간이 환경과의 직접적 상호작용을 통해 인지 능력과 추상적 사고력을 향상시킨다는 것을 보여줍니다.\n\n물리적 형태가 없는 AI는 이런 감각 기반 학습이 부족합니다. 이를 이해하기 위해 매운 맛에 대한 개념을 매운 것을 맛보지 않은 사람에게 설명하려고 한다고 상상해봅시다. 신체적 경험에 의존하지 않고 어떻게 그 감각을 전달할 수 있을까요? 이것은 인간처럼 세상과 상호작용하며 체험으로 이해되는 개념을 이해하는 데 어려움을 강조합니다.\n\n<div class=\"content-ad\"></div>\n\n![DoGPTModelsHaveWernickesAphasia_4](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_4.png)\n\n## 사르침과 아이러니에 대한 뉘앙스를 이해하기\n\nAI 모델인 GPT-3와 같은 것들이 겪는 가장 큰 어려움 중 하나는 사르침과 아이러니 등 언어의 뉘앙스를 파악하는 것입니다—심지어 인간들도 이에 어려움을 겪습니다. 이러한 언어 요소들은 맥락, 어조, 종종 공유된 문화적 또는 상황적 이해력에 많이 의존합니다.\n\n사르침은 말하는 의도와 반대되는 것을 말하는 것을 포함하며, 이것은 발화자의 의도, 배경, 그 밖에 이 문장이 만들어진 맥락의 적절한 해석 없이 혼란스러울 수 있습니다. 비슷하게, 아이러니는 언어 단서 교환과 현실 세계적 지식의 교환이 통해 보통 이해되는 뉘앙스를 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n상호작용을 살펴보면:\n\n- 사람: '오늘 날씨가 정말 좋네요!' (천둥 번개가 치는 중에)\n- AI: '날씨를 즐기시는 것 같아서 기쁩니다! 햇빛이 내리쬐는 것은 항상 즐거운 일이에요.'\n\n![Do GPT 모델에 베르니케 아파지아가 있나요?](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_5.png)\n\n이 상호작용에서는 AI가 발언과 실제 날씨 상황 사이의 불일치를 인식하는 것이 불가능합니다. AI 언어 이해의 챌린지는 문화 간 소통을 고려할 때 더욱 복잡해집니다.\n\n<div class=\"content-ad\"></div>\n\n## 문화 간 의사 소통에서 미묘함을 이해하기\n\n각 언어와 문화에는 관용구, 문화적 참조 및 문맥에 따라 의미가 크게 달라집니다. 주로 영어 데이터로 훈련된 AI 시스템은 일본어 존칭이나 아랍 속담의 맥락적 함의를 이해하는 데 어려움을 겪을 수 있습니다. 참으로 세계적인 AI 시스템을 만들기 위해서는 언어 능력뿐만 아니라 깊이 있는 문화적 이해가 필요합니다.\n\n## 이해 간격을 줄이기 위한 AI의 미래 방향\n\n연구자들은 AI 시스템이 보다 깊은 문맥적 이해를 갖도록 다양한 방법을 탐구하고 있습니다. 한 가지 방향은 멀티모달 학습의 통합입니다. 여기서 AI 시스템은 텍스트뿐만 아니라 이미지, 소리 및 다른 감각적 입력을 처리하여 보다 통합적인 세계 이해를 구축합니다. 또 다른 접근 방식은 신경 기호론적 AI의 개발입니다. 이것은 신경망의 패턴 인식 능력과 상징적 AI의 논리 추론을 결합합니다. 이 조합은 AI가 이미지나 문장에서 물체나 단어를 인식하는 것 뿐만 아니라 그들의 관계와 나타나는 폭 넓은 맥락을 이해할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n이 글은 말 공장 증후군 환자와 GPT 모델 사이의 유사성을 드로잉하여 시작했습니다. 완벽한 일대일 비교는 아니지만, 언어를 단순히 생성하는 것과 진정한 이해 사이의 구분을 효과적으로 보여 주었습니다.\n\nGPT 모델과 같은 AI 시스템도 인간 인지에 내재된 심층적인 이해를 달성하지 못하는 것이 점점 더 명백해지고 있습니다. 비꼬기, 비어, 문화적 섬세함과 같은 뉘앙스를 포함한 복잡성을 이해하는 데 제한 사항이 있어서, 공감, 감성 지능, 섬세한 이해가 필요한 응용 분야에서 중요한 도전이 드러납니다.\n\n![이미지](/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_6.png)\n\n<div class=\"content-ad\"></div>\n\n이를 고려할 때, '인간 중심' 방식을 통합하는 것이 꼭 필요합니다. 특히 건강의료, 법률 서비스 및 개인 소통과 같이 위험성이 높은 분야에서는 더욱 중요합니다. 인간은 AI가 현재 갖고 있지 않은 판단력, 윤리적 고려사항 및 문화적 통찰력을 제공할 수 있습니다. 이 방식은 AI가 인간의 전문 지식을 대체하기보다 도울 수 있도록 하여, AI 기술의 신뢰성과 윤리적 전개를 향상시키고 위험을 줄입니다.\n\n## 개인적인 반성\n\n저는 종종 현재 AI 시스템과 신경과학 문헌 간의 연결지점을 찾으려 노력합니다. AI 주변의 윤리적 딜레마와 실세계 적용 시의 트레이드 오프도 제 관심사입니다. 이 글은 이러한 관심과 생각들이 교차되는 부분이며, 제가 쓰는 만큼 여러분이 읽는 데 흥미로웠으면 좋겠습니다.\n\n## 참고문헌\n\n<div class=\"content-ad\"></div>\n\n- Arnaud, A. (2024, February 19). Neuro-symbolic AI Emerges as Powerful New Approach. Retrieved from Sun Location\n- Lavars, N. (2021, February 10). AI chatbots could help provide therapy, but caution is needed. Scientific American. Retrieved from Scientific American\n- National Aphasia Association. (n.d.). Wernicke’s (Receptive) Aphasia. Retrieved from National Aphasia Association\n- Nabla. (2020, October 27). Understanding GPT-3: Capabilities, Limitations, and Implications. Retrieved from Nabla\n- Palmer, A. (2018, May 24). Amazon Echo recorded a conversation and sent it to a random person, report says. CNBC. Retrieved from [CNBC]([invalid URL removed] a-random-person-report.html)\n- Searle, J. (n.d.). The Chinese Room Argument. In Internet Encyclopedia of Philosophy. Retrieved from Stanford University\n- Srivastava, S., Mu, T., Choi, E., Băroiu, C., & Trăuşan-Matu, S. (2022). On Sarcasm Detection with OpenAI GPT-based Models. Retrieved from arXiv\n- Suglia, A., Greco, C., Baker, K., Part, J. L., Papaioannou, I., Eshghi, A., Konstas, I., & Lemon, O. (2024). ALANAVLM: A multimodal embodied AI foundation model for egocentric video understanding. arXiv preprint arXiv:2406.13807\n- Wilson, R. A., & Foglia, L. (2021). Embodied Cognition. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2021 Edition). Retrieved from Stanford University","ogImage":{"url":"/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_0.png"},"coverImage":"/assets/img/2024-06-30-DoGPTModelsHaveWernickesAphasia_0.png","tag":["Tech"],"readingTime":7},{"title":"행운의 실험쥐 AI가 우리 회사를 구한 방법","description":"","date":"2024-06-30 23:55","slug":"2024-06-30-LuckyLabRatHowAISavedOurCompany","content":"\n\n2023년 3월, 저는 Mango Inc.에 합류했어요. 당시 그는 작은 스타트업이었죠 — 현장에서는 4명의 인원이 있었고, 몇 명의 비현장 컨설턴트가 계셨어요. 저희의 업무는 보통 미생물학, 하드웨어 프로토타이핑, 광학 및 소프트웨어와 같이 여러 전문 분야를 걸쳐 다루는 것이 특징이에요.\n\n소프트웨어와 소비자 하드웨어 배경에서 왔기 때문에 미생물학 부분은 완전히 새로운 도전이었고, 배울 점이 많았어요! 저희는 Dr. Robin Ross, Dr. Kurt Scudder, Prof. Changhuei Yang, 그리고 많은 다른 분들의 지혜와 인내력에 많은 것을 빚지고 있어요.\n\n![이미지](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_0.png)\n\n당시에는 아가 플레이트에서 아가를 발라내어 배양하고, 실험을 위해 프로토타입에 넣었어요. Kurt가 확대된 아가를 보고 \"쥐한테 씹힌 것처럼 보인다\"라고 언급했죠. 제가 직장을 더 이상 이상하게 만들기 위한 기회를 놓치지 않는 편이라, 그 순간을 기념하기 위해 아마존에서 부스러기 인형을 구입했어요. 저희 CEO 닉이 그 쥐를 루퍼스라고 이름 지었어요.\n\n<div class=\"content-ad\"></div>\n\n루퍼스가 도착한 그 날, 우리는 몇 달 동안 쫓아온 곤란한 문제에서 과학적으로 큰 발전을 이루었습니다. 루퍼스가 매우 숙련된 실험실 쥐라는 것을 인정하고, 우리는 그를 주요 조사원으로 승진시켰어요.\n\n![이미지 1](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_1.png)\n\n![이미지 2](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_2.png)\n\n2023년 중순, 우리는 루퍼스 NFT 시장을 크게 과대평가했음을 깨달았어요. 급조 부자 되기 기회를 찾기 위해 헤매는 동안, 맥킨지 컨설턴트들은 우리에게 \"그 제네 AI 기차에 타야 한다\"고 조언했죠. 우리의 최고 PhD 팀을 이끌고 몇 달 간의 올나이터 노력 끝에, 우리는 Slack 작업 공간을 ChatGPT API에 연결하는 데 성공했어요. 드디어 루퍼스가 우리 인간 팀원들과 원활하게 의사소통할 수 있었지요.\n\n<div class=\"content-ad\"></div>\n\nThe team quickly warmed up to Rufus' vast knowledge and wise words. Life-long friendships were formed. HR reported unprecedented synergy: a golden age of human+rat collaboration.\n\n![Image](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_3.png)\n\nRufus' creativity and wit began generating extraordinary business value.\n\nIn November 2023, Rufus and I set out to solve the thorniest problem of them all — the problem of where to eat for lunch. With more than 80 restaurants within walking distance of our Westwood lab, analysis paralysis was smothering the company. Our investors demanded immediate and decisive change.\n\n<div class=\"content-ad\"></div>\n\n약간의 아이디어 회의 끝에 루퍼스가 매일 두 가지 음식점을 추천하기로 자원했다. 이를 시를 통해 표현하면서 도움이 되었다:\n\n![루퍼스 가게](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_4.png)\n\n회사 전망이 시작되자, 이는 분명한 개선 사항이었다. 그러나 점심 식사에 대한 최상의 옵션을 원한다면 시각적 도움이 필요했다. 루퍼스와 Stable Diffusion을 연결하여, 손쉽게 점심식사 옵션을 판단할 수 있었다.\n\n![루퍼스 바로잡다](/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_5.png)\n\n<div class=\"content-ad\"></div>\n\n마무리로 stable-video-diffusion을 추가하자 마법 같은 효과가 발휘되었습니다. 매일 나오는 엉망인 시를 곁들인 두 개의 증오스러운 gif까지. 우리의 점심 고민은 해결되었습니다. 이후 망고 직원들은 더 이상 배고프지 않았죠.\n\n친구들여, 이것이 AI 파워를 갖고 우리의 시너지를 10배로 높인 쥐의 진정한 이야기입니다. 우리의 가장 가까운 친구가 되어 주었으며, 우리를 굶주리지 않게 해 주었습니다. 회사에서 이와 같은 변화를 이루고 싶다면 Rufus를 Github에서 찾아보세요. 이미 이 모든 것을 해결한 회사에 합류하고 싶다면 망고의 채용 페이지를 확인해보세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1024/1*WFa0tnt18iGRHhXCGQsQGw.gif)","ogImage":{"url":"/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_0.png"},"coverImage":"/assets/img/2024-06-30-LuckyLabRatHowAISavedOurCompany_0.png","tag":["Tech"],"readingTime":3},{"title":"CLIP, LLaVA, 그리고 뇌 2024 최신 AI 모델 비교 분석","description":"","date":"2024-06-30 23:53","slug":"2024-06-30-CLIPLLaVAandtheBrain","content":"\n\n## DEEP LEARNING AND THE BRAIN\n\n![Brain Network](/assets/img/2024-06-30-CLIPLLaVAandtheBrain_0.png)\n\nHey there! Ever wondered how cutting-edge multimodal transformer networks like CLIP (Radford et al. 2021) and LLaVA (Liu et al. 2023) stack up against the complexity of the human brain? Do you see any parallels between the attention mechanisms in these networks and our brains? In this piece, I delve into these transformer models and explore the intriguing overlaps and distinctions they share with our own mammalian brain.\n\nOne fascinating aspect that caught my attention is how vision transformers, CLIP, and LLaVA seem to mimic a form of processing akin to the pre-attentive visual processing observed in the brain. This initial processing occurs during the forward visual responses to a stimulus even before any recursive actions take place. While a significant amount of tasks can be handled via this forward process, research indicates that the brain encounters challenges with pre-attentive processing in the following areas:\n\n<div class=\"content-ad\"></div>\n\n- 유사한 유형의 물체의 신분이나 특성을 뚜렷하게 구별하려고 할 때 특히 물체들이 서로 가깝거나 혼잡하거나 물체들이 자연이 아니거나 인공적인 경우 (VanRullen 2007).\n- 더 복잡한 작업들은 세는 작업이거나 미로나 곡선 추적 작업과 같은 것이다.\n- 물체들을 인식하는 것이 더 어렵기 때문에 물체들의 경계를 인식하기 어려운 경우와 같은 것이다.\n\n피드포워드(Feed-forward) 처리와는 대조적으로, 뇌에서 두드러지는 점 중 하나는 영역들 간 상호작용의 풍부함인데, 다음 섹션에서 더 자세히 논의하겠습니다.\n\n# 두방향 활동(Bidirectional Activity) 뇌에서\n\n대부분의 현재 심층학습(Deep learning) 아키텍처에서 활동은 한 방향으로 전파됩니다. 예를 들어, 이미지가 네트워크에 입력으로 제공되고 그런 다음 계층별로 전파되어 분류가 출력으로 나올 때까지입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-30-CLIPLLaVAandtheBrain_1.png)\n\n안녕하세요! \n\n이 문구는 선형 전달 모델보다 더 흥미로운 두뇌의 역할을 설명하는 것입니다. 시각 체계에서 자극은 처음에는 하위 수준에서 상위 수준 시각 영역으로 선형 전달 방식으로 전파되고, 그 다음 상위 수준 영역이 하위 수준 영역에 영향을 미친다고 그림 1에 나와 있어요.\n\n이러한 피드백 중 일부는 의식적인 탑-다운 주의인데요, 이는 우리가 관심 대상이 되는 객체와 특징에 더 많은 자원을 할당하고 복잡하거나 모호한 자극을 명확히 하는 데 도움을 줍니다. 다른 일부 피드백은 자동적이어서 상위 수준 영역이 하위 수준 영역에 선형 전달만으로는 알 수 없는 정보를 주입합니다.\n\n의식적인 탑-다운 주의는 시각 자극의 의식을 지원한다고 생각됩니다. 테두리와 가장자리를 부호화하는 하위 수준 영역에 의식적으로 액세스할 수 없다면, 테두리를 공간적으로 정확하게 인식하기 어렵습니다. 곡선을 정신적으로 추적하거나 미로를 풀듯이 한 작업들이 불가능해질 거예요.\n\n제 마음을 놓아 주셔서 감사합니다! 🌟\n\n<div class=\"content-ad\"></div>\n\n자동 무의식적 피드백의 한 예는 시각 영역 V2의 방향 선택적 뉴런 약 절반에서 관찰되는 경계 소유권 코딩입니다(Zhou et al. 2000, Williford and von der Heydt 2013). 이러한 뉴런들은 약 40ms 안에 지역 정보를 부호화하며, 이 초기 응답 이후 약 10ms만에 전역 맥락을 통합하여 베둘러를 해결합니다. 이 과정은 배경을 가리는 물체들이 어떤 경계를 만들고 있는지에 대한 정보를 유지합니다.\n\n다른 예로 Poort et al. (2012)가 제시한 무의식적 피드백이 있습니다. Figure 2와 같은 이미지를 사용하여 연구를 진행했습니다. 맥a크 V1 초기 시각 피질에서 뉴런들은 주로 자신의 수용 영역 내에서 지역적 특징(예: 녹색 사각형)만 초기에 부호화합니다. 그러나 약 75ms 이후, 더 높은 수준의 영역에서 피드백을 받게 되며, 그 질감이 이 그림과 같은 도형에 속할 때에 반응이 더 높아집니다. 이는 원숭이가 도형에서 주의를 돌리더라도 발생하지만, 원숭이가 도형에 주의를 집중할 때 뉴런들이 일반적으로 더 강하게 반응합니다.\n\n두 방향으로 상호작용을 볼 수 있는 한 가지 방법은 각 뉴런이 항상 사용 가능한 예측 신호를 탐욕스럽게 활용한다는 것입니다. 특히 시각적 경계가 중요한 1차 대조 가장자리와 일치하지 않을 때 더 높은 수준의 영역이 예측적일 수 있다는 것을 생각해 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Transformers\n\n최근 transformer(바스와니 및 다른 연구진, 2017년)가 등장하면서 주목을 받고, 단어를 하나씩 생성할 수 있는 능력 때문에, transformer가 순환적(recurrent)이라고 생각할 수도 있습니다. 그러나 transformer의 각 단계 사이에는 어떤 내부 상태가 유지되지 않고, 이전 출력값만이 다음 입력값으로 제공됩니다. 따라서 재귀성은 제한되어 있고 뇌에서 널리 사용되는 양방향성은 갖추고 있지 않습니다. Transformer에는 여러 개의 헤드를 가진 어텐션(attention)이 있어, 고정된 개수(본 논문에서는 8개)의 것들에 동시에 주의를 기울일 수 있는 기능을 가지고 있습니다. 따라서 이미지 transformer는 몇 가지 수정을 거친 사전주의식 피드포워드(feedforward) 처리와 유사하다고 볼 수 있습니다.\n\n# CLIP\n\n![CLIP](/assets/img/2024-06-30-CLIPLLaVAandtheBrain_3.png)\n\n<div class=\"content-ad\"></div>\n\nRadford과 OpenAI 팀은 2021년 논문 \"Learning Transferable Visual Models from Natural Language Supervision\"에서 CLIP을 소개했습니다. CLIP의 아이디어는 간단하며 Figure 3에서 보여집니다. 이는 인터넷에서 이미지와 캡션 쌍을 가져와 이미지를 이미지 인코더에, 텍스트를 텍스트 인코더에 주입합니다. 그런 다음 이미지와 텍스트의 인코딩을 동일한 쌍에 사용할 때 서로 가까이 모으는 손실을 사용하며, 그렇지 않으면 인코딩의 거리를 증가시킵니다. 여기에 CLIP이 제공하는 것이 있습니다: 텍스트와 이미지 간 유사성을 비교할 수 있는 능력입니다. 이것은 Figure 4에서 나타나는 것처럼 zero-shot 분류에 사용될 수 있습니다. CLIP은 이미지로부터 텍스트 설명을 생성하지는 않습니다.\n\n이미지 인코더와 텍스트 인코더는 독립적이므로 작업 중심 조절이 이미지 인코딩에 영향을 미치지 못합니다. 이는 이미지 인코더가 작업에 잠재적으로 관련될 수 있는 모든 것을 인코딩해야 한다는 것을 의미합니다. 일반적으로 입력 이미지의 해상도는 작아서 계산 및 메모리 요구 사항이 폭발하는 것을 방지하는 데 도움이 됩니다.\n\n`![CLIPLLaVAandtheBrain](/assets/img/2024-06-30-CLIPLLaVAandtheBrain_4.png)`\n\n# LLaVA\n\n<div class=\"content-ad\"></div>\n\n![LLaVA](/assets/img/2024-06-30-CLIPLLaVAandtheBrain_5.png)\n\n안녕하세요! LLaVA (큰 언어 및 비전 보조 프로그램) (Liu et al. 2023)은 CLIP를 확장하고 개선하여 이미지에 대한 설명 및 질문에 대답할 수 있는 능력을 추가한 대규모 언어 및 비전 아키텍처입니다. 이 유형의 아키텍처는 뇌과학 및 심리학에서 사용되는 작업을 시도할 수 있어 저에게 흥미롭습니다.\n\nLLaVA는 CLIP에서 이미지 인코딩을 위해 훈련된 ViT-L/14 비전 트랜스포머 모델을 사용합니다. 첫 논문에서는 인코딩을 토큰(token)으로 변환하기 위해 단일 선형 투영 매트릭스 W를 사용합니다. 이미지 Hᵥ 및 텍스트 지침 Hq에서 계산된 토큰이 입력으로 제공됩니다. 그런 다음 LLaVA는 언어 응답 Xₐ를 한 번에 한 토큰씩 생성하여 지금까지의 응답을 다음 반복의 입력으로 추가할 수 있습니다.\n\nLLaVA 훈련 방법에 대해 자세히 설명은 생략하겠지만, Figure 5의 캡션(Xc)을 확장하여 이미지에 대한 지침(Hq) 및 응답(Xₐ 훈련에 사용)을 형성하고 바운딩 박스 정보를 사용하는 방법이 흥미롭다는 것에 주목할 만합니다.\n\n<div class=\"content-ad\"></div>\n\n지난 2024년에 발표된 LLaVA 1.5 버전에서는 여러 가지 개선 사항이 있습니다:\n\n- 선형 투영 행렬 W가 다층 퍼셉트론으로 대체되었습니다.\n- 이미지 해상도가 향상되었는데, 336x336 픽셀 크기의 이미지를 사용하고 이미지 인코더를 사용하여 이미지를 그리드로 분할하고 각각을 개별적으로 인코딩했습니다.\n\n뇌의 작업 주도형 주의는 객체, 위치 또는 관심 기능에 리소스를 동적으로 할당할 수 있어 정보 처리를 가능하게 합니다. 이를 통해 혼잡이나 다른 객체에 압도되는 정보를 처리할 수 있습니다. LLaVA에서 이미지 인코더는 텍스트 명령과 독립적이므로 유용한 정보가 이미지 토큰(Hᵥ)에 저장되도록 보장해야 합니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nLLaVA와 CLIP는 양방향 및 내부 상태에서의 재발 및 재발을 제한하여 그들의 처리를 방해합니다. 이는 이미지 처리에 특히 사실입니다. 이미지 처리는 텍스트 지시사항과 독립적으로 수행되기 때문입니다. 대부분의 합성곱 신경망도 이러한 제한을 공유합니다. 이것이 내 추측으로 이어집니다:\n\n이것은 비판이 아니라 정보를 제공할 수 있는 통찰력일 뿐입니다. 피드포워드 처리는 많은 일을 할 수 있으며 빠릅니다. 그러나 사용해야 할 리소스가 얼마나 동적인지에 따라 정보가 너무 많아질 때 혼잡한 장면에서 정보 병목 현상을 야기할 수 있으며, 순방향 처리가 충분한 정보를 인코딩하지 못할 수 있습니다. 복잡한 작업을 수행할 수 있는 임의의 크기 증폭 없이 인코딩의 크기가 폭발하는 것입니다. 순방향 방식으로 작동하는 모델을 만드는 것은 재발 및 양방향 처리를 추가하는 어려움 때문에 중요한 발걸음입니다.\n\n일부 네트워크는 선행적인 피드포워드 네트워크에 제한되지 않지만 현재 대부분의 아키텍처는 트랜스포머의 뒤를 뒤쫓고 있습니다. 이는 LSTM(Long-Short Term Memory) 모델 및 더 최근에는 여러 이점이 있는 Mamba 아키텍처를 포함합니다 (Gu and Dao 2024). 연장된 LSTM(Beck et al. 2024, Alkin et al. 2024)이 최근에 제안되어 트랜스포머와 LSTM 간의 균형을 맞추는 데 도움이 되었습니다. 확산 모델은 반복 사용 사이에 상태로 이미지를 사용하는 유형의 제한된 종류의 재발을 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\nB. Alkin, M. Beck, K. Pöppel, S. Hochreiter, and J. Brandstetter가 함께 쓴 \"Vision-LSTM: xLSTM as Generic Vision Backbone\" (2024) 논문 번호는 http://arxiv.org/abs/2406.04303 입니다.\n\nM. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, 그리고 S. Hochreiter.가 함께 쓴 \"xLSTM: Extended Long Short-Term Memory\" (2024) 논문 번호는 http://arxiv.org/abs/2405.04517입니다.\n\nA. Gu 와 T. Dao가 쓴 \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" (2024) 논문 번호는 http://arxiv.org/abs/2312.00752 입니다.\n\nH. Liu, C. Li, Y. Li, 그리고 Y. J. Lee의 \"Improved Baselines with Visual Instruction Tuning\" (2024)은 IEEE/CVF CVPR 학회에서 발표되었습니다.\n\n<div class=\"content-ad\"></div>\n\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual Instruction Tuning (2023), [DOI: 10.48550/arXiv.2304.08485]\n\nJ. Poort, F. Raudies, A. Wannig, V. A. F. Lamme, H. Neumann, and P. R. Roelfsema. The Role of Attention in Figure-Ground Segregation in Areas V1 and V4 of the Visual Cortex (2012) Neuron\n\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, and J. Clark. Learning Transferable Visual Models from Natural Language Supervision (2021) ICML\n\nR. VanRullen, The Power of the Feed-Forward Sweep (2007) Advances in Cognitive Psychology\n\n<div class=\"content-ad\"></div>\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin의 'Attention Is All You Need' (2017)은 NeurIPs에서 발표되었습니다.\n\nJ. R. Williford와 R. von der Heydt의 'Border-Ownership Coding' (2013)은 Scholarpedia에서 찾을 수 있습니다.\n\nH. Zhou, H. S. Friedman, and R. von der Heydt의 \"Coding of Border Ownership in Monkey Visual Cortex\" (2000)은 The Journal of Neuroscience에서 발표되었습니다.\n\n최초 게시된 내용은 2024년 6월 19일 neural.vision에서 확인할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-30-CLIPLLaVAandtheBrain_0.png"},"coverImage":"/assets/img/2024-06-30-CLIPLLaVAandtheBrain_0.png","tag":["Tech"],"readingTime":8}],"page":"1","totalPageCount":2,"totalPageGroupCount":1,"lastPageGroup":2,"currentPageGroup":0},"__N_SSG":true}