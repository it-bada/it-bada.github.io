<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>자율주행의 혁신 자율주행 자동차 속 CNN | it-bada</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://it-bada.github.io///post/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="자율주행의 혁신 자율주행 자동차 속 CNN | it-bada" data-gatsby-head="true"/><meta property="og:title" content="자율주행의 혁신 자율주행 자동차 속 CNN | it-bada" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://it-bada.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://it-bada.github.io///post/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars" data-gatsby-head="true"/><meta name="twitter:title" content="자율주행의 혁신 자율주행 자동차 속 CNN | it-bada" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | it-bada" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 12:20" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PDYZ2R0CH9"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-PDYZ2R0CH9');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-90e194c785348efe.js" defer=""></script><script src="/_next/static/pS12KbT5cDEZt16dno2Ez/_buildManifest.js" defer=""></script><script src="/_next/static/pS12KbT5cDEZt16dno2Ez/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Bada</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">자율주행의 혁신 자율주행 자동차 속 CNN</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="자율주행의 혁신 자율주행 자동차 속 CNN" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Bada</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>NVIDIA의 예측 주행 스티어링을 위한 DAVE-2 시스템 깊이 분석</h2>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png" alt="이미지"></p>
<p>본 기사는 센터, 좌측, 우측 세 대의 카메라로부터 입력 이미지를 사용하여 주행 중인 자동차의 조향 휠 각도를 예측하는 데 합성곱 신경망(CNN) 접근 방식을 활용합니다. 사용된 모델 아키텍처는 NVIDIA가 자체 자율주행 시스템인 DAVE-2를 위해 개발했습니다. CNN은 Udacity 시뮬레이터에서 테스트되었습니다.</p>
<h1>소개</h1>
<p>자율 주행 자동차는 주변 환경을 감지하고 인간의 개입 없이 교통과 다른 장애물을 독립적으로 움직일 수 있습니다. (출처: [1]) 자율 주행 차량(AVs)의 사용은 거의 전 세계적으로 주요 산업이 되었습니다. 과거 몇 년 동안 보다 빠르고 가치 있는 차량이 생산되었지만, 더 많은 차량이 있는 가속화된 세계에서는 불행히도 사고 사례가 증가했습니다. 대부분의 경우 사고는 인간 운전자의 잘못이 원인입니다. 따라서 자율 주행 차량의 도움으로 이론상으로 대체 가능할 수 있습니다. (출처: [2]) Waymo, Zoox, NVIDIA, Continental, Uber와 같은 다수의 관련 기업이 이 제품을 개발 중에 있습니다. 이 종류의 차량을 통해 자동차 교통의 안전성, 보안성, 효율성이 향상되며, 운전이 가장 안전하게 이루어질 수 있습니다. (출처: [1])</p>
<p>AVs는 사람과 같이 주변 환경을 인식하고 수집한 정보를 기반으로 논리적인 결정을 내리기 위해 다양한 센서 기술에 의존합니다. 가장 일반적인 AV 센서 유형은 레이다, LiDAR, 초음파, 카메라, 글로벌 네비게이션 시스템 등이 있습니다. (출처: [3])</p>
<p>고급 운전자 보조 시스템(ADAS)은 다양한 자율 주행 수준을 분류하는 다계층 시스템입니다. 이는 완전히 인간 운전에서부터 완전히 자율 주행까지 다양한 수준을 보여줍니다. (Figure 1 참고)</p>
<h1>합성곱 신경망 (CNNs)</h1>
<p>현대 CNN에 대한 첫 연구는 1990년대에 발표되었으며, 네오코그니트론에 영감을 받았습니다. 야너 르쿤 등은 "Gradient-Based Learning Applied to Document Recognition" 논문에서 더 간단한 특징을 점차적으로 더 복잡한 특징으로 집계하는 CNN 모델이 손글씨 문자 인식에 성공적으로 사용될 수 있다는 것을 입증했습니다.</p>
<p>CNN은 하나 이상의 합성곱층을 가진 신경망으로 주로 이미지 처리, 분류, 분할 및 기타 자기 상관 데이터에 사용됩니다. CNN의 채택 이전에는 대부분의 패턴 인식 작업이 초기 특징 추출 단계에서 수동으로 수행되고 분류기가 뒤를 이었습니다. CNN의 발전으로 인해 훈련 예제로부터 특징이 자동으로 학습되어 인간의 성능을 훌륭하게 능가하는 것이 가능해졌습니다.</p>
<p>CNN 접근은 합성곱 연산이 2D 이미지를 캡처하기 때문에 이미지 인식 작업에서 매력적입니다. 또한, 전체 이미지를 스캔하는 데 합성곱 커널을 사용하면 학습할 매개변수가 총 연산 수에 비해 상대적으로 적기 때문에 효율적입니다.</p>
<h1>데이터셋</h1>
<p>데이터를 얻기 위해 Udacity 시뮬레이터 [6]에서 훈련 모드를 사용하여 차량을 수동으로 운전하여 첫 번째 트랙을 한 방향으로 4바퀴 돌고 반대 방향으로 4바퀴 더 돌았습니다. 데이터 로그는 CSV 파일에 저장되어 있으며 이미지의 경로, 폴더에 저장된 방향표, 스티어링 휠 각도, 액셀, 후진 및 속도를 포함하고 있습니다. 스티어링 각도는 1/25 배율로 사전 스케일링되어 있어 -1에서 1 사이의 값을 갖습니다. 제공된 데이터에는 6563개의 중앙, 왼쪽 및 오른쪽 jpg 이미지가 포함되어 총 19689개의 예시 데이터 크기가 있었습니다. 사진의 너비는 320이고 높이는 160이었습니다. 차량의 왼쪽/중앙/오른쪽 이미지 한 장을 찍은 예시는 아래와 같습니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_2.png" alt="이미지"></p>
<h2>데이터 증강</h2>
<p>확대는 데이터에서 가능한 많은 정보를 추출하는 데 도움이 됩니다. 모델이 훈련 중에 볼 이미지의 수를 늘리기 위해 네 가지 다른 확대 기술을 사용했으며, 이는 오버피팅 경향을 줄였습니다. 사용된 이미지 확대 기술은 다음과 같습니다:</p>
<ul>
<li>밝기 감소: 밝기를 변경하여 낮과 밤 조건을 모방합니다.</li>
<li>좌우 카메라 이미지: 좌우 카메라 이미지를 사용하여 자동차가 측면으로 이탈하고 회복되는 효과를 모방합니다.</li>
<li>가로 및 세로 이동: 카메라 이미지를 가로로 이동시켜 도로 상의 자동차의 다양한 위치를 모방하고, 세로로 이동시켜 오르락내리락하는 효과를 모방합니다.</li>
<li>뒤집기: 훈련 데이터에서 좌우 회전이 균등하지 않기 때문에, 모델의 일반화를 위해 이미지 뒤집기가 필수적이었습니다.</li>
</ul>
<p>다음 이미지는 적용된 데이터 확대의 예시를 보여줍니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_3.png" alt="Data Augmentation"></p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_4.png" alt="Image 4"></p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_5.png" alt="Image 5"></p>
<h2>데이터 전처리</h2>
<p>이미지를 증강한 후, 모델 훈련에서 사용하기 전에 데이터 전처리가 적용되었습니다. 전처리는 이미지 품질을 향상시켜 분석에 더 잘 사용할 수 있도록 합니다. 사용된 이미지 전처리 기술은 다음과 같이 설명됩니다:</p>
<ul>
<li>잘라내기: 각 이미지의 하단 25픽셀과 상단 40픽셀을 잘라내어 차량의 전면과 지평선 위쪽의 하늘 대부분을 제거했습니다.</li>
<li>RGB에서 YUV로 변환: 이미지를 RGB에서 YUV 유형으로 변환했는데, 이렇게 하는 것이 조명 변화와 모양 감지에 더 유리합니다.</li>
<li>크기 조정: NVIDIA 모델과 일관성 있게 유지하기 위해 모든 이미지를 66 x 200으로 크기 조정했습니다.</li>
<li>정규화: 이미지 픽셀 값에 255로 나누어 0과 1 사이의 픽셀 값만 남도록 했습니다.</li>
</ul>
<p>아래 그림은 적용된 이미지 전처리의 예시를 보여줍니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_6.png" alt="Image Preprocessing Example"></p>
<h2>데이터 생성자</h2>
<p>수천 개의 새로운 훈련 인스턴스가 각 원본 이미지로부터 필요하기 때문에 이 모든 데이터를 디스크에 생성하고 저장하는 것은 불가능합니다. 따라서 Keras 제너레이터를 사용하여 로그 파일로부터 원본 데이터를 읽고 필요에 따라 데이터를 증가시키어 모델을 훈련하고, 각 배치에서 새 이미지를 생성했습니다.</p>
<h2>모델 제안</h2>
<p>이전에 언급했던 대로 CNN 모델을 사용했습니다. NVIDIA의 DAVE-2 시스템에서 사용된 모델에 영감을 받은 모델 아키텍처는 아래 그림 7에 나와 있습니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_7.png" alt="Model Architecture"></p>
<p>해당 모델은 다음과 같은 특징을 가지고 있어.</p>
<ul>
<li>입력 이미지는 66 x 200 크기야.</li>
<li>5x5 필터를 사용한 세 개의 컨볼루션 레이어가 있는데, 신경망을 따라갈수록 각 레이어는 24, 36, 48의 깊이를 가지고 있어.</li>
<li>그런 다음, 3x3 필터를 사용한 두 개의 연속적인 컨볼루션 레이어가 깊이 64로 이어져.</li>
<li>결과는 완전 연결 단계에 들어가기 위해 평평하게 만들어져.</li>
<li>마지막으로, 점점 작아지는 완전 연결 레이어인 1164, 200, 50, 10이 순서대로 이어져.</li>
<li>출력 레이어는 하나의 크기이고, 이는 조향 각도 하나만을 예측하기 때문이야.</li>
<li>모델은 252219개의 학습 가능한 매개변수를 가지고 있어.</li>
</ul>
<p>구현된 모델의 완전한 요약은 아래의 Figure 8에서 확인할 수 있어.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_8.png" alt="Figure 8"></p>
<p>모델은 활성화 함수로 지수 선형 단위(ELU) 비선형을 사용합니다. ReLU와는 달리 ELU는 음수 값을 가지며, 이는 평균 유닛 활성화를 제로에 가깝게 이동시킬 수 있도록 합니다. 아래 그림 9에서 ReLU와 ELU의 차이를 보실 수 있습니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_9.png" alt="Figure 9"></p>
<p>또한 Adam 옵티마이저가 사용되었는데, 이는 확률적 경사 하강법의 확장판으로 제곱 그래디언트를 사용하여 학습 속도를 조절하며, 훈련 데이터를 기반으로 네트워크 가중치를 반복적으로 업데이트합니다. 사용된 학습률은 0.0001 알파였습니다. 아래 표 1은 모델에 사용된 매개변수를 요약한 것입니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_10.png" alt="Table 1"></p>
<p><strong>카드 제작 및 검증</strong></p>
<p>이 모델은 NVIDIA GeForce RTX 3050 GPU를 사용하여 훈련되었고, 전체 훈련 소요 시간은 약 6시간이 소요되었습니다.</p>
<p>초기 데이터셋은 80%의 학습용 데이터와 20%의 검증용 데이터로 나뉘었습니다. 아래 그림 10은 학습과 검증 사이의 손실 비교를 나타냅니다.</p>
<p><img src="/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_11.png" alt="RevolutionizingAutonomyCNNsinSelf-DrivingCars_11"></p>
<p>Figure 10 shows that as the epochs increase, the loss value decreases. At first, there's a noticeable gap between the training and validation loss, but they eventually converge to be almost the same.</p>
<p>The model underwent testing on two different tracks. While it performed adequately on the first track, where the training data originated, it showed instability with frequent small corrections between left and right.</p>
<p>In contrast, the model behaved much better on the second track, demonstrating consistent and excellent performance, indicating strong generalization capabilities.</p>
<p>You can watch the video showcasing the model's performance on both tracks through the following link: <a href="youtube.com">Self-Driving Car: Predicting Steering Wheel Angle using Udacity’s Simulator — Model Performance</a></p>
<h1>결론</h1>
<p>깊은 신경망과 다양한 데이터 증강 기술을 사용하여 차량의 조향 휠 각도를 신뢰할 수 있는 방법으로 예측하는 모델을 만들 수 있다는 것이 입증되었습니다.</p>
<p>결과를 관찰한 후, 얻은 모델은 두 트랙에서 모두 잘 수행했기 때문에 매우 좋습니다. 그러나 매개변수를 조정하고 에폭 수를 늘림으로써 더 나은 교육을 할 수 있습니다.</p>
<p>현재와 같은 성능을 내면서 더 짧은 교육 시간에 더 나은 모델을 만들기 위해 모델 매개변수를 조정할 수 있습니다.</p>
<p>미래에는 다른 모델 아키텍처를 사용해 성능을 탐색하거나 다양한 데이터 증강 기술을 적용하며 강화 학습 모델을 실험하고, 실제 자동차에서 모델의 성능을 테스트해보는 것도 좋을 것 같아요.</p>
<p>독자 여러분, 감사합니다!</p>
<h1>참고 자료</h1>
<p>[1] Manikandan, T. (2020). Self-driving car</p>
<h2>References:</h2>
<ul>
<li>Szikora, P. (2017). <em>Self-driving cars — The human side</em></li>
<li>Vargas, J., et al. (2021). <em>An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions</em></li>
<li>Draelos, R. (2019). <em>The History of Convolutional Neural Networks</em></li>
<li>Bojarski, M., et al. (2016). <em>End to End Learning for Self-Driving Cars</em></li>
</ul>
<p>[6] Udacity. (2016). Udacity’s Self-Driving Car Simulator.</p>
<p>[7] Yang, L., et al. (2020). Random Noise Attenuation Based on Residual Convolutional Neural Network in Seismic Datasets</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"자율주행의 혁신 자율주행 자동차 속 CNN","description":"","date":"2024-05-15 12:20","slug":"2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars","content":"\n\n## NVIDIA의 예측 주행 스티어링을 위한 DAVE-2 시스템 깊이 분석\n\n![이미지](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png)\n\n본 기사는 센터, 좌측, 우측 세 대의 카메라로부터 입력 이미지를 사용하여 주행 중인 자동차의 조향 휠 각도를 예측하는 데 합성곱 신경망(CNN) 접근 방식을 활용합니다. 사용된 모델 아키텍처는 NVIDIA가 자체 자율주행 시스템인 DAVE-2를 위해 개발했습니다. CNN은 Udacity 시뮬레이터에서 테스트되었습니다.\n\n# 소개\n\n\n\n자율 주행 자동차는 주변 환경을 감지하고 인간의 개입 없이 교통과 다른 장애물을 독립적으로 움직일 수 있습니다. (출처: [1]) 자율 주행 차량(AVs)의 사용은 거의 전 세계적으로 주요 산업이 되었습니다. 과거 몇 년 동안 보다 빠르고 가치 있는 차량이 생산되었지만, 더 많은 차량이 있는 가속화된 세계에서는 불행히도 사고 사례가 증가했습니다. 대부분의 경우 사고는 인간 운전자의 잘못이 원인입니다. 따라서 자율 주행 차량의 도움으로 이론상으로 대체 가능할 수 있습니다. (출처: [2]) Waymo, Zoox, NVIDIA, Continental, Uber와 같은 다수의 관련 기업이 이 제품을 개발 중에 있습니다. 이 종류의 차량을 통해 자동차 교통의 안전성, 보안성, 효율성이 향상되며, 운전이 가장 안전하게 이루어질 수 있습니다. (출처: [1])\n\nAVs는 사람과 같이 주변 환경을 인식하고 수집한 정보를 기반으로 논리적인 결정을 내리기 위해 다양한 센서 기술에 의존합니다. 가장 일반적인 AV 센서 유형은 레이다, LiDAR, 초음파, 카메라, 글로벌 네비게이션 시스템 등이 있습니다. (출처: [3])\n\n고급 운전자 보조 시스템(ADAS)은 다양한 자율 주행 수준을 분류하는 다계층 시스템입니다. 이는 완전히 인간 운전에서부터 완전히 자율 주행까지 다양한 수준을 보여줍니다. (Figure 1 참고)\n\n\n\n# 합성곱 신경망 (CNNs)\n\n현대 CNN에 대한 첫 연구는 1990년대에 발표되었으며, 네오코그니트론에 영감을 받았습니다. 야너 르쿤 등은 \"Gradient-Based Learning Applied to Document Recognition\" 논문에서 더 간단한 특징을 점차적으로 더 복잡한 특징으로 집계하는 CNN 모델이 손글씨 문자 인식에 성공적으로 사용될 수 있다는 것을 입증했습니다.\n\nCNN은 하나 이상의 합성곱층을 가진 신경망으로 주로 이미지 처리, 분류, 분할 및 기타 자기 상관 데이터에 사용됩니다. CNN의 채택 이전에는 대부분의 패턴 인식 작업이 초기 특징 추출 단계에서 수동으로 수행되고 분류기가 뒤를 이었습니다. CNN의 발전으로 인해 훈련 예제로부터 특징이 자동으로 학습되어 인간의 성능을 훌륭하게 능가하는 것이 가능해졌습니다.\n\nCNN 접근은 합성곱 연산이 2D 이미지를 캡처하기 때문에 이미지 인식 작업에서 매력적입니다. 또한, 전체 이미지를 스캔하는 데 합성곱 커널을 사용하면 학습할 매개변수가 총 연산 수에 비해 상대적으로 적기 때문에 효율적입니다.\n\n\n\n# 데이터셋\n\n데이터를 얻기 위해 Udacity 시뮬레이터 [6]에서 훈련 모드를 사용하여 차량을 수동으로 운전하여 첫 번째 트랙을 한 방향으로 4바퀴 돌고 반대 방향으로 4바퀴 더 돌았습니다. 데이터 로그는 CSV 파일에 저장되어 있으며 이미지의 경로, 폴더에 저장된 방향표, 스티어링 휠 각도, 액셀, 후진 및 속도를 포함하고 있습니다. 스티어링 각도는 1/25 배율로 사전 스케일링되어 있어 -1에서 1 사이의 값을 갖습니다. 제공된 데이터에는 6563개의 중앙, 왼쪽 및 오른쪽 jpg 이미지가 포함되어 총 19689개의 예시 데이터 크기가 있었습니다. 사진의 너비는 320이고 높이는 160이었습니다. 차량의 왼쪽/중앙/오른쪽 이미지 한 장을 찍은 예시는 아래와 같습니다.\n\n![이미지](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_2.png)\n\n## 데이터 증강\n\n\n\n확대는 데이터에서 가능한 많은 정보를 추출하는 데 도움이 됩니다. 모델이 훈련 중에 볼 이미지의 수를 늘리기 위해 네 가지 다른 확대 기술을 사용했으며, 이는 오버피팅 경향을 줄였습니다. 사용된 이미지 확대 기술은 다음과 같습니다:\n\n- 밝기 감소: 밝기를 변경하여 낮과 밤 조건을 모방합니다.\n- 좌우 카메라 이미지: 좌우 카메라 이미지를 사용하여 자동차가 측면으로 이탈하고 회복되는 효과를 모방합니다.\n- 가로 및 세로 이동: 카메라 이미지를 가로로 이동시켜 도로 상의 자동차의 다양한 위치를 모방하고, 세로로 이동시켜 오르락내리락하는 효과를 모방합니다.\n- 뒤집기: 훈련 데이터에서 좌우 회전이 균등하지 않기 때문에, 모델의 일반화를 위해 이미지 뒤집기가 필수적이었습니다.\n\n다음 이미지는 적용된 데이터 확대의 예시를 보여줍니다.\n\n![Data Augmentation](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_3.png)\n\n\n\n![Image 4](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_4.png)\n\n![Image 5](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_5.png)\n\n## 데이터 전처리\n\n이미지를 증강한 후, 모델 훈련에서 사용하기 전에 데이터 전처리가 적용되었습니다. 전처리는 이미지 품질을 향상시켜 분석에 더 잘 사용할 수 있도록 합니다. 사용된 이미지 전처리 기술은 다음과 같이 설명됩니다:\n\n\n\n- 잘라내기: 각 이미지의 하단 25픽셀과 상단 40픽셀을 잘라내어 차량의 전면과 지평선 위쪽의 하늘 대부분을 제거했습니다.\n- RGB에서 YUV로 변환: 이미지를 RGB에서 YUV 유형으로 변환했는데, 이렇게 하는 것이 조명 변화와 모양 감지에 더 유리합니다.\n- 크기 조정: NVIDIA 모델과 일관성 있게 유지하기 위해 모든 이미지를 66 x 200으로 크기 조정했습니다.\n- 정규화: 이미지 픽셀 값에 255로 나누어 0과 1 사이의 픽셀 값만 남도록 했습니다.\n\n아래 그림은 적용된 이미지 전처리의 예시를 보여줍니다.\n\n![Image Preprocessing Example](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_6.png)\n\n## 데이터 생성자\n\n\n\n수천 개의 새로운 훈련 인스턴스가 각 원본 이미지로부터 필요하기 때문에 이 모든 데이터를 디스크에 생성하고 저장하는 것은 불가능합니다. 따라서 Keras 제너레이터를 사용하여 로그 파일로부터 원본 데이터를 읽고 필요에 따라 데이터를 증가시키어 모델을 훈련하고, 각 배치에서 새 이미지를 생성했습니다.\n\n## 모델 제안\n\n이전에 언급했던 대로 CNN 모델을 사용했습니다. NVIDIA의 DAVE-2 시스템에서 사용된 모델에 영감을 받은 모델 아키텍처는 아래 그림 7에 나와 있습니다.\n\n![Model Architecture](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_7.png)\n\n\n\n해당 모델은 다음과 같은 특징을 가지고 있어.\n\n- 입력 이미지는 66 x 200 크기야.\n- 5x5 필터를 사용한 세 개의 컨볼루션 레이어가 있는데, 신경망을 따라갈수록 각 레이어는 24, 36, 48의 깊이를 가지고 있어.\n- 그런 다음, 3x3 필터를 사용한 두 개의 연속적인 컨볼루션 레이어가 깊이 64로 이어져.\n- 결과는 완전 연결 단계에 들어가기 위해 평평하게 만들어져.\n- 마지막으로, 점점 작아지는 완전 연결 레이어인 1164, 200, 50, 10이 순서대로 이어져.\n- 출력 레이어는 하나의 크기이고, 이는 조향 각도 하나만을 예측하기 때문이야.\n- 모델은 252219개의 학습 가능한 매개변수를 가지고 있어.\n\n구현된 모델의 완전한 요약은 아래의 Figure 8에서 확인할 수 있어.\n\n![Figure 8](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_8.png)\n\n\n\n모델은 활성화 함수로 지수 선형 단위(ELU) 비선형을 사용합니다. ReLU와는 달리 ELU는 음수 값을 가지며, 이는 평균 유닛 활성화를 제로에 가깝게 이동시킬 수 있도록 합니다. 아래 그림 9에서 ReLU와 ELU의 차이를 보실 수 있습니다.\n\n![Figure 9](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_9.png)\n\n또한 Adam 옵티마이저가 사용되었는데, 이는 확률적 경사 하강법의 확장판으로 제곱 그래디언트를 사용하여 학습 속도를 조절하며, 훈련 데이터를 기반으로 네트워크 가중치를 반복적으로 업데이트합니다. 사용된 학습률은 0.0001 알파였습니다. 아래 표 1은 모델에 사용된 매개변수를 요약한 것입니다.\n\n![Table 1](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_10.png)\n\n\n\n**카드 제작 및 검증**\n\n이 모델은 NVIDIA GeForce RTX 3050 GPU를 사용하여 훈련되었고, 전체 훈련 소요 시간은 약 6시간이 소요되었습니다.\n\n초기 데이터셋은 80%의 학습용 데이터와 20%의 검증용 데이터로 나뉘었습니다. 아래 그림 10은 학습과 검증 사이의 손실 비교를 나타냅니다.\n\n![RevolutionizingAutonomyCNNsinSelf-DrivingCars_11](/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_11.png)\n\n\n\nFigure 10 shows that as the epochs increase, the loss value decreases. At first, there's a noticeable gap between the training and validation loss, but they eventually converge to be almost the same.\n\nThe model underwent testing on two different tracks. While it performed adequately on the first track, where the training data originated, it showed instability with frequent small corrections between left and right.\n\nIn contrast, the model behaved much better on the second track, demonstrating consistent and excellent performance, indicating strong generalization capabilities.\n\nYou can watch the video showcasing the model's performance on both tracks through the following link: [Self-Driving Car: Predicting Steering Wheel Angle using Udacity’s Simulator — Model Performance](youtube.com)\n\n\n\n# 결론\n\n깊은 신경망과 다양한 데이터 증강 기술을 사용하여 차량의 조향 휠 각도를 신뢰할 수 있는 방법으로 예측하는 모델을 만들 수 있다는 것이 입증되었습니다.\n\n결과를 관찰한 후, 얻은 모델은 두 트랙에서 모두 잘 수행했기 때문에 매우 좋습니다. 그러나 매개변수를 조정하고 에폭 수를 늘림으로써 더 나은 교육을 할 수 있습니다.\n\n현재와 같은 성능을 내면서 더 짧은 교육 시간에 더 나은 모델을 만들기 위해 모델 매개변수를 조정할 수 있습니다.\n\n\n\n미래에는 다른 모델 아키텍처를 사용해 성능을 탐색하거나 다양한 데이터 증강 기술을 적용하며 강화 학습 모델을 실험하고, 실제 자동차에서 모델의 성능을 테스트해보는 것도 좋을 것 같아요.\n\n독자 여러분, 감사합니다!\n\n# 참고 자료\n\n[1] Manikandan, T. (2020). Self-driving car\n\n\n\n## References:\n\n- Szikora, P. (2017). *Self-driving cars — The human side*\n- Vargas, J., et al. (2021). *An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions*\n- Draelos, R. (2019). *The History of Convolutional Neural Networks*\n- Bojarski, M., et al. (2016). *End to End Learning for Self-Driving Cars*\n\n\n\n[6] Udacity. (2016). Udacity’s Self-Driving Car Simulator.\n\n[7] Yang, L., et al. (2020). Random Noise Attenuation Based on Residual Convolutional Neural Network in Seismic Datasets","ogImage":{"url":"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png"},"coverImage":"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003eNVIDIA의 예측 주행 스티어링을 위한 DAVE-2 시스템 깊이 분석\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e본 기사는 센터, 좌측, 우측 세 대의 카메라로부터 입력 이미지를 사용하여 주행 중인 자동차의 조향 휠 각도를 예측하는 데 합성곱 신경망(CNN) 접근 방식을 활용합니다. 사용된 모델 아키텍처는 NVIDIA가 자체 자율주행 시스템인 DAVE-2를 위해 개발했습니다. CNN은 Udacity 시뮬레이터에서 테스트되었습니다.\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e자율 주행 자동차는 주변 환경을 감지하고 인간의 개입 없이 교통과 다른 장애물을 독립적으로 움직일 수 있습니다. (출처: [1]) 자율 주행 차량(AVs)의 사용은 거의 전 세계적으로 주요 산업이 되었습니다. 과거 몇 년 동안 보다 빠르고 가치 있는 차량이 생산되었지만, 더 많은 차량이 있는 가속화된 세계에서는 불행히도 사고 사례가 증가했습니다. 대부분의 경우 사고는 인간 운전자의 잘못이 원인입니다. 따라서 자율 주행 차량의 도움으로 이론상으로 대체 가능할 수 있습니다. (출처: [2]) Waymo, Zoox, NVIDIA, Continental, Uber와 같은 다수의 관련 기업이 이 제품을 개발 중에 있습니다. 이 종류의 차량을 통해 자동차 교통의 안전성, 보안성, 효율성이 향상되며, 운전이 가장 안전하게 이루어질 수 있습니다. (출처: [1])\u003c/p\u003e\n\u003cp\u003eAVs는 사람과 같이 주변 환경을 인식하고 수집한 정보를 기반으로 논리적인 결정을 내리기 위해 다양한 센서 기술에 의존합니다. 가장 일반적인 AV 센서 유형은 레이다, LiDAR, 초음파, 카메라, 글로벌 네비게이션 시스템 등이 있습니다. (출처: [3])\u003c/p\u003e\n\u003cp\u003e고급 운전자 보조 시스템(ADAS)은 다양한 자율 주행 수준을 분류하는 다계층 시스템입니다. 이는 완전히 인간 운전에서부터 완전히 자율 주행까지 다양한 수준을 보여줍니다. (Figure 1 참고)\u003c/p\u003e\n\u003ch1\u003e합성곱 신경망 (CNNs)\u003c/h1\u003e\n\u003cp\u003e현대 CNN에 대한 첫 연구는 1990년대에 발표되었으며, 네오코그니트론에 영감을 받았습니다. 야너 르쿤 등은 \"Gradient-Based Learning Applied to Document Recognition\" 논문에서 더 간단한 특징을 점차적으로 더 복잡한 특징으로 집계하는 CNN 모델이 손글씨 문자 인식에 성공적으로 사용될 수 있다는 것을 입증했습니다.\u003c/p\u003e\n\u003cp\u003eCNN은 하나 이상의 합성곱층을 가진 신경망으로 주로 이미지 처리, 분류, 분할 및 기타 자기 상관 데이터에 사용됩니다. CNN의 채택 이전에는 대부분의 패턴 인식 작업이 초기 특징 추출 단계에서 수동으로 수행되고 분류기가 뒤를 이었습니다. CNN의 발전으로 인해 훈련 예제로부터 특징이 자동으로 학습되어 인간의 성능을 훌륭하게 능가하는 것이 가능해졌습니다.\u003c/p\u003e\n\u003cp\u003eCNN 접근은 합성곱 연산이 2D 이미지를 캡처하기 때문에 이미지 인식 작업에서 매력적입니다. 또한, 전체 이미지를 스캔하는 데 합성곱 커널을 사용하면 학습할 매개변수가 총 연산 수에 비해 상대적으로 적기 때문에 효율적입니다.\u003c/p\u003e\n\u003ch1\u003e데이터셋\u003c/h1\u003e\n\u003cp\u003e데이터를 얻기 위해 Udacity 시뮬레이터 [6]에서 훈련 모드를 사용하여 차량을 수동으로 운전하여 첫 번째 트랙을 한 방향으로 4바퀴 돌고 반대 방향으로 4바퀴 더 돌았습니다. 데이터 로그는 CSV 파일에 저장되어 있으며 이미지의 경로, 폴더에 저장된 방향표, 스티어링 휠 각도, 액셀, 후진 및 속도를 포함하고 있습니다. 스티어링 각도는 1/25 배율로 사전 스케일링되어 있어 -1에서 1 사이의 값을 갖습니다. 제공된 데이터에는 6563개의 중앙, 왼쪽 및 오른쪽 jpg 이미지가 포함되어 총 19689개의 예시 데이터 크기가 있었습니다. 사진의 너비는 320이고 높이는 160이었습니다. 차량의 왼쪽/중앙/오른쪽 이미지 한 장을 찍은 예시는 아래와 같습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e데이터 증강\u003c/h2\u003e\n\u003cp\u003e확대는 데이터에서 가능한 많은 정보를 추출하는 데 도움이 됩니다. 모델이 훈련 중에 볼 이미지의 수를 늘리기 위해 네 가지 다른 확대 기술을 사용했으며, 이는 오버피팅 경향을 줄였습니다. 사용된 이미지 확대 기술은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e밝기 감소: 밝기를 변경하여 낮과 밤 조건을 모방합니다.\u003c/li\u003e\n\u003cli\u003e좌우 카메라 이미지: 좌우 카메라 이미지를 사용하여 자동차가 측면으로 이탈하고 회복되는 효과를 모방합니다.\u003c/li\u003e\n\u003cli\u003e가로 및 세로 이동: 카메라 이미지를 가로로 이동시켜 도로 상의 자동차의 다양한 위치를 모방하고, 세로로 이동시켜 오르락내리락하는 효과를 모방합니다.\u003c/li\u003e\n\u003cli\u003e뒤집기: 훈련 데이터에서 좌우 회전이 균등하지 않기 때문에, 모델의 일반화를 위해 이미지 뒤집기가 필수적이었습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e다음 이미지는 적용된 데이터 확대의 예시를 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_3.png\" alt=\"Data Augmentation\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_4.png\" alt=\"Image 4\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_5.png\" alt=\"Image 5\"\u003e\u003c/p\u003e\n\u003ch2\u003e데이터 전처리\u003c/h2\u003e\n\u003cp\u003e이미지를 증강한 후, 모델 훈련에서 사용하기 전에 데이터 전처리가 적용되었습니다. 전처리는 이미지 품질을 향상시켜 분석에 더 잘 사용할 수 있도록 합니다. 사용된 이미지 전처리 기술은 다음과 같이 설명됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e잘라내기: 각 이미지의 하단 25픽셀과 상단 40픽셀을 잘라내어 차량의 전면과 지평선 위쪽의 하늘 대부분을 제거했습니다.\u003c/li\u003e\n\u003cli\u003eRGB에서 YUV로 변환: 이미지를 RGB에서 YUV 유형으로 변환했는데, 이렇게 하는 것이 조명 변화와 모양 감지에 더 유리합니다.\u003c/li\u003e\n\u003cli\u003e크기 조정: NVIDIA 모델과 일관성 있게 유지하기 위해 모든 이미지를 66 x 200으로 크기 조정했습니다.\u003c/li\u003e\n\u003cli\u003e정규화: 이미지 픽셀 값에 255로 나누어 0과 1 사이의 픽셀 값만 남도록 했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e아래 그림은 적용된 이미지 전처리의 예시를 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_6.png\" alt=\"Image Preprocessing Example\"\u003e\u003c/p\u003e\n\u003ch2\u003e데이터 생성자\u003c/h2\u003e\n\u003cp\u003e수천 개의 새로운 훈련 인스턴스가 각 원본 이미지로부터 필요하기 때문에 이 모든 데이터를 디스크에 생성하고 저장하는 것은 불가능합니다. 따라서 Keras 제너레이터를 사용하여 로그 파일로부터 원본 데이터를 읽고 필요에 따라 데이터를 증가시키어 모델을 훈련하고, 각 배치에서 새 이미지를 생성했습니다.\u003c/p\u003e\n\u003ch2\u003e모델 제안\u003c/h2\u003e\n\u003cp\u003e이전에 언급했던 대로 CNN 모델을 사용했습니다. NVIDIA의 DAVE-2 시스템에서 사용된 모델에 영감을 받은 모델 아키텍처는 아래 그림 7에 나와 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_7.png\" alt=\"Model Architecture\"\u003e\u003c/p\u003e\n\u003cp\u003e해당 모델은 다음과 같은 특징을 가지고 있어.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e입력 이미지는 66 x 200 크기야.\u003c/li\u003e\n\u003cli\u003e5x5 필터를 사용한 세 개의 컨볼루션 레이어가 있는데, 신경망을 따라갈수록 각 레이어는 24, 36, 48의 깊이를 가지고 있어.\u003c/li\u003e\n\u003cli\u003e그런 다음, 3x3 필터를 사용한 두 개의 연속적인 컨볼루션 레이어가 깊이 64로 이어져.\u003c/li\u003e\n\u003cli\u003e결과는 완전 연결 단계에 들어가기 위해 평평하게 만들어져.\u003c/li\u003e\n\u003cli\u003e마지막으로, 점점 작아지는 완전 연결 레이어인 1164, 200, 50, 10이 순서대로 이어져.\u003c/li\u003e\n\u003cli\u003e출력 레이어는 하나의 크기이고, 이는 조향 각도 하나만을 예측하기 때문이야.\u003c/li\u003e\n\u003cli\u003e모델은 252219개의 학습 가능한 매개변수를 가지고 있어.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e구현된 모델의 완전한 요약은 아래의 Figure 8에서 확인할 수 있어.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_8.png\" alt=\"Figure 8\"\u003e\u003c/p\u003e\n\u003cp\u003e모델은 활성화 함수로 지수 선형 단위(ELU) 비선형을 사용합니다. ReLU와는 달리 ELU는 음수 값을 가지며, 이는 평균 유닛 활성화를 제로에 가깝게 이동시킬 수 있도록 합니다. 아래 그림 9에서 ReLU와 ELU의 차이를 보실 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_9.png\" alt=\"Figure 9\"\u003e\u003c/p\u003e\n\u003cp\u003e또한 Adam 옵티마이저가 사용되었는데, 이는 확률적 경사 하강법의 확장판으로 제곱 그래디언트를 사용하여 학습 속도를 조절하며, 훈련 데이터를 기반으로 네트워크 가중치를 반복적으로 업데이트합니다. 사용된 학습률은 0.0001 알파였습니다. 아래 표 1은 모델에 사용된 매개변수를 요약한 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_10.png\" alt=\"Table 1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e카드 제작 및 검증\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e이 모델은 NVIDIA GeForce RTX 3050 GPU를 사용하여 훈련되었고, 전체 훈련 소요 시간은 약 6시간이 소요되었습니다.\u003c/p\u003e\n\u003cp\u003e초기 데이터셋은 80%의 학습용 데이터와 20%의 검증용 데이터로 나뉘었습니다. 아래 그림 10은 학습과 검증 사이의 손실 비교를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars_11.png\" alt=\"RevolutionizingAutonomyCNNsinSelf-DrivingCars_11\"\u003e\u003c/p\u003e\n\u003cp\u003eFigure 10 shows that as the epochs increase, the loss value decreases. At first, there's a noticeable gap between the training and validation loss, but they eventually converge to be almost the same.\u003c/p\u003e\n\u003cp\u003eThe model underwent testing on two different tracks. While it performed adequately on the first track, where the training data originated, it showed instability with frequent small corrections between left and right.\u003c/p\u003e\n\u003cp\u003eIn contrast, the model behaved much better on the second track, demonstrating consistent and excellent performance, indicating strong generalization capabilities.\u003c/p\u003e\n\u003cp\u003eYou can watch the video showcasing the model's performance on both tracks through the following link: \u003ca href=\"youtube.com\"\u003eSelf-Driving Car: Predicting Steering Wheel Angle using Udacity’s Simulator — Model Performance\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e깊은 신경망과 다양한 데이터 증강 기술을 사용하여 차량의 조향 휠 각도를 신뢰할 수 있는 방법으로 예측하는 모델을 만들 수 있다는 것이 입증되었습니다.\u003c/p\u003e\n\u003cp\u003e결과를 관찰한 후, 얻은 모델은 두 트랙에서 모두 잘 수행했기 때문에 매우 좋습니다. 그러나 매개변수를 조정하고 에폭 수를 늘림으로써 더 나은 교육을 할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e현재와 같은 성능을 내면서 더 짧은 교육 시간에 더 나은 모델을 만들기 위해 모델 매개변수를 조정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e미래에는 다른 모델 아키텍처를 사용해 성능을 탐색하거나 다양한 데이터 증강 기술을 적용하며 강화 학습 모델을 실험하고, 실제 자동차에서 모델의 성능을 테스트해보는 것도 좋을 것 같아요.\u003c/p\u003e\n\u003cp\u003e독자 여러분, 감사합니다!\u003c/p\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cp\u003e[1] Manikandan, T. (2020). Self-driving car\u003c/p\u003e\n\u003ch2\u003eReferences:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSzikora, P. (2017). \u003cem\u003eSelf-driving cars — The human side\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eVargas, J., et al. (2021). \u003cem\u003eAn Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eDraelos, R. (2019). \u003cem\u003eThe History of Convolutional Neural Networks\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eBojarski, M., et al. (2016). \u003cem\u003eEnd to End Learning for Self-Driving Cars\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e[6] Udacity. (2016). Udacity’s Self-Driving Car Simulator.\u003c/p\u003e\n\u003cp\u003e[7] Yang, L., et al. (2020). Random Noise Attenuation Based on Residual Convolutional Neural Network in Seismic Datasets\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-RevolutionizingAutonomyCNNsinSelf-DrivingCars"},"buildId":"pS12KbT5cDEZt16dno2Ez","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>