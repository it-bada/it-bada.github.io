<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://it-bada.github.io///post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta property="og:title" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://it-bada.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://it-bada.github.io///post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro" data-gatsby-head="true"/><meta name="twitter:title" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | it-bada" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 23:37" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-1b0cb1352029e0fa.js" defer=""></script><script src="/_next/static/gXGk3PSGokmBKk5HZp8Um/_buildManifest.js" defer=""></script><script src="/_next/static/gXGk3PSGokmBKk5HZp8Um/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Bada</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Bada</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">3<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>Let&#x27;s deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.</p>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" alt="Click here to see the image."/></p>
<p>Ollama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.</p>
<p>It usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.</p>
<p>대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.</p>
<h2>1. &#x27;https://ollama.com/download/mac&#x27;으로 이동해주세요.</h2>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png" alt="이미지"/></p>
<p>Zip 파일을 다운로드하고 압축을 풉니다.</p>
<h2>2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.</h2>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png" alt="image"/></p>
<h2>3. 터미널을 열고 다음과 같이 입력해주세요.</h2>
<pre><code class="hljs language-js">ollama run llama3
</code></pre>
<p>그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.</p>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png" alt="image"/></p>
<p>브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 &quot;Ollama is running&quot;이 표시된다면 정상적으로 작동 중입니다.</p>
<h2>4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.</h2>
<p>마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.</p>
<pre><code class="hljs language-js">vim ~/.<span class="hljs-property">zshrc</span>

#파일에 아래 두 줄 추가

alias ollama_stop=<span class="hljs-string">&#x27;osascript -e &quot;tell application \&quot;Ollama\&quot; to quit&quot;&#x27;</span>
alias ollama_start=<span class="hljs-string">&#x27;ollama run llama3&#x27;</span>

#새 세션을 열고 아래 명령어를 사용하여 <span class="hljs-title class_">Ollama</span> 시작 및 중지

ollama_start
ollama_stop
</code></pre>
<h2>5. Llama3 성능 평가</h2>
<pre><code class="hljs language-js">git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/shadabshaukat/llm-benchmark.git</span>

cd llm-benchmark

python3<span class="hljs-number">.11</span> -m venv venv

source venv/bin/activate

pip install -r requirements.<span class="hljs-property">txt</span>

# <span class="hljs-title class_">Ollama</span>가 실행 중인지 확인하세요 <span class="hljs-string">&#x27;ollama serve&#x27;</span>

python benchmark.<span class="hljs-property">py</span> - verbose - prompts <span class="hljs-string">&quot;하늘이 파란 이유는 무엇인가요?&quot;</span> <span class="hljs-string">&quot;Nvidia의 재무에 관한 보고서 작성&quot;</span>

----------------------------------------------------

평균 통계:

----------------------------------------------------
        dolphin-<span class="hljs-attr">llama3</span>:latest
         프롬프트 평가: <span class="hljs-number">40.44</span> t/s
         응답: <span class="hljs-number">30.13</span> t/s
         총합: <span class="hljs-number">30.45</span> t/s

        통계:
         프롬프트 토큰: <span class="hljs-number">25</span>
         응답 토큰: <span class="hljs-number">576</span>
         모델 로드 시간: <span class="hljs-number">0.00</span>초
         프롬프트 평가 시간: <span class="hljs-number">0.62</span>초
         응답 시간: <span class="hljs-number">19.12</span>초
         총 시간: <span class="hljs-number">19.75</span>초
---------------------------------------------------- 
</code></pre>
<p>행복한 AI체험 되세요 :)</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요","description":"","date":"2024-05-15 23:37","slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro","content":"\n\nLet's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\n\n![Click here to see the image.](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png)\n\nOllama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\n\nIt usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\n\n\n\n대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\n\n## 1. 'https://ollama.com/download/mac'으로 이동해주세요.\n\n![이미지](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png)\n\nZip 파일을 다운로드하고 압축을 풉니다.\n\n\n\n## 2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png)\n\n## 3. 터미널을 열고 다음과 같이 입력해주세요.\n\n```js\nollama run llama3\n```\n\n\n\n그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png)\n\n브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.\n\n## 4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\n\n\n\n마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\n\n```js\nvim ~/.zshrc\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop='osascript -e \"tell application \\\"Ollama\\\" to quit\"'\nalias ollama_start='ollama run llama3'\n\n#새 세션을 열고 아래 명령어를 사용하여 Ollama 시작 및 중지\n\nollama_start\nollama_stop\n```\n\n## 5. Llama3 성능 평가\n\n```js\ngit clone https://github.com/shadabshaukat/llm-benchmark.git\n\ncd llm-benchmark\n\npython3.11 -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n# Ollama가 실행 중인지 확인하세요 'ollama serve'\n\npython benchmark.py - verbose - prompts \"하늘이 파란 이유는 무엇인가요?\" \"Nvidia의 재무에 관한 보고서 작성\"\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-llama3:latest\n         프롬프트 평가: 40.44 t/s\n         응답: 30.13 t/s\n         총합: 30.45 t/s\n\n        통계:\n         프롬프트 토큰: 25\n         응답 토큰: 576\n         모델 로드 시간: 0.00초\n         프롬프트 평가 시간: 0.62초\n         응답 시간: 19.12초\n         총 시간: 19.75초\n---------------------------------------------------- \n```\n\n\n\n행복한 AI체험 되세요 :)","ogImage":{"url":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png"},"coverImage":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png","tag":["Tech"],"readingTime":3},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h2: \"h2\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Let's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png\",\n        alt: \"Click here to see the image.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"It usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"1. 'https://ollama.com/download/mac'으로 이동해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Zip 파일을 다운로드하고 압축을 풉니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3. 터미널을 열고 다음과 같이 입력해주세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"ollama run llama3\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \\\"Ollama is running\\\"이 표시된다면 정상적으로 작동 중입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"vim ~/.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"zshrc\"\n        }), \"\\n\\n#파일에 아래 두 줄 추가\\n\\nalias ollama_stop=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'osascript -e \\\"tell application \\\\\\\"Ollama\\\\\\\" to quit\\\"'\"\n        }), \"\\nalias ollama_start=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ollama run llama3'\"\n        }), \"\\n\\n#새 세션을 열고 아래 명령어를 사용하여 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \" 시작 및 중지\\n\\nollama_start\\nollama_stop\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"5. Llama3 성능 평가\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"git clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/shadabshaukat/llm-benchmark.git\"\n        }), \"\\n\\ncd llm-benchmark\\n\\npython3\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".11\"\n        }), \" -m venv venv\\n\\nsource venv/bin/activate\\n\\npip install -r requirements.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"txt\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \"가 실행 중인지 확인하세요 \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ollama serve'\"\n        }), \"\\n\\npython benchmark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"py\"\n        }), \" - verbose - prompts \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"하늘이 파란 이유는 무엇인가요?\\\"\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Nvidia의 재무에 관한 보고서 작성\\\"\"\n        }), \"\\n\\n----------------------------------------------------\\n\\n평균 통계:\\n\\n----------------------------------------------------\\n        dolphin-\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama3\"\n        }), \":latest\\n         프롬프트 평가: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.44\"\n        }), \" t/s\\n         응답: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"30.13\"\n        }), \" t/s\\n         총합: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"30.45\"\n        }), \" t/s\\n\\n        통계:\\n         프롬프트 토큰: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"25\"\n        }), \"\\n         응답 토큰: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"576\"\n        }), \"\\n         모델 로드 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.00\"\n        }), \"초\\n         프롬프트 평가 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.62\"\n        }), \"초\\n         응답 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"19.12\"\n        }), \"초\\n         총 시간: \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"19.75\"\n        }), \"초\\n---------------------------------------------------- \\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"행복한 AI체험 되세요 :)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro"},"buildId":"gXGk3PSGokmBKk5HZp8Um","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>