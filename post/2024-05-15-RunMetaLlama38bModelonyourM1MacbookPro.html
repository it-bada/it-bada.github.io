<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://it-bada.github.io///post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta property="og:title" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://it-bada.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://it-bada.github.io///post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro" data-gatsby-head="true"/><meta name="twitter:title" content="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요 | it-bada" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | it-bada" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 23:37" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PDYZ2R0CH9"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-PDYZ2R0CH9');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/acd99c507555fdc6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/acd99c507555fdc6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-98bd357da1abcbf7.js" defer=""></script><script src="/_next/static/PXx-V41wNX8ZAfWDX1ICy/_buildManifest.js" defer=""></script><script src="/_next/static/PXx-V41wNX8ZAfWDX1ICy/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Bada</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Bada</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">3<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>Let's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.</p>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png" alt="Click here to see the image."></p>
<p>Ollama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.</p>
<p>It usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.</p>
<p>대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.</p>
<h2>1. '<a href="https://ollama.com/download/mac&#x27;%EC%9C%BC%EB%A1%9C" rel="nofollow" target="_blank">https://ollama.com/download/mac'으로</a> 이동해주세요.</h2>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png" alt="이미지"></p>
<p>Zip 파일을 다운로드하고 압축을 풉니다.</p>
<h2>2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.</h2>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png" alt="image"></p>
<h2>3. 터미널을 열고 다음과 같이 입력해주세요.</h2>
<pre><code class="hljs language-js">ollama run llama3
</code></pre>
<p>그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.</p>
<p><img src="/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png" alt="image"></p>
<p>브라우저에서 <a href="http://localhost:11434/%EC%9D%84" rel="nofollow" target="_blank">http://localhost:11434/을</a> 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 "Ollama is running"이 표시된다면 정상적으로 작동 중입니다.</p>
<h2>4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.</h2>
<p>마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.</p>
<pre><code class="hljs language-js">vim ~/.<span class="hljs-property">zshrc</span>

#파일에 아래 두 줄 추가

alias ollama_stop=<span class="hljs-string">'osascript -e "tell application \"Ollama\" to quit"'</span>
alias ollama_start=<span class="hljs-string">'ollama run llama3'</span>

#새 세션을 열고 아래 명령어를 사용하여 <span class="hljs-title class_">Ollama</span> 시작 및 중지

ollama_start
ollama_stop
</code></pre>
<h2>5. Llama3 성능 평가</h2>
<pre><code class="hljs language-js">git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/shadabshaukat/llm-benchmark.git</span>

cd llm-benchmark

python3<span class="hljs-number">.11</span> -m venv venv

source venv/bin/activate

pip install -r requirements.<span class="hljs-property">txt</span>

# <span class="hljs-title class_">Ollama</span>가 실행 중인지 확인하세요 <span class="hljs-string">'ollama serve'</span>

python benchmark.<span class="hljs-property">py</span> - verbose - prompts <span class="hljs-string">"하늘이 파란 이유는 무엇인가요?"</span> <span class="hljs-string">"Nvidia의 재무에 관한 보고서 작성"</span>

----------------------------------------------------

평균 통계:

----------------------------------------------------
        dolphin-<span class="hljs-attr">llama3</span>:latest
         프롬프트 평가: <span class="hljs-number">40.44</span> t/s
         응답: <span class="hljs-number">30.13</span> t/s
         총합: <span class="hljs-number">30.45</span> t/s

        통계:
         프롬프트 토큰: <span class="hljs-number">25</span>
         응답 토큰: <span class="hljs-number">576</span>
         모델 로드 시간: <span class="hljs-number">0.00</span>초
         프롬프트 평가 시간: <span class="hljs-number">0.62</span>초
         응답 시간: <span class="hljs-number">19.12</span>초
         총 시간: <span class="hljs-number">19.75</span>초
---------------------------------------------------- 
</code></pre>
<p>행복한 AI체험 되세요 :)</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"M1 맥북 프로에서 Meta Llama 3 8b 모델을 실행해주세요","description":"","date":"2024-05-15 23:37","slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro","content":"\n\nLet's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\n\n![Click here to see the image.](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png)\n\nOllama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\n\nIt usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\n\n\n\n대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\n\n## 1. 'https://ollama.com/download/mac'으로 이동해주세요.\n\n![이미지](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png)\n\nZip 파일을 다운로드하고 압축을 풉니다.\n\n\n\n## 2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png)\n\n## 3. 터미널을 열고 다음과 같이 입력해주세요.\n\n```js\nollama run llama3\n```\n\n\n\n그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\n\n![image](/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png)\n\n브라우저에서 http://localhost:11434/을 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.\n\n## 4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\n\n\n\n마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\n\n```js\nvim ~/.zshrc\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop='osascript -e \"tell application \\\"Ollama\\\" to quit\"'\nalias ollama_start='ollama run llama3'\n\n#새 세션을 열고 아래 명령어를 사용하여 Ollama 시작 및 중지\n\nollama_start\nollama_stop\n```\n\n## 5. Llama3 성능 평가\n\n```js\ngit clone https://github.com/shadabshaukat/llm-benchmark.git\n\ncd llm-benchmark\n\npython3.11 -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n# Ollama가 실행 중인지 확인하세요 'ollama serve'\n\npython benchmark.py - verbose - prompts \"하늘이 파란 이유는 무엇인가요?\" \"Nvidia의 재무에 관한 보고서 작성\"\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-llama3:latest\n         프롬프트 평가: 40.44 t/s\n         응답: 30.13 t/s\n         총합: 30.45 t/s\n\n        통계:\n         프롬프트 토큰: 25\n         응답 토큰: 576\n         모델 로드 시간: 0.00초\n         프롬프트 평가 시간: 0.62초\n         응답 시간: 19.12초\n         총 시간: 19.75초\n---------------------------------------------------- \n```\n\n\n\n행복한 AI체험 되세요 :)","ogImage":{"url":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png"},"coverImage":"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png","tag":["Tech"],"readingTime":3},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eLet's deploy the new Meta Llama 3 8b parameters model on your M1 Pro MacBook using Ollama.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_0.png\" alt=\"Click here to see the image.\"\u003e\u003c/p\u003e\n\u003cp\u003eOllama is a fantastic deployment platform designed to make deploying Open Source Large Language Models (LLM) a breeze.\u003c/p\u003e\n\u003cp\u003eIt usually takes around 15-20 minutes to have everything set up and running smoothly on a humble M1 Pro MacBook with 16GB of memory.\u003c/p\u003e\n\u003cp\u003e대부분의 시간은 실제로 5GB의 추론 파일을 다운로드하는 데 사용됩니다. 모델 자체는 약 30초 만에 시작됩니다.\u003c/p\u003e\n\u003ch2\u003e1. '\u003ca href=\"https://ollama.com/download/mac\u0026#x27;%EC%9C%BC%EB%A1%9C\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/download/mac'으로\u003c/a\u003e 이동해주세요.\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eZip 파일을 다운로드하고 압축을 풉니다.\u003c/p\u003e\n\u003ch2\u003e2. 올라마 애플리케이션을 열어주세요. 그 후 애플리케이션으로 이동해주세요.\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e3. 터미널을 열고 다음과 같이 입력해주세요.\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama run llama3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그럼 이제 끝났어요! 다운로드 및 빌드가 완료되기까지는 네트워크 대역폭에 따라 약 15-20분이 소요됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e브라우저에서 \u003ca href=\"http://localhost:11434/%EC%9D%84\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:11434/을\u003c/a\u003e 열어서 Ollama가 실행 중인지 확인해보세요. 화면에 \"Ollama is running\"이 표시된다면 정상적으로 작동 중입니다.\u003c/p\u003e\n\u003ch2\u003e4. 이제 Ollama를 실행하고 추론 속도를 테스트해봅시다.\u003c/h2\u003e\n\u003cp\u003e마침내, MacOS에서 Ollama를 빠르게 시작하고 중지할 수 있는 별칭 바로 가기를 추가해 봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003evim ~/.\u003cspan class=\"hljs-property\"\u003ezshrc\u003c/span\u003e\n\n#파일에 아래 두 줄 추가\n\nalias ollama_stop=\u003cspan class=\"hljs-string\"\u003e'osascript -e \"tell application \\\"Ollama\\\" to quit\"'\u003c/span\u003e\nalias ollama_start=\u003cspan class=\"hljs-string\"\u003e'ollama run llama3'\u003c/span\u003e\n\n#새 세션을 열고 아래 명령어를 사용하여 \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e 시작 및 중지\n\nollama_start\nollama_stop\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e5. Llama3 성능 평가\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003egit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/shadabshaukat/llm-benchmark.git\u003c/span\u003e\n\ncd llm-benchmark\n\npython3\u003cspan class=\"hljs-number\"\u003e.11\u003c/span\u003e -m venv venv\n\nsource venv/bin/activate\n\npip install -r requirements.\u003cspan class=\"hljs-property\"\u003etxt\u003c/span\u003e\n\n# \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e가 실행 중인지 확인하세요 \u003cspan class=\"hljs-string\"\u003e'ollama serve'\u003c/span\u003e\n\npython benchmark.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e - verbose - prompts \u003cspan class=\"hljs-string\"\u003e\"하늘이 파란 이유는 무엇인가요?\"\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Nvidia의 재무에 관한 보고서 작성\"\u003c/span\u003e\n\n----------------------------------------------------\n\n평균 통계:\n\n----------------------------------------------------\n        dolphin-\u003cspan class=\"hljs-attr\"\u003ellama3\u003c/span\u003e:latest\n         프롬프트 평가: \u003cspan class=\"hljs-number\"\u003e40.44\u003c/span\u003e t/s\n         응답: \u003cspan class=\"hljs-number\"\u003e30.13\u003c/span\u003e t/s\n         총합: \u003cspan class=\"hljs-number\"\u003e30.45\u003c/span\u003e t/s\n\n        통계:\n         프롬프트 토큰: \u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e\n         응답 토큰: \u003cspan class=\"hljs-number\"\u003e576\u003c/span\u003e\n         모델 로드 시간: \u003cspan class=\"hljs-number\"\u003e0.00\u003c/span\u003e초\n         프롬프트 평가 시간: \u003cspan class=\"hljs-number\"\u003e0.62\u003c/span\u003e초\n         응답 시간: \u003cspan class=\"hljs-number\"\u003e19.12\u003c/span\u003e초\n         총 시간: \u003cspan class=\"hljs-number\"\u003e19.75\u003c/span\u003e초\n---------------------------------------------------- \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e행복한 AI체험 되세요 :)\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-RunMetaLlama38bModelonyourM1MacbookPro"},"buildId":"PXx-V41wNX8ZAfWDX1ICy","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>