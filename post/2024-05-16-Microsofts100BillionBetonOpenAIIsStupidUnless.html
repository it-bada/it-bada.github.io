<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만 | it-bada</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://it-bada.github.io///post/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만 | it-bada" data-gatsby-head="true"/><meta property="og:title" content="Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만 | it-bada" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://it-bada.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://it-bada.github.io///post/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless" data-gatsby-head="true"/><meta name="twitter:title" content="Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만 | it-bada" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | it-bada" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 09:35" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PDYZ2R0CH9"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-PDYZ2R0CH9');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/acd99c507555fdc6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/acd99c507555fdc6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-98bd357da1abcbf7.js" defer=""></script><script src="/_next/static/PXx-V41wNX8ZAfWDX1ICy/_buildManifest.js" defer=""></script><script src="/_next/static/PXx-V41wNX8ZAfWDX1ICy/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">IT Bada</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">IT Bada</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>The Arizona desert houses a supercomputer created by a massive corporation to attain superintelligence - quite the science fiction plot, right? Well, it's happening now! Rumor has it that Microsoft is collaborating with OpenAI to establish the most advanced data center globally. They've even given this secret project a captivating name - Stargate.</p>
<p>The allocated resources for this endeavor seem mind-blowing, especially with the recent progress made in the AI realm aiming for a more streamlined future.</p>
<p>만약... 내가 단순사한 것일지도 모르죠. 장기 추론 인공지능 모델의 이론은 종이 위에 써져 있는 것이나, 세계 최고의 인공지능 연구소에서 더 나아가지 못한 것이 아니라 실제로 현실이 되고 있는 것일지도 모르는데, 우리 사회는 아직 이를 인식하거나 대비해두지 못해요.</p>
<h1>역사상 가장 중요한 인공지능 법칙</h1>
<p>시간이 흘러도 AI의 확장 법칙은 의심자들을 한없이 틀리게 증명해 왔습니다.</p>
<p>그리고 지금, 이 두 단어는 세계에서 가장 가치 있는 회사인 Microsoft를 이끌며 세계 66번째로 큰 경제의 GDP에 해당하는 투자를 단 한 프로젝트, 스타게이트에 집중하게 했어요.</p>
<p>But why, you might ask?</p>
<h2>Scaling: The First and Best Option</h2>
<p>Even as Large Language Models (LLMs) are steadily approaching the trillion-parameter mark, with advanced models like GPT-4, Claude 3, or Gemini surpassing it effortlessly, we have yet to see any signs of saturation as models grow larger.</p>
<p>Simply put, creating larger models continues to deliver better results, plain and simple.</p>
<p>더 기술적인 용어로 말하자면, LLM이 커질수록 모델을 훈련하는 데 사용되는 매개변수는 계속해서 감소하고 있습니다.</p>
<p>모든 것을 고려해 봤을 때, 이러한 연구소 뒤에 있는 대규모 기술 기업들(구글, 마이크로소프트, 메타 등)은 점점 더 큰 데이터 센터를 구축할 만큼 강력한 인센티브를 가지고 있어야 합니다.</p>
<p>하지만 마이크로소프트가 다른 이유를 가질 수도 있습니다.</p>
<p>SemiAnalysis 블로그에서 설명한 대로, 구글의 컴퓨팅 능력은 다른 모든 사람들을 '어리석게' 만드는 수준이다.</p>
<p>위 이미지를 보실 수 있습니다.</p>
<p>하지만, 진실로 $100 억을 투자해야 할까요?</p>
<p>최근의 발전을 살펴보면, 분명히 그렇지 않습니다.</p>
<h1>작아지는 경쟁</h1>
<p>최근 연구 트렌드를 살펴보면, AI가 장기적으로 훨씬 저렴해져야 한다는 네 가지 중요한 발전 덕분에 저도 귀가 멀어진 듯합니다.</p>
<h2>모든 것이 MoE</h2>
<p>이제 어느 새 모델은 전문가들의 혼합(Mixture-of-Experts)이 아니면 없다고 볼 수 있을 정도입니다.</p>
<p>아름답게 설명된 하이에나 오퍼레이터의 논문에 따르면 "주목 메커니즘은 언어 처리를 위해 제곱 기능 중 작은 부분만 활용한다는 것을 보여주는 증거가 점점 더 늘어나고 있다."</p>
<p>다시 말해, 더 커지는 모델은 더 좋아지지만, 함수에 대해 근사하는 데 더 소홀해지며 매우 희소해진다는 것을 뜻합니다.</p>
<p>새로운 예측마다 모델의 매우 작은 부분만 실제로 예측에 참여하므로 매우 비효율적인 프로세스입니다.</p>
<p>그러므로 MoE는 매혹적인 기회를 제공합니다.</p>
<p>에는 전체 네트워크를 매번 실행하는 것은 돈과 시간 낭비라고 알고 있기 때문에 전문가(뉴런 그룹)로 모델을 '분할'합니다.</p>
<p>간단히 말해서, 아래에 표시된 대로, 각 예측마다 '루터'라고 불리는 소프트맥스 게이트가 일정 수의 전문가를 선택합니다.</p>
<p>이러한 전문가들은 이미 훈련 중에 존재하기 때문에, 이들 전문가 안의 뉴런들은 특정 주제에 특화되어 있습니다.</p>
<p><img src="/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_1.png" alt="이미지"></p>
<p>그 방식으로, '모든 것을 아는' 하나의 거대한 신경망이 아닌, 매우 특정한 주제에 특화된 전문가 세트를 얻게 됩니다.</p>
<p>이에는 두 가지 의미가 있습니다:</p>
<ul>
<li>각 예측마다 모델의 일부만 실행됩니다. 예를 들어, Mixtral 8x7B의 경우, 8명의 전문가 중에 각 예측마다 2명만 실행됩니다. 이는 450억 모델이 모든 예측마다 활성화되는 매개변수가 120억이라는 것을 의미하며, 비용이 4로 줄어들게 됩니다. 이는 큰 모델이 훨씬 작은 모델처럼 작동하는 것입니다.</li>
<li>보다 정확한 훈련. 각 뉴런 세트가 적은 주제를 배워야 하므로 그들의 지식을 요출하기가 더 쉽고, '지식 붕괴'의 가능성을 줄입니다.</li>
</ul>
<p>하지만 모델들은 더 효율적으로만 되는 것이 아닙니다. 더 작아지고 있는 것입니다.</p>
<h2>1-비트 시대</h2>
<p>미국의 마이크로소프트가 주도하는 것은 놀랍게도 여기에 있습니다.</p>
<p>우리는 선도적인 예를 통해 각 매개 변수의 정밀도를 줄일 수 있지만 성능에는 눈에 띄지 않는 영향이 있다는 것을 깨닫고 있습니다.</p>
<p>게다가 모든 것을 1과 0으로 변환함으로써 행렬 곱셈을 필요로하는 것을 피할 수 있습니다. 왜냐하면 각 곱셈이 덧셈이 되기 때문입니다. 이것은 또한 GPU가 결국 그다지 필요하지 않을 수도 있다는 것을 의미할 것입니다.</p>
<p><img src="/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_2.png" alt="Image"></p>
<p>But if this doesn’t seem convincing enough to you that the models are becoming more affordable, other researchers are also toying with the idea of challenging the established 'status quo' of the past 7 years in AI.</p>
<h2>Hybrid architectures, just a matter of time</h2>
<p>Even though Transformers and their renowned attention mechanism boast top-tier performance, they are still not the perfect match for language tasks.</p>
<p>이유는 이 모델들은 비교할 수 없는 언어 모델링 성능을 제공하지만 엄청나게 비싸다는 것입니다.</p>
<p>문제는 무엇일까요?</p>
<p>음, Transformer는 상태가 없기 때문에 (반복적이고 고정된 메모리나 상태가 없음) 각 토큰은 시퀀스의 모든 다른 단어에 대해 자신이 주의를 기울이는 정도인 어텐션 점수를 계산해야 합니다. 이것은 어텐션의 비용 복잡성이 이차적이라는 것을 의미합니다.</p>
<p>이제 새로운 유형의 모델이 등장하고 있는데, 어텐션 연산을 버리지 않고 서브이차 연산과 결합시키는 방식입니다.</p>
<p>최근에 나온 좋은 예시가 있습니다. Mamba 연산자와 주의(그리고 전문가들의 혼합)를 결합한 Jamba는 독립형 트랜스포머의 성능을 맞먹으면서도 상당히 저렴하다는 것으로 나타났습니다.</p>
<p>그런데 혁신은 소프트웨어 쪽뿐만 아니라 하드웨어 수준에서도 일어나고 있습니다.</p>
<h2>손에 반지를 꼽으세요</h2>
<p>지난 몇 달 동안 가장 관련성 있는 혁신 중 하나는 Ring Attention입니다. 매우 낮은 메모리 요구 사항을 대폭 감소시키는 새로운 분산 컴퓨팅 아키텍처입니다.</p>
<p>사실, 여러 개의 GPU에 시퀀스를 분할함으로써 장치당 메모리 병목 현상을 없애고 엄청나게 긴 시퀀스 길이로 모델을 훈련하고 실행할 수 있게 됩니다.</p>
<p>그럼에도 불구하고, 거의 내일이 없는 것처럼 대규모 AI 기업들이 계속해서 GPU를 축적하고 있습니다. 그 이유는 바로 우리가 새로운 AI 패러다임으로 전환하고 있다는 사실 때문일 수도 있습니다.</p>
<p>하지만 이게 무슨 뜻일까요?</p>
<h1>모델들에게 생각을 할 수 있게 해주세요!</h1>
<p>지금은 너무나 압도적인 증거들이 있는 시기입니다. OpenAI부터 MIT, Google Deepmind에 이르기까지, 심지어 최근에는 Andrew Ng까지. 이유를 전적으로 이해할 수 없는 이유로, 모델이 탐색하고 반복할 수 있도록 추론 시간을 늘리면 결과가 현저히 향상된다는 점입니다.</p>
<p>아래에서 보시다시피, Agentic Workflows 내에서 실행될 때 심지어 GPT-3.5도 GPT-4를 압도합니다. 모델이 복잡한 작업을 해결하기 위해 당신처럼 반복할 수 있는 워크플로우에 포함되는 것입니다.</p>
<p><img src="/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_3.png" alt="이미지"></p>
<p>그러나 이러한 모델과 프레임워크에는 문제가 있습니다.</p>
<p>그리고 그것을 증명하는 예시가 있습니다.</p>
<p><strong>경쟁 프로그래머와 수학 올림피아드 수상자</strong></p>
<p>작년이나 그 이후에 출시된 가장 인상적인 AI 모델 중 하나는 AlphaCode 2입니다. 이 모델은 Gemini 1.0 상에서 실행되는 AI 모델/프레임워크로, 경쟁 프로그래밍 대회에서 85% 백분위에 도달했습니다.</p>
<p>문제는 무엇일까요? 연구자들이 인정했듯이, 그것을 대규모로 배포하는 것은 단순히 너무 비용이 많이 든다는 것입니다.</p>
<p>이유는 명백하고 간단해요. 컴퓨팅 리소스를 엄청나게 많이 소비하죠. 문제 하나를 해결하려 할 때, 한 번에 그 문제를 해결하려는 대신 100만 개까지 다양한 가능한 답변을 샘플링해요.</p>
<p>그런 다음, 똑똑한 필터링, 클러스터링, 그리고 걸러내기를 통해 결국 하나의 답변을 선택하고 응답해요.</p>
<p>이 절차의 장점은 분명해요: 더 많은 샘플을 만들면, 올바른 답변을 발견할 확률이 높아져요.</p>
<p>Let. Models. Think.</p>
<p>또 다른 훌륭한 예는 Google Deepmind의 AlphaGeometry입니다. 여기서는 검색 알고리즘의 실제 구현을 볼 수 있습니다.</p>
<p>간단히 말해서, 이 모델은 '가능한 보조 구성물의 영역을 탐색'합니다 (기하학 정리를 간단히 하는 데 도움이 되는 단서). 이를 통해 연습문제를 좁히게 됩니다.</p>
<p>더욱이, 모델이 막다른 길에 처했을 때, 되돌아가 다른 경로를 탐색할 수 있습니다. 이는 올바른 경로를 탐색하는 가능성을 크게 향상시킵니다. 이 모델은 일부 정리를 증명하는 더 스마트하고 간단한 방법을 찾아내는 것으로 밝혀졌습니다.</p>
<p>그렇다면, 이 모든 것이 Project Stargate와 무슨 관련이 있을까요?</p>
<p>예술 또는 창의성과 관련된 주제를 재미있고 흥미로운 관점에서 다루는 포스트입니다.</p>
<p>자료에 따르면 OpenAI도 유사한 모델과 협력하고 있다는 것이 잘 알려져 있습니다. Q* (Q-Star)라는 이름으로 알려진 이 모델은 수학에 능한 검색 및 생성 알고리즘입니다.</p>
<p>모든 것이 동일한 원리로 이어집니다:</p>
<p>LLM은 요청의 복잡성과 관계없이 토큰 예측 당 동일한 연산 노력을 할당하는데, 이는 바람직하지 않다고 할 수 있습니다.</p>
<p><strong>얼음 봉우리의 끝</strong></p>
<p>나에게 있어서, 스타게이트는 우리가 중요한 것을 찾아가고 있다는 명백한 징후입니다. 마이크로소프트가 GPT-6이나 그들이 어찌됐든 만들 것이 "더 큰 GPT-4"에 불과하다면 OpenAI에 1000억 달러를 투자하지 않을 것입니다.</p>
<p>그리고 그것이 불충분한 기준의 문제인지 아닌지에 상관없이, 사실은 LLM(Large Language Models)은 이전 버전을 향상하는 능력이 포화되는 것으로 보입니다. 예를 들어, GPT-4 교육이 끝난 후 거의 두 년이 지난 시점에 출시된 Claude 3는 약간 더 나아진 것 뿐입니다.</p>
<p>하지만 당신은 어떻게 생각하시나요? 대기업들이 단지 돈이 넘치는 상태라서 아무 말 없이 컴퓨트 자원을 낭비하는 것이라고 생각하시나요, 아니면 그들이 세상의 나머지 사람들이 알지 못하는 것을 알고 있는 것일지도 모르겠네요?</p>
<p>내 생각으로는, 스타게이트의 목적은 AI 모델의 다음 세대인 Long-Inference 검색 + 생성을 훈련하는 것이라고 생각합니다.</p>
<p>그리고 당신의 이야기는요?</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Microsoft의 OpenAI에 대한 1조 달러 투자는 어리석은 선택입니다 하지만","description":"","date":"2024-05-16 09:35","slug":"2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless","content":"\n\nThe Arizona desert houses a supercomputer created by a massive corporation to attain superintelligence - quite the science fiction plot, right? Well, it's happening now! Rumor has it that Microsoft is collaborating with OpenAI to establish the most advanced data center globally. They've even given this secret project a captivating name - Stargate.\n\nThe allocated resources for this endeavor seem mind-blowing, especially with the recent progress made in the AI realm aiming for a more streamlined future.\n\n\n\n만약... 내가 단순사한 것일지도 모르죠. 장기 추론 인공지능 모델의 이론은 종이 위에 써져 있는 것이나, 세계 최고의 인공지능 연구소에서 더 나아가지 못한 것이 아니라 실제로 현실이 되고 있는 것일지도 모르는데, 우리 사회는 아직 이를 인식하거나 대비해두지 못해요.\n\n# 역사상 가장 중요한 인공지능 법칙\n\n시간이 흘러도 AI의 확장 법칙은 의심자들을 한없이 틀리게 증명해 왔습니다.\n\n그리고 지금, 이 두 단어는 세계에서 가장 가치 있는 회사인 Microsoft를 이끌며 세계 66번째로 큰 경제의 GDP에 해당하는 투자를 단 한 프로젝트, 스타게이트에 집중하게 했어요.\n\n\n\nBut why, you might ask?\n\n## Scaling: The First and Best Option\n\nEven as Large Language Models (LLMs) are steadily approaching the trillion-parameter mark, with advanced models like GPT-4, Claude 3, or Gemini surpassing it effortlessly, we have yet to see any signs of saturation as models grow larger.\n\nSimply put, creating larger models continues to deliver better results, plain and simple.\n\n\n\n더 기술적인 용어로 말하자면, LLM이 커질수록 모델을 훈련하는 데 사용되는 매개변수는 계속해서 감소하고 있습니다.\n\n모든 것을 고려해 봤을 때, 이러한 연구소 뒤에 있는 대규모 기술 기업들(구글, 마이크로소프트, 메타 등)은 점점 더 큰 데이터 센터를 구축할 만큼 강력한 인센티브를 가지고 있어야 합니다.\n\n하지만 마이크로소프트가 다른 이유를 가질 수도 있습니다.\n\nSemiAnalysis 블로그에서 설명한 대로, 구글의 컴퓨팅 능력은 다른 모든 사람들을 '어리석게' 만드는 수준이다.\n\n\n\n위 이미지를 보실 수 있습니다.\n\n하지만, 진실로 $100 억을 투자해야 할까요?\n\n최근의 발전을 살펴보면, 분명히 그렇지 않습니다.\n\n\n\n# 작아지는 경쟁\n\n최근 연구 트렌드를 살펴보면, AI가 장기적으로 훨씬 저렴해져야 한다는 네 가지 중요한 발전 덕분에 저도 귀가 멀어진 듯합니다.\n\n## 모든 것이 MoE\n\n이제 어느 새 모델은 전문가들의 혼합(Mixture-of-Experts)이 아니면 없다고 볼 수 있을 정도입니다.\n\n\n\n아름답게 설명된 하이에나 오퍼레이터의 논문에 따르면 \"주목 메커니즘은 언어 처리를 위해 제곱 기능 중 작은 부분만 활용한다는 것을 보여주는 증거가 점점 더 늘어나고 있다.\"\n\n다시 말해, 더 커지는 모델은 더 좋아지지만, 함수에 대해 근사하는 데 더 소홀해지며 매우 희소해진다는 것을 뜻합니다.\n\n새로운 예측마다 모델의 매우 작은 부분만 실제로 예측에 참여하므로 매우 비효율적인 프로세스입니다.\n\n그러므로 MoE는 매혹적인 기회를 제공합니다.\n\n\n\n에는 전체 네트워크를 매번 실행하는 것은 돈과 시간 낭비라고 알고 있기 때문에 전문가(뉴런 그룹)로 모델을 '분할'합니다.\n\n간단히 말해서, 아래에 표시된 대로, 각 예측마다 '루터'라고 불리는 소프트맥스 게이트가 일정 수의 전문가를 선택합니다.\n\n이러한 전문가들은 이미 훈련 중에 존재하기 때문에, 이들 전문가 안의 뉴런들은 특정 주제에 특화되어 있습니다.\n\n![이미지](/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_1.png)\n\n\n\n그 방식으로, '모든 것을 아는' 하나의 거대한 신경망이 아닌, 매우 특정한 주제에 특화된 전문가 세트를 얻게 됩니다.\n\n이에는 두 가지 의미가 있습니다:\n\n- 각 예측마다 모델의 일부만 실행됩니다. 예를 들어, Mixtral 8x7B의 경우, 8명의 전문가 중에 각 예측마다 2명만 실행됩니다. 이는 450억 모델이 모든 예측마다 활성화되는 매개변수가 120억이라는 것을 의미하며, 비용이 4로 줄어들게 됩니다. 이는 큰 모델이 훨씬 작은 모델처럼 작동하는 것입니다.\n- 보다 정확한 훈련. 각 뉴런 세트가 적은 주제를 배워야 하므로 그들의 지식을 요출하기가 더 쉽고, '지식 붕괴'의 가능성을 줄입니다.\n\n하지만 모델들은 더 효율적으로만 되는 것이 아닙니다. 더 작아지고 있는 것입니다.\n\n\n\n## 1-비트 시대\n\n미국의 마이크로소프트가 주도하는 것은 놀랍게도 여기에 있습니다.\n\n우리는 선도적인 예를 통해 각 매개 변수의 정밀도를 줄일 수 있지만 성능에는 눈에 띄지 않는 영향이 있다는 것을 깨닫고 있습니다.\n\n게다가 모든 것을 1과 0으로 변환함으로써 행렬 곱셈을 필요로하는 것을 피할 수 있습니다. 왜냐하면 각 곱셈이 덧셈이 되기 때문입니다. 이것은 또한 GPU가 결국 그다지 필요하지 않을 수도 있다는 것을 의미할 것입니다.\n\n\n\n![Image](/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_2.png)\n\nBut if this doesn’t seem convincing enough to you that the models are becoming more affordable, other researchers are also toying with the idea of challenging the established 'status quo' of the past 7 years in AI.\n\n## Hybrid architectures, just a matter of time\n\nEven though Transformers and their renowned attention mechanism boast top-tier performance, they are still not the perfect match for language tasks.\n\n\n\n이유는 이 모델들은 비교할 수 없는 언어 모델링 성능을 제공하지만 엄청나게 비싸다는 것입니다.\n\n문제는 무엇일까요?\n\n음, Transformer는 상태가 없기 때문에 (반복적이고 고정된 메모리나 상태가 없음) 각 토큰은 시퀀스의 모든 다른 단어에 대해 자신이 주의를 기울이는 정도인 어텐션 점수를 계산해야 합니다. 이것은 어텐션의 비용 복잡성이 이차적이라는 것을 의미합니다.\n\n이제 새로운 유형의 모델이 등장하고 있는데, 어텐션 연산을 버리지 않고 서브이차 연산과 결합시키는 방식입니다.\n\n\n\n최근에 나온 좋은 예시가 있습니다. Mamba 연산자와 주의(그리고 전문가들의 혼합)를 결합한 Jamba는 독립형 트랜스포머의 성능을 맞먹으면서도 상당히 저렴하다는 것으로 나타났습니다.\n\n그런데 혁신은 소프트웨어 쪽뿐만 아니라 하드웨어 수준에서도 일어나고 있습니다.\n\n## 손에 반지를 꼽으세요\n\n지난 몇 달 동안 가장 관련성 있는 혁신 중 하나는 Ring Attention입니다. 매우 낮은 메모리 요구 사항을 대폭 감소시키는 새로운 분산 컴퓨팅 아키텍처입니다.\n\n\n\n사실, 여러 개의 GPU에 시퀀스를 분할함으로써 장치당 메모리 병목 현상을 없애고 엄청나게 긴 시퀀스 길이로 모델을 훈련하고 실행할 수 있게 됩니다.\n\n그럼에도 불구하고, 거의 내일이 없는 것처럼 대규모 AI 기업들이 계속해서 GPU를 축적하고 있습니다. 그 이유는 바로 우리가 새로운 AI 패러다임으로 전환하고 있다는 사실 때문일 수도 있습니다.\n\n하지만 이게 무슨 뜻일까요?\n\n# 모델들에게 생각을 할 수 있게 해주세요!\n\n\n\n지금은 너무나 압도적인 증거들이 있는 시기입니다. OpenAI부터 MIT, Google Deepmind에 이르기까지, 심지어 최근에는 Andrew Ng까지. 이유를 전적으로 이해할 수 없는 이유로, 모델이 탐색하고 반복할 수 있도록 추론 시간을 늘리면 결과가 현저히 향상된다는 점입니다.\n\n아래에서 보시다시피, Agentic Workflows 내에서 실행될 때 심지어 GPT-3.5도 GPT-4를 압도합니다. 모델이 복잡한 작업을 해결하기 위해 당신처럼 반복할 수 있는 워크플로우에 포함되는 것입니다.\n\n![이미지](/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_3.png)\n\n그러나 이러한 모델과 프레임워크에는 문제가 있습니다.\n\n\n\n그리고 그것을 증명하는 예시가 있습니다.\n\n**경쟁 프로그래머와 수학 올림피아드 수상자**\n\n작년이나 그 이후에 출시된 가장 인상적인 AI 모델 중 하나는 AlphaCode 2입니다. 이 모델은 Gemini 1.0 상에서 실행되는 AI 모델/프레임워크로, 경쟁 프로그래밍 대회에서 85% 백분위에 도달했습니다.\n\n문제는 무엇일까요? 연구자들이 인정했듯이, 그것을 대규모로 배포하는 것은 단순히 너무 비용이 많이 든다는 것입니다.\n\n\n\n이유는 명백하고 간단해요. 컴퓨팅 리소스를 엄청나게 많이 소비하죠. 문제 하나를 해결하려 할 때, 한 번에 그 문제를 해결하려는 대신 100만 개까지 다양한 가능한 답변을 샘플링해요.\n\n그런 다음, 똑똑한 필터링, 클러스터링, 그리고 걸러내기를 통해 결국 하나의 답변을 선택하고 응답해요.\n\n이 절차의 장점은 분명해요: 더 많은 샘플을 만들면, 올바른 답변을 발견할 확률이 높아져요.\n\nLet. Models. Think.\n\n\n\n또 다른 훌륭한 예는 Google Deepmind의 AlphaGeometry입니다. 여기서는 검색 알고리즘의 실제 구현을 볼 수 있습니다.\n\n간단히 말해서, 이 모델은 '가능한 보조 구성물의 영역을 탐색'합니다 (기하학 정리를 간단히 하는 데 도움이 되는 단서). 이를 통해 연습문제를 좁히게 됩니다.\n\n더욱이, 모델이 막다른 길에 처했을 때, 되돌아가 다른 경로를 탐색할 수 있습니다. 이는 올바른 경로를 탐색하는 가능성을 크게 향상시킵니다. 이 모델은 일부 정리를 증명하는 더 스마트하고 간단한 방법을 찾아내는 것으로 밝혀졌습니다.\n\n그렇다면, 이 모든 것이 Project Stargate와 무슨 관련이 있을까요?\n\n\n\n예술 또는 창의성과 관련된 주제를 재미있고 흥미로운 관점에서 다루는 포스트입니다.\n\n\n자료에 따르면 OpenAI도 유사한 모델과 협력하고 있다는 것이 잘 알려져 있습니다. Q* (Q-Star)라는 이름으로 알려진 이 모델은 수학에 능한 검색 및 생성 알고리즘입니다. \n\n\n모든 것이 동일한 원리로 이어집니다:\n\nLLM은 요청의 복잡성과 관계없이 토큰 예측 당 동일한 연산 노력을 할당하는데, 이는 바람직하지 않다고 할 수 있습니다.\n\n\n**얼음 봉우리의 끝**\n\n\n\n나에게 있어서, 스타게이트는 우리가 중요한 것을 찾아가고 있다는 명백한 징후입니다. 마이크로소프트가 GPT-6이나 그들이 어찌됐든 만들 것이 \"더 큰 GPT-4\"에 불과하다면 OpenAI에 1000억 달러를 투자하지 않을 것입니다.\n\n그리고 그것이 불충분한 기준의 문제인지 아닌지에 상관없이, 사실은 LLM(Large Language Models)은 이전 버전을 향상하는 능력이 포화되는 것으로 보입니다. 예를 들어, GPT-4 교육이 끝난 후 거의 두 년이 지난 시점에 출시된 Claude 3는 약간 더 나아진 것 뿐입니다.\n\n하지만 당신은 어떻게 생각하시나요? 대기업들이 단지 돈이 넘치는 상태라서 아무 말 없이 컴퓨트 자원을 낭비하는 것이라고 생각하시나요, 아니면 그들이 세상의 나머지 사람들이 알지 못하는 것을 알고 있는 것일지도 모르겠네요?\n\n내 생각으로는, 스타게이트의 목적은 AI 모델의 다음 세대인 Long-Inference 검색 + 생성을 훈련하는 것이라고 생각합니다.\n\n\n\n그리고 당신의 이야기는요?","ogImage":{"url":"/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_0.png"},"coverImage":"/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eThe Arizona desert houses a supercomputer created by a massive corporation to attain superintelligence - quite the science fiction plot, right? Well, it's happening now! Rumor has it that Microsoft is collaborating with OpenAI to establish the most advanced data center globally. They've even given this secret project a captivating name - Stargate.\u003c/p\u003e\n\u003cp\u003eThe allocated resources for this endeavor seem mind-blowing, especially with the recent progress made in the AI realm aiming for a more streamlined future.\u003c/p\u003e\n\u003cp\u003e만약... 내가 단순사한 것일지도 모르죠. 장기 추론 인공지능 모델의 이론은 종이 위에 써져 있는 것이나, 세계 최고의 인공지능 연구소에서 더 나아가지 못한 것이 아니라 실제로 현실이 되고 있는 것일지도 모르는데, 우리 사회는 아직 이를 인식하거나 대비해두지 못해요.\u003c/p\u003e\n\u003ch1\u003e역사상 가장 중요한 인공지능 법칙\u003c/h1\u003e\n\u003cp\u003e시간이 흘러도 AI의 확장 법칙은 의심자들을 한없이 틀리게 증명해 왔습니다.\u003c/p\u003e\n\u003cp\u003e그리고 지금, 이 두 단어는 세계에서 가장 가치 있는 회사인 Microsoft를 이끌며 세계 66번째로 큰 경제의 GDP에 해당하는 투자를 단 한 프로젝트, 스타게이트에 집중하게 했어요.\u003c/p\u003e\n\u003cp\u003eBut why, you might ask?\u003c/p\u003e\n\u003ch2\u003eScaling: The First and Best Option\u003c/h2\u003e\n\u003cp\u003eEven as Large Language Models (LLMs) are steadily approaching the trillion-parameter mark, with advanced models like GPT-4, Claude 3, or Gemini surpassing it effortlessly, we have yet to see any signs of saturation as models grow larger.\u003c/p\u003e\n\u003cp\u003eSimply put, creating larger models continues to deliver better results, plain and simple.\u003c/p\u003e\n\u003cp\u003e더 기술적인 용어로 말하자면, LLM이 커질수록 모델을 훈련하는 데 사용되는 매개변수는 계속해서 감소하고 있습니다.\u003c/p\u003e\n\u003cp\u003e모든 것을 고려해 봤을 때, 이러한 연구소 뒤에 있는 대규모 기술 기업들(구글, 마이크로소프트, 메타 등)은 점점 더 큰 데이터 센터를 구축할 만큼 강력한 인센티브를 가지고 있어야 합니다.\u003c/p\u003e\n\u003cp\u003e하지만 마이크로소프트가 다른 이유를 가질 수도 있습니다.\u003c/p\u003e\n\u003cp\u003eSemiAnalysis 블로그에서 설명한 대로, 구글의 컴퓨팅 능력은 다른 모든 사람들을 '어리석게' 만드는 수준이다.\u003c/p\u003e\n\u003cp\u003e위 이미지를 보실 수 있습니다.\u003c/p\u003e\n\u003cp\u003e하지만, 진실로 $100 억을 투자해야 할까요?\u003c/p\u003e\n\u003cp\u003e최근의 발전을 살펴보면, 분명히 그렇지 않습니다.\u003c/p\u003e\n\u003ch1\u003e작아지는 경쟁\u003c/h1\u003e\n\u003cp\u003e최근 연구 트렌드를 살펴보면, AI가 장기적으로 훨씬 저렴해져야 한다는 네 가지 중요한 발전 덕분에 저도 귀가 멀어진 듯합니다.\u003c/p\u003e\n\u003ch2\u003e모든 것이 MoE\u003c/h2\u003e\n\u003cp\u003e이제 어느 새 모델은 전문가들의 혼합(Mixture-of-Experts)이 아니면 없다고 볼 수 있을 정도입니다.\u003c/p\u003e\n\u003cp\u003e아름답게 설명된 하이에나 오퍼레이터의 논문에 따르면 \"주목 메커니즘은 언어 처리를 위해 제곱 기능 중 작은 부분만 활용한다는 것을 보여주는 증거가 점점 더 늘어나고 있다.\"\u003c/p\u003e\n\u003cp\u003e다시 말해, 더 커지는 모델은 더 좋아지지만, 함수에 대해 근사하는 데 더 소홀해지며 매우 희소해진다는 것을 뜻합니다.\u003c/p\u003e\n\u003cp\u003e새로운 예측마다 모델의 매우 작은 부분만 실제로 예측에 참여하므로 매우 비효율적인 프로세스입니다.\u003c/p\u003e\n\u003cp\u003e그러므로 MoE는 매혹적인 기회를 제공합니다.\u003c/p\u003e\n\u003cp\u003e에는 전체 네트워크를 매번 실행하는 것은 돈과 시간 낭비라고 알고 있기 때문에 전문가(뉴런 그룹)로 모델을 '분할'합니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해서, 아래에 표시된 대로, 각 예측마다 '루터'라고 불리는 소프트맥스 게이트가 일정 수의 전문가를 선택합니다.\u003c/p\u003e\n\u003cp\u003e이러한 전문가들은 이미 훈련 중에 존재하기 때문에, 이들 전문가 안의 뉴런들은 특정 주제에 특화되어 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e그 방식으로, '모든 것을 아는' 하나의 거대한 신경망이 아닌, 매우 특정한 주제에 특화된 전문가 세트를 얻게 됩니다.\u003c/p\u003e\n\u003cp\u003e이에는 두 가지 의미가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e각 예측마다 모델의 일부만 실행됩니다. 예를 들어, Mixtral 8x7B의 경우, 8명의 전문가 중에 각 예측마다 2명만 실행됩니다. 이는 450억 모델이 모든 예측마다 활성화되는 매개변수가 120억이라는 것을 의미하며, 비용이 4로 줄어들게 됩니다. 이는 큰 모델이 훨씬 작은 모델처럼 작동하는 것입니다.\u003c/li\u003e\n\u003cli\u003e보다 정확한 훈련. 각 뉴런 세트가 적은 주제를 배워야 하므로 그들의 지식을 요출하기가 더 쉽고, '지식 붕괴'의 가능성을 줄입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e하지만 모델들은 더 효율적으로만 되는 것이 아닙니다. 더 작아지고 있는 것입니다.\u003c/p\u003e\n\u003ch2\u003e1-비트 시대\u003c/h2\u003e\n\u003cp\u003e미국의 마이크로소프트가 주도하는 것은 놀랍게도 여기에 있습니다.\u003c/p\u003e\n\u003cp\u003e우리는 선도적인 예를 통해 각 매개 변수의 정밀도를 줄일 수 있지만 성능에는 눈에 띄지 않는 영향이 있다는 것을 깨닫고 있습니다.\u003c/p\u003e\n\u003cp\u003e게다가 모든 것을 1과 0으로 변환함으로써 행렬 곱셈을 필요로하는 것을 피할 수 있습니다. 왜냐하면 각 곱셈이 덧셈이 되기 때문입니다. 이것은 또한 GPU가 결국 그다지 필요하지 않을 수도 있다는 것을 의미할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_2.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eBut if this doesn’t seem convincing enough to you that the models are becoming more affordable, other researchers are also toying with the idea of challenging the established 'status quo' of the past 7 years in AI.\u003c/p\u003e\n\u003ch2\u003eHybrid architectures, just a matter of time\u003c/h2\u003e\n\u003cp\u003eEven though Transformers and their renowned attention mechanism boast top-tier performance, they are still not the perfect match for language tasks.\u003c/p\u003e\n\u003cp\u003e이유는 이 모델들은 비교할 수 없는 언어 모델링 성능을 제공하지만 엄청나게 비싸다는 것입니다.\u003c/p\u003e\n\u003cp\u003e문제는 무엇일까요?\u003c/p\u003e\n\u003cp\u003e음, Transformer는 상태가 없기 때문에 (반복적이고 고정된 메모리나 상태가 없음) 각 토큰은 시퀀스의 모든 다른 단어에 대해 자신이 주의를 기울이는 정도인 어텐션 점수를 계산해야 합니다. 이것은 어텐션의 비용 복잡성이 이차적이라는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e이제 새로운 유형의 모델이 등장하고 있는데, 어텐션 연산을 버리지 않고 서브이차 연산과 결합시키는 방식입니다.\u003c/p\u003e\n\u003cp\u003e최근에 나온 좋은 예시가 있습니다. Mamba 연산자와 주의(그리고 전문가들의 혼합)를 결합한 Jamba는 독립형 트랜스포머의 성능을 맞먹으면서도 상당히 저렴하다는 것으로 나타났습니다.\u003c/p\u003e\n\u003cp\u003e그런데 혁신은 소프트웨어 쪽뿐만 아니라 하드웨어 수준에서도 일어나고 있습니다.\u003c/p\u003e\n\u003ch2\u003e손에 반지를 꼽으세요\u003c/h2\u003e\n\u003cp\u003e지난 몇 달 동안 가장 관련성 있는 혁신 중 하나는 Ring Attention입니다. 매우 낮은 메모리 요구 사항을 대폭 감소시키는 새로운 분산 컴퓨팅 아키텍처입니다.\u003c/p\u003e\n\u003cp\u003e사실, 여러 개의 GPU에 시퀀스를 분할함으로써 장치당 메모리 병목 현상을 없애고 엄청나게 긴 시퀀스 길이로 모델을 훈련하고 실행할 수 있게 됩니다.\u003c/p\u003e\n\u003cp\u003e그럼에도 불구하고, 거의 내일이 없는 것처럼 대규모 AI 기업들이 계속해서 GPU를 축적하고 있습니다. 그 이유는 바로 우리가 새로운 AI 패러다임으로 전환하고 있다는 사실 때문일 수도 있습니다.\u003c/p\u003e\n\u003cp\u003e하지만 이게 무슨 뜻일까요?\u003c/p\u003e\n\u003ch1\u003e모델들에게 생각을 할 수 있게 해주세요!\u003c/h1\u003e\n\u003cp\u003e지금은 너무나 압도적인 증거들이 있는 시기입니다. OpenAI부터 MIT, Google Deepmind에 이르기까지, 심지어 최근에는 Andrew Ng까지. 이유를 전적으로 이해할 수 없는 이유로, 모델이 탐색하고 반복할 수 있도록 추론 시간을 늘리면 결과가 현저히 향상된다는 점입니다.\u003c/p\u003e\n\u003cp\u003e아래에서 보시다시피, Agentic Workflows 내에서 실행될 때 심지어 GPT-3.5도 GPT-4를 압도합니다. 모델이 복잡한 작업을 해결하기 위해 당신처럼 반복할 수 있는 워크플로우에 포함되는 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e그러나 이러한 모델과 프레임워크에는 문제가 있습니다.\u003c/p\u003e\n\u003cp\u003e그리고 그것을 증명하는 예시가 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e경쟁 프로그래머와 수학 올림피아드 수상자\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e작년이나 그 이후에 출시된 가장 인상적인 AI 모델 중 하나는 AlphaCode 2입니다. 이 모델은 Gemini 1.0 상에서 실행되는 AI 모델/프레임워크로, 경쟁 프로그래밍 대회에서 85% 백분위에 도달했습니다.\u003c/p\u003e\n\u003cp\u003e문제는 무엇일까요? 연구자들이 인정했듯이, 그것을 대규모로 배포하는 것은 단순히 너무 비용이 많이 든다는 것입니다.\u003c/p\u003e\n\u003cp\u003e이유는 명백하고 간단해요. 컴퓨팅 리소스를 엄청나게 많이 소비하죠. 문제 하나를 해결하려 할 때, 한 번에 그 문제를 해결하려는 대신 100만 개까지 다양한 가능한 답변을 샘플링해요.\u003c/p\u003e\n\u003cp\u003e그런 다음, 똑똑한 필터링, 클러스터링, 그리고 걸러내기를 통해 결국 하나의 답변을 선택하고 응답해요.\u003c/p\u003e\n\u003cp\u003e이 절차의 장점은 분명해요: 더 많은 샘플을 만들면, 올바른 답변을 발견할 확률이 높아져요.\u003c/p\u003e\n\u003cp\u003eLet. Models. Think.\u003c/p\u003e\n\u003cp\u003e또 다른 훌륭한 예는 Google Deepmind의 AlphaGeometry입니다. 여기서는 검색 알고리즘의 실제 구현을 볼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e간단히 말해서, 이 모델은 '가능한 보조 구성물의 영역을 탐색'합니다 (기하학 정리를 간단히 하는 데 도움이 되는 단서). 이를 통해 연습문제를 좁히게 됩니다.\u003c/p\u003e\n\u003cp\u003e더욱이, 모델이 막다른 길에 처했을 때, 되돌아가 다른 경로를 탐색할 수 있습니다. 이는 올바른 경로를 탐색하는 가능성을 크게 향상시킵니다. 이 모델은 일부 정리를 증명하는 더 스마트하고 간단한 방법을 찾아내는 것으로 밝혀졌습니다.\u003c/p\u003e\n\u003cp\u003e그렇다면, 이 모든 것이 Project Stargate와 무슨 관련이 있을까요?\u003c/p\u003e\n\u003cp\u003e예술 또는 창의성과 관련된 주제를 재미있고 흥미로운 관점에서 다루는 포스트입니다.\u003c/p\u003e\n\u003cp\u003e자료에 따르면 OpenAI도 유사한 모델과 협력하고 있다는 것이 잘 알려져 있습니다. Q* (Q-Star)라는 이름으로 알려진 이 모델은 수학에 능한 검색 및 생성 알고리즘입니다.\u003c/p\u003e\n\u003cp\u003e모든 것이 동일한 원리로 이어집니다:\u003c/p\u003e\n\u003cp\u003eLLM은 요청의 복잡성과 관계없이 토큰 예측 당 동일한 연산 노력을 할당하는데, 이는 바람직하지 않다고 할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e얼음 봉우리의 끝\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e나에게 있어서, 스타게이트는 우리가 중요한 것을 찾아가고 있다는 명백한 징후입니다. 마이크로소프트가 GPT-6이나 그들이 어찌됐든 만들 것이 \"더 큰 GPT-4\"에 불과하다면 OpenAI에 1000억 달러를 투자하지 않을 것입니다.\u003c/p\u003e\n\u003cp\u003e그리고 그것이 불충분한 기준의 문제인지 아닌지에 상관없이, 사실은 LLM(Large Language Models)은 이전 버전을 향상하는 능력이 포화되는 것으로 보입니다. 예를 들어, GPT-4 교육이 끝난 후 거의 두 년이 지난 시점에 출시된 Claude 3는 약간 더 나아진 것 뿐입니다.\u003c/p\u003e\n\u003cp\u003e하지만 당신은 어떻게 생각하시나요? 대기업들이 단지 돈이 넘치는 상태라서 아무 말 없이 컴퓨트 자원을 낭비하는 것이라고 생각하시나요, 아니면 그들이 세상의 나머지 사람들이 알지 못하는 것을 알고 있는 것일지도 모르겠네요?\u003c/p\u003e\n\u003cp\u003e내 생각으로는, 스타게이트의 목적은 AI 모델의 다음 세대인 Long-Inference 검색 + 생성을 훈련하는 것이라고 생각합니다.\u003c/p\u003e\n\u003cp\u003e그리고 당신의 이야기는요?\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-Microsofts100BillionBetonOpenAIIsStupidUnless"},"buildId":"PXx-V41wNX8ZAfWDX1ICy","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>